<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>动手深度学习(九):卷积神经网络 CNN 基础</title>
      <link href="/2020/02/20/dl-notes9-cnn-basic/"/>
      <url>/2020/02/20/dl-notes9-cnn-basic/</url>
      
        <content type="html"><![CDATA[<h1 id="卷积神经网络基础"><a href="#卷积神经网络基础" class="headerlink" title="卷积神经网络基础"></a>卷积神经网络基础</h1><p>卷积本质上体现了<code>两种信号的时移累加</code>。</p><p>本节我们介绍卷积神经网络的基础概念，主要是卷积层和池化层，并解释填充、步幅、输入通道和输出通道的含义。</p><h2 id="二维卷积层"><a href="#二维卷积层" class="headerlink" title="二维卷积层"></a>二维卷积层</h2><p>本节介绍的是最常见的二维卷积层，常用于处理图像数据。</p><h3 id="二维互相关运算"><a href="#二维互相关运算" class="headerlink" title="二维互相关运算"></a>二维互相关运算</h3><p>二维互相关（cross-correlation）运算的输入是一个二维输入数组和一个二维<code>核（kernel）数组</code>，输出也是一个二维数组，其中核数组通常称为<code>卷积核</code>或<code>过滤器</code>（filter）。卷积核的尺寸通常小于输入数组，卷积核在输入数组上滑动，在每个位置上，卷积核与该位置处的输入子数组按元素相乘并求和，得到输出数组中相应位置的元素。图1展示了一个互相关运算的例子，阴影部分分别是输入的第一个计算区域、核数组以及对应的输出。</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/xTyKEpR7j6gVaHb.png" alt="二维互相关运算"></p><p>下面我们用<code>corr2d</code>函数实现二维互相关运算，它接受输入数组<code>X</code>与核数组<code>K</code>，并输出数组<code>Y</code>。</p><pre><code class="lang-python">import torch import torch.nn as nndef corr2d(X, K):    H, W = X.shape    h, w = K.shape    Y = torch.zeros(H - h + 1, W - w + 1)    for i in range(Y.shape[0]):        for j in range(Y.shape[1]):            Y[i, j] = (X[i: i + h, j: j + w] * K).sum()    return Y</code></pre><p>构造上图中的输入数组<code>X</code>、核数组<code>K</code>来验证二维互相关运算的输出。</p><pre><code class="lang-python">X = torch.tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]])K = torch.tensor([[0, 1], [2, 3]])Y = corr2d(X, K)print(Y)</code></pre><pre><code>tensor([[19., 25.],        [37., 43.]])</code></pre><h3 id="二维卷积层-1"><a href="#二维卷积层-1" class="headerlink" title="二维卷积层"></a>二维卷积层</h3><p>二维卷积层将输入和卷积核做互相关运算，并加上一个标量偏置来得到输出。卷积层的<code>模型参数</code>包括「卷积核」<code>self.weight</code>和「标量偏置」<code>self.bias</code>。</p><ul><li><p><code>nn.Parameter()</code>是 tensor 的子类，会为参数自动附上梯度，是可学习的</p></li><li><p><code>nn.Module</code>表示的模型中会维护一个参数集合，定义nn.Parameter()会自动把参数注册到 model 的参数集合中</p></li></ul><pre><code class="lang-python">class Conv2D(nn.Module):    def __init__(self, kernel_size):  # kernel_size 卷积层的高和宽        super(Conv2D, self).__init__()        self.weight = nn.Parameter(torch.randn(kernel_size))        self.bias = nn.Parameter(torch.randn(1))    def forward(self, x):        return corr2d(x, self.weight) + self.bias  # 广播机制，每个位置加了同样一个偏置</code></pre><p>下面我们看一个例子，我们构造一张$6 \times 8$的图像，中间4列为黑（0），其余为白（1），希望检测到颜色边缘。我们的标签是一个$6 \times 7$的二维数组，第2列是1（从1到0的边缘），第6列是-1（从0到1的边缘）。</p><pre><code class="lang-python">X = torch.ones(6, 8)Y = torch.zeros(6, 7)X[:, 2: 6] = 0Y[:, 1] = 1Y[:, 5] = -1print(X)print(Y)</code></pre><pre><code>tensor([[1., 1., 0., 0., 0., 0., 1., 1.],        [1., 1., 0., 0., 0., 0., 1., 1.],        [1., 1., 0., 0., 0., 0., 1., 1.],        [1., 1., 0., 0., 0., 0., 1., 1.],        [1., 1., 0., 0., 0., 0., 1., 1.],        [1., 1., 0., 0., 0., 0., 1., 1.]])tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])</code></pre><p>我们希望学习一个$1 \times 2$卷积层，通过卷积层来检测颜色边缘。（因为计算边缘考虑的是同一行的相邻两个元素，1 行 2 列）</p><pre><code class="lang-python">conv2d = Conv2D(kernel_size=(1, 2))step = 30lr = 0.01for i in range(step):    Y_hat = conv2d(X)    l = ((Y_hat - Y) ** 2).sum()    l.backward()    # 梯度下降    conv2d.weight.data -= lr * conv2d.weight.grad    conv2d.bias.data -= lr * conv2d.bias.grad    # 梯度清零    conv2d.weight.grad.zero_()    conv2d.bias.grad.zero_()    if (i + 1) % 5 == 0:        print(&#39;Step %d, loss %.3f&#39; % (i + 1, l.item()))print(conv2d.weight.data)print(conv2d.bias.data)</code></pre><pre><code>Step 5, loss 4.569Step 10, loss 0.949Step 15, loss 0.228Step 20, loss 0.060Step 25, loss 0.016Step 30, loss 0.004tensor([[ 1.0161, -1.0177]])tensor([0.0009])</code></pre><h3 id="互相关运算与卷积运算"><a href="#互相关运算与卷积运算" class="headerlink" title="互相关运算与卷积运算"></a>互相关运算与卷积运算</h3><p>卷积层得名于卷积运算，但卷积层中用到的并非卷积运算而是互相关运算。我们将核数组上下翻转、左右翻转，再与输入数组做互相关运算，这一过程就是卷积运算。由于卷积层的核数组是可学习的，所以使用互相关运算与使用卷积运算并无本质区别。</p><h3 id="特征图与感受野"><a href="#特征图与感受野" class="headerlink" title="特征图与感受野"></a>特征图与感受野</h3><p>二维卷积层<strong>输出</strong>的二维数组可以看作是输入在空间维度（宽和高）上某一级的表征，也叫<strong>「特征图」（feature map）</strong>。影响元素$x$的前向计算的所有可能<strong>输入</strong>区域（可能大于输入的实际尺寸）叫做$x$的 <strong>「感受野」（receptive field）</strong>。</p><p>以图1为例，输入中阴影部分的四个元素是输出中阴影部分元素的感受野。我们将图中形状为$2 \times 2$的输出记为$Y$，将$Y$与另一个形状为$2 \times 2$的核数组做互相关运算，输出单个元素$z$。那么，$z$在$Y$上的感受野包括$Y$的全部四个元素，在输入上的感受野包括其中全部9个元素。可见，我们可以通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征。</p><p>堆叠加深，不同层的感受野不同，<code>越深越大</code></p><h2 id="填充和步幅"><a href="#填充和步幅" class="headerlink" title="填充和步幅"></a>填充和步幅</h2><p>我们介绍卷积层的两个超参数，即填充和步幅，它们可以对给定形状的输入和卷积核改变输出形状。</p><h3 id="填充"><a href="#填充" class="headerlink" title="填充"></a>填充</h3><p>填充（padding）是指在输入高和宽的两侧填充元素（通常是0元素），图2里我们在原输入高和宽的两侧分别添加了值为0的元素。</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/qzCBiYL8nONrIds.png" alt="填充"></p><p>图2 在输入的高和宽两侧分别填充了0元素的二维互相关计算</p><p>如果原输入的高和宽是$n_h$和$n_w$，卷积核的高和宽是$k_h$和$k_w$，在高的两侧一共填充$p_h$行，在宽的两侧一共填充$p_w$列，则输出形状为：</p><script type="math/tex; mode=display">(n_h+p_h-k_h+1)\times(n_w+p_w-k_w+1)</script><p>我们在卷积神经网络中使用奇数高宽的核，比如$3 \times 3$，$5 \times 5$的卷积核，对于高度（或宽度）为大小为$2 k + 1$的核，令步幅为1，在高（或宽）两侧选择大小为$k$的填充，便可保持输入与输出尺寸相同。</p><h3 id="步幅"><a href="#步幅" class="headerlink" title="步幅"></a>步幅</h3><p>在互相关运算中，卷积核在输入数组上滑动，每次滑动的行数与列数即是步幅（stride）。此前我们使用的步幅都是1，图3展示了在高上步幅为3、在宽上步幅为2的二维互相关运算。</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/r9cVRdxynqzvjbt.png" alt="步幅"></p><p>图3 高和宽上步幅分别为3和2的二维互相关运算</p><p>一般来说，当高上步幅为$s_h$，宽上步幅为$s_w$时，输出形状为：</p><script type="math/tex; mode=display">\lfloor(n_h+p_h-k_h+s_h)/s_h\rfloor \times \lfloor(n_w+p_w-k_w+s_w)/s_w\rfloor</script><p>如果$p_h=k_h-1$，$p_w=k_w-1$，那么输出形状将简化为$\lfloor(n_h+s_h-1)/s_h\rfloor \times \lfloor(n_w+s_w-1)/s_w\rfloor$。更进一步，如果输入的高和宽能分别被高和宽上的步幅整除，那么输出形状将是$(n_h / s_h) \times (n_w/s_w)$。</p><p>当$p_h = p_w = p$时，我们称填充为$p$；当$s_h = s_w = s$时，我们称步幅为$s$。</p><h2 id="多输入通道和多输出通道"><a href="#多输入通道和多输出通道" class="headerlink" title="多输入通道和多输出通道"></a>多输入通道和多输出通道</h2><p>之前的输入和输出都是二维数组，但真实数据的维度经常更高。例如，彩色图像在高和宽2个维度外还有RGB（红、绿、蓝）3个颜色通道。假设彩色图像的高和宽分别是$h$和$w$（像素），那么它可以表示为一个$3 \times h \times w$的多维数组，我们将大小为3的这一维称为通道（channel）维。</p><h3 id="多输入通道"><a href="#多输入通道" class="headerlink" title="多输入通道"></a>多输入通道</h3><p>卷积层的输入可以包含多个通道，图4展示了一个含2个输入通道的二维互相关计算的例子。（每层的输入和该层的核相乘，最后叠加成单维度输出，输出只有一个通道）</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/NCjg1iDhPY8ItOF.png" alt="2 输入通道 1 输出通道"></p><p>图4 含2个输入通道的互相关计算</p><p>假设输入数据的通道数为$c_i$，卷积核形状为$k_h\times k_w$，我们为每个输入通道各分配一个形状为$k_h\times k_w$的核数组，将$c_i$个互相关运算的二维输出按通道相加，得到一个二维数组作为输出。我们把$c_i$个核数组在通道维上连结，即得到一个形状为$c_i\times k_h\times k_w$的卷积核。</p><h3 id="多输出通道"><a href="#多输出通道" class="headerlink" title="多输出通道"></a>多输出通道</h3><p>卷积层的输出也可以包含多个通道，设卷积核输入通道数和输出通道数分别为$c_i$和$c_o$，高和宽分别为$k_h$和$k_w$。如果希望得到含多个通道的输出，我们可以为每个输出通道分别创建形状为$c_i\times k_h\times k_w$的核数组，将它们在输出通道维上连结（有加法计算），卷积核的形状即$c_o\times c_i\times k_h\times k_w$。<br>（如下图，3 个输入通道，2 个输出通道，中间有 6 个 $1 \times 1$ 的核数组。)</p><p>对于输出通道的卷积核，我们提供这样一种理解，一个$c_i \times k_h \times k_w$的核数组可以提取某种局部特征，但是输入可能具有相当丰富的特征，我们需要有多个这样的$c_i \times k_h \times k_w$的核数组，不同的核数组提取的是不同的特征。</p><h3 id="1x1卷积层"><a href="#1x1卷积层" class="headerlink" title="1x1卷积层"></a>1x1卷积层</h3><p>最后讨论形状为$1 \times 1$的卷积核，我们通常称这样的卷积运算为$1 \times 1$卷积，称包含这种卷积核的卷积层为$1 \times 1$卷积层。图5展示了使用输入通道数为3、输出通道数为2的$1\times 1$卷积核的互相关计算。</p><p>$1\times 1$卷积层的作用与<code>全连接层</code>等价，相当于<code>矩阵乘法</code>。</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/I8MX65oJvur3b7f.png" alt="2x3=6 个 1x1 卷积层"></p><p>图5 1x1卷积核的互相关计算。输入和输出具有<code>相同的高和宽</code></p><p>$1 \times 1$卷积核可在不改变高宽的情况下，<code>调整通道数</code>。$1 \times 1$卷积核不识别高和宽维度上相邻元素构成的模式，其主要计算发生在通道维上。假设我们将通道维当作特征维，将高和宽维度上的元素当成数据样本，那么$1\times 1$卷积层的作用与全连接层等价。</p><h2 id="卷积层与全连接层的对比"><a href="#卷积层与全连接层的对比" class="headerlink" title="卷积层与全连接层的对比"></a>卷积层与全连接层的对比</h2><p>二维卷积层经常用于处理图像，与此前的全连接层相比，它主要有两个优势：</p><ul><li><p>一是全连接层把图像<code>展平</code>成一个向量，在输入图像上相邻的元素可能因为展平操作不再相邻，网络难以捕捉局部信息。而卷积层的设计，天然地具有提取局部信息的能力。</p></li><li><p>二是卷积层的参数量更少。不考虑偏置的情况下，一个形状为$(c_i, c_o, h, w)$的卷积核的参数量是$c_i \times c_o \times h \times w$，<code>与输入图像的宽高无关</code>。假如一个卷积层的输入和输出形状分别是$(c_1, h_1, w_1)$和$(c_2, h_2, w_2)$，如果要用全连接层进行连接，参数数量就是$c_1 \times c_2 \times h_1 \times w_1 \times h_2 \times w_2$。使用卷积层可以以较少的参数数量来处理更大的图像。</p></li></ul><h2 id="卷积层的简洁实现"><a href="#卷积层的简洁实现" class="headerlink" title="卷积层的简洁实现"></a>卷积层的简洁实现</h2><p>我们使用Pytorch中的<code>nn.Conv2d</code>类来实现二维卷积层，主要关注以下几个构造函数参数：</p><ul><li><code>in_channels</code> (python:int) – Number of channels in the input imag  输入通道数</li><li><code>out_channels</code> (python:int) – Number of channels produced by the convolution 输出通道数</li><li><code>kernel_size</code> (python:int or tuple) – Size of the convolving kernel 卷积核大小，int 表示高和宽相等，元组表示不相等</li><li><code>stride</code> (python:int or tuple, optional) – Stride of the convolution. Default: 1</li><li><code>padding</code> (python:int or tuple, optional) – Zero-padding added to both sides of the input. Default: 0</li><li><code>bias</code> (bool, optional) – If True, adds a learnable bias to the output. Default: True</li></ul><p><code>forward</code>函数的参数为一个四维张量，形状为$(N批量大小, C_{in}通道数， H_{in}高度, W_{in}宽度)$，返回值也是一个四维张量，形状为$(N, C_{out}, H_{out}, W_{out})$</p><p>kernel_size（即卷积核大小）这是需要人为设定的参数，该参数是不需要学习的，当然大小不同，卷积结果也是不同的。经过大量实验表明，大多选用<code>1x1、3x3、5x5</code>等尺寸较小且长宽为奇数的卷积核。</p><p>代码讲解</p><pre><code class="lang-python">X = torch.rand(4, 2, 3, 5)print(X.shape)conv2d = nn.Conv2d(in_channels=2, out_channels=3, kernel_size=(3, 5), stride=1, padding=(1, 2))Y = conv2d(X)print(&#39;Y.shape: &#39;, Y.shape)print(&#39;weight.shape: &#39;, conv2d.weight.shape)  # padding 和 size 刚好是 2k+1 的关系，所以 in 和 out 宽高不变print(&#39;bias.shape: &#39;, conv2d.bias.shape) # Y.shape:  torch.Size([4, 2, 3, 5])# Y.shape:  torch.Size([4, 3, 3, 5])# weight.shape:  torch.Size([3, 2, 3, 5])# bias.shape:  torch.Size([3])</code></pre><h2 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h2><h3 id="二维池化层"><a href="#二维池化层" class="headerlink" title="二维池化层"></a>二维池化层</h3><p>池化层主要用于缓解卷积层对位置的过度敏感性。同卷积层一样，池化层每次对输入数据的一个固定形状窗口（又称池化窗口）中的元素计算输出，池化层直接计算池化窗口内元素的最大值或者平均值，该运算也分别叫做<code>最大池化</code>或<code>平均池化</code>。图6展示了池化窗口形状为$2\times 2$的最大池化。</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/CpF4XrYOE3Z2qKc.png" alt="2x2 的最大池化"></p><p>图6 池化窗口形状为 2 x 2 的最大池化</p><p>二维平均池化的工作原理与二维最大池化类似，但将最大运算符替换成平均运算符。池化窗口形状为$p \times q$的池化层称为$p \times q$池化层，其中的池化运算叫作$p \times q$池化。</p><p>池化层也可以在输入的高和宽两侧填充并调整窗口的移动步幅来改变输出形状。池化层填充和步幅与卷积层填充和步幅的工作机制一样。</p><p>在处理多通道输入数据时，池化层对每个输入通道分别池化，但不会像卷积层那样将各通道的结果按通道相加。这意味着池化层的<code>输出通道数与输入通道数相等</code>。</p><ul><li><code>卷积与池化的区别</code>：<br>  池化层没有需要学习的特征<br>  卷积层用来学习识别某一种特征<br>  池化层的主要作用是降维<br>  卷积层当设置步长 &gt;1 时也可以用来完成降维的工作</li></ul><h3 id="池化层的简洁实现"><a href="#池化层的简洁实现" class="headerlink" title="池化层的简洁实现"></a>池化层的简洁实现</h3><p>我们使用Pytorch中的<code>nn.MaxPool2d</code>实现最大池化层，关注以下构造函数参数：</p><ul><li><code>kernel_size</code> – the size of the window to take a max over</li><li><code>stride</code> – the stride of the window. Default value is kernel_size</li><li><code>padding</code> – implicit zero padding to be added on both sides</li></ul><p><code>forward</code>函数的参数为一个四维张量，形状为$(N, C, H_{in}, W_{in})$，返回值也是一个四维张量，形状为$(N, C, H_{out}, W_{out})$，其中$N$是批量大小，$C, H, W$分别表示通道数、高度、宽度。</p><p>代码讲解</p><pre><code class="lang-python">X = torch.arange(32, dtype=torch.float32).view(1, 2, 4, 4)pool2d = nn.MaxPool2d(kernel_size=3, padding=1, stride=(2, 1))Y = pool2d(X)print(X)print(Y)</code></pre><pre><code>tensor([[[[ 0.,  1.,  2.,  3.],          [ 4.,  5.,  6.,  7.],          [ 8.,  9., 10., 11.],          [12., 13., 14., 15.]],         [[16., 17., 18., 19.],          [20., 21., 22., 23.],          [24., 25., 26., 27.],          [28., 29., 30., 31.]]]])tensor([[[[ 5.,  6.,  7.,  7.],          [13., 14., 15., 15.]],         [[21., 22., 23., 23.],          [29., 30., 31., 31.]]]])</code></pre><p>平均池化层使用的是<code>nn.AvgPool2d</code>，使用方法与<code>nn.MaxPool2d</code>相同。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动手深度学习(八):机器翻译简介</title>
      <link href="/2020/02/20/dl-notes8-mach-trans/"/>
      <url>/2020/02/20/dl-notes8-mach-trans/</url>
      
        <content type="html"><![CDATA[<h1 id="机器翻译和数据集"><a href="#机器翻译和数据集" class="headerlink" title="机器翻译和数据集"></a>机器翻译和数据集</h1><p>机器翻译（MT）：将一段文本从一种语言自动翻译为另一种语言，用神经网络解决这个问题通常称为神经机器翻译（NMT）。<br>主要特征：输出是单词<code>序列</code>而不是单个单词。 输出序列的长度可能与源序列的长度<code>不同</code>。</p><h2 id="机器翻译任务代码总结如下"><a href="#机器翻译任务代码总结如下" class="headerlink" title="机器翻译任务代码总结如下"></a>机器翻译任务代码总结如下</h2><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><ol><li><code>读取数据</code>，处理数据中的编码问题，并将无效的字符串删除</li><li><code>分词</code>，分词的目的就是将字符串转换成单词组成的列表。目前有很多现成的分词工具可以直接使用，也可以直接按照空格进行分词(不推荐，因为分词不是很准确)</li><li><code>建立词典</code>，将单词组成的列表编程单词id组成的列表，这里会得到如下几样东西<ol><li>去重后词典，及其中单词对应的索引列表</li><li>还可以得到给定索引找到其对应的单词的列表，以及给定单词得到对应索引的字典</li><li>原始语料所有词对应的词典索引的列表</li></ol></li><li>对数据进行<code>padding操作</code>。基于rnn的Seq2Seq模型，可以处理任意长度，“变长数据的读入”基于rnn的机器翻译也并不需要固定长度。要<code>padding</code>，是因为像tf、pytorch这些框架要求一个batch的数据必须长度相等，不然会报错；要<code>截断</code>，设置最大的数据长度是因为decode的时候达到这个长度我们就停止；再一个原因就是为了加快计算，不然的话为了单个特别长的数据，batch中的其他数据都补成这么长，很慢。</li><li><code>制作数据生成器</code>，但是需要注意的是对于翻译任务的数据格式，机器翻译的输入是一段文本序列，输出也是一段文本序列。</li></ol><h3 id="Seq2Seq模型的构建"><a href="#Seq2Seq模型的构建" class="headerlink" title="Seq2Seq模型的构建"></a>Seq2Seq模型的构建</h3><ol><li>先编码后解码的框架: 先对输入序列使用循环神经网络对他进行编码，编码成一个<code>向量</code>之后，再将编码得到的向量作为一个新的<code>解码循环神经网络</code>的隐藏状态的输入，进行解码，一次输出一个序列的元素，再将模型训练输出的序列元素与真实标签计算损失进行学习。</li><li>词嵌入: 一般情况下输入到编码网络中的数据不是一个<code>onehot</code>向量而是经过了编码之后的向量，比如由<code>word2vec</code>技术，让编码后的向量由更加丰富的含义。</li><li>在进行编码和解码的过程中数据都是以<code>时间步</code>展开，也就是(Seq_len,)这种形式的数据进行处理的</li><li>对于编码与解码的循环神经网络，可以通过控制隐藏层的<code>层数</code>及每一层隐藏层神经元的<code>数量</code>来控制模型的复杂度</li><li>编码部分，RNN的用0初始化隐含状态，最后的输出主要是隐藏状态, 编码RNN输出的隐含状态认为是其对应的编码向量</li><li>解码器的整体形状与编码器是一样的，只不过解码器的模型的隐藏状态是<code>由编码器的输出的隐藏状态初始化</code>的。</li></ol><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><ol><li>解码器的输出是一个和词典维度相同的向量，其每个值对应与向量索引位置对应词的分数，一般是选择<code>分数最大</code>的那个词作为最终的输出。</li><li>在计算损失函数之前，要把<code>padding去掉</code>，因为padding的部分不参与计算</li></ol><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><ol><li>解码器在测试的时候需要将模型的输出作为下一个时间步的输入</li><li>Beam Search搜索算法。<ol><li>假设预测的时候词典的大小为3，内容为a,b,c. <code>beam size</code>为2，解码的时候过程如下</li><li>生成第一个词的时候，选择概率最大的两个词，假设为a,c.那么当前的两个序列就是a和c。</li><li>生成第二个词的时候，将当前序列a和c，分别与此表中的所有词进行组合，得到新的6个序列aa ab ac ca cb cc,计算每个序列的得分，并选择得分最高的2个序列，作为新的当前序列，假如为aa cb </li><li>后面不断重复这个过程，直到遇到结束符或者达到最大长度为止，最终输出得分最高的2个序列。</li></ol></li></ol><pre><code class="lang-python">import osos.listdir(&#39;/home/kesci/input/&#39;)</code></pre><pre><code>[&#39;fraeng6506&#39;, &#39;d2l9528&#39;, &#39;d2l6239&#39;]</code></pre><pre><code class="lang-python">import syssys.path.append(&#39;/home/kesci/input/d2l9528/&#39;)import collectionsimport d2limport zipfilefrom d2l.data.base import Vocabimport timeimport torchimport torch.nn as nnimport torch.nn.functional as Ffrom torch.utils import datafrom torch import optim</code></pre><h3 id="数据预处理-1"><a href="#数据预处理-1" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>将数据集清洗、转化为神经网络的输入minbatch</p><pre><code class="lang-python">with open(&#39;/home/kesci/input/fraeng6506/fra.txt&#39;, &#39;r&#39;) as f:      raw_text = f.read()print(raw_text[0:1000])</code></pre><pre><code>Go.    Va !    CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) &amp; #1158250 (Wittydev)Hi.    Salut !    CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) &amp; #509819 (Aiji)Hi.    Salut.    CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) &amp; #4320462 (gillux)Run!    Cours !    CC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) &amp; #906331 (sacredceltic)Run!    Courez !    CC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) &amp; #906332 (sacredceltic)Who?    Qui ?    CC-BY 2.0 (France) Attribution: tatoeba.org #2083030 (CK) &amp; #4366796 (gillux)Wow!    Ça alors !    CC-BY 2.0 (France) Attribution: tatoeba.org #52027 (Zifre) &amp; #374631 (zmoo)Fire!    Au feu !    CC-BY 2.0 (France) Attribution: tatoeba.org #1829639 (Spamster) &amp; #4627939 (sacredceltic)Help!    À l&#39;aide !    CC-BY 2.0 (France) Attribution: tatoeba.org #435084 (lukaszpp) &amp; #128430 (sysko)Jump.    Saute.    CC-BY 2.0 (France) Attribution: tatoeba.org #631038 (Shishir) &amp; #2416938 (Phoenix)Stop!    Ça suffit !    CC-BY 2.0 (France) Attribution: tato</code></pre><pre><code class="lang-python">def preprocess_raw(text):    text = text.replace(&#39;\u202f&#39;, &#39; &#39;).replace(&#39;\xa0&#39;, &#39; &#39;)    out = &#39;&#39;    for i, char in enumerate(text.lower()):        if char in (&#39;,&#39;, &#39;!&#39;, &#39;.&#39;) and i &gt; 0 and text[i-1] != &#39; &#39;:            out += &#39; &#39;        out += char    return outtext = preprocess_raw(raw_text)print(text[0:1000])</code></pre><pre><code>go .    va !    cc-by 2 .0 (france) attribution: tatoeba .org #2877272 (cm) &amp; #1158250 (wittydev)hi .    salut !    cc-by 2 .0 (france) attribution: tatoeba .org #538123 (cm) &amp; #509819 (aiji)hi .    salut .    cc-by 2 .0 (france) attribution: tatoeba .org #538123 (cm) &amp; #4320462 (gillux)run !    cours !    cc-by 2 .0 (france) attribution: tatoeba .org #906328 (papabear) &amp; #906331 (sacredceltic)run !    courez !    cc-by 2 .0 (france) attribution: tatoeba .org #906328 (papabear) &amp; #906332 (sacredceltic)who?    qui ?    cc-by 2 .0 (france) attribution: tatoeba .org #2083030 (ck) &amp; #4366796 (gillux)wow !    ça alors !    cc-by 2 .0 (france) attribution: tatoeba .org #52027 (zifre) &amp; #374631 (zmoo)fire !    au feu !    cc-by 2 .0 (france) attribution: tatoeba .org #1829639 (spamster) &amp; #4627939 (sacredceltic)help !    à l&#39;aide !    cc-by 2 .0 (france) attribution: tatoeba .org #435084 (lukaszpp) &amp; #128430 (sysko)jump .    saute .    cc-by 2 .0 (france) attribution: tatoeba .org #631038 (shishir) &amp; #2416938 (phoenix)stop !    ça suffit !    cc-b</code></pre><p>字符在计算机里是以编码的形式存在，我们通常所用的空格是 \x20 ，是在标准ASCII可见字符 0x20~0x7e 范围内。<br>而 \xa0 属于 latin1 （ISO/IEC_8859-1）中的扩展字符集字符，代表不间断空白符nbsp(non-breaking space)，超出gbk编码范围，是需要去除的特殊字符。再数据预处理的过程中，我们首先需要对数据进行清洗。</p><h3 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h3><p>字符串—-单词组成的列表</p><pre><code class="lang-python">num_examples = 50000source, target = [], []for i, line in enumerate(text.split(&#39;\n&#39;)):    if i &gt; num_examples:        break    parts = line.split(&#39;\t&#39;)    if len(parts) &gt;= 2:        source.append(parts[0].split(&#39; &#39;))        target.append(parts[1].split(&#39; &#39;))source[0:3], target[0:3]</code></pre><pre><code>([[&#39;go&#39;, &#39;.&#39;], [&#39;hi&#39;, &#39;.&#39;], [&#39;hi&#39;, &#39;.&#39;]], [[&#39;va&#39;, &#39;!&#39;], [&#39;salut&#39;, &#39;!&#39;], [&#39;salut&#39;, &#39;.&#39;]])</code></pre><pre><code class="lang-python">d2l.set_figsize()d2l.plt.hist([[len(l) for l in source], [len(l) for l in target]],label=[&#39;source&#39;, &#39;target&#39;])d2l.plt.legend(loc=&#39;upper right&#39;);</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/t19DZi4dB2XOPEn.png" alt="分词结果"></p><h3 id="建立词典"><a href="#建立词典" class="headerlink" title="建立词典"></a>建立词典</h3><p>单词组成的列表—-单词id组成的列表</p><pre><code class="lang-python">def build_vocab(tokens):    tokens = [token for line in tokens for token in line]    return d2l.data.base.Vocab(tokens, min_freq=3, use_special_tokens=True)src_vocab = build_vocab(source)  # 4 个特殊字符，pad 补充 bos句开始 eos句结束 unk未知字符len(src_vocab)</code></pre><pre><code>3789</code></pre><p><code>Vocab</code> 会进行词频统计, 从大到小排序<br><code>__len__</code> 返回长度<br><code>__getitem__</code> 返回单词对应的 id<br><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/3oBvEZdHYKXzwfg.png" alt="Vocab"></p><h3 id="载入数据集"><a href="#载入数据集" class="headerlink" title="载入数据集"></a>载入数据集</h3><p>进行 pad 保证句子长度是一样的, 一个 batch 使用的同一个长度</p><pre><code class="lang-python">def pad(line, max_len, padding_token):    if len(line) &gt; max_len:        return line[:max_len]    return line + [padding_token] * (max_len - len(line))pad(src_vocab[source[0]], 10, src_vocab.pad)</code></pre><pre><code>[38, 4, 0, 0, 0, 0, 0, 0, 0, 0]</code></pre><p>首尾标注 <code>bos</code> 和 <code>eos</code></p><pre><code class="lang-python">def build_array(lines, vocab, max_len, is_source):    lines = [vocab[line] for line in lines]    if not is_source:  # 如果是目标数据集，就要加一对首尾标注        lines = [[vocab.bos] + line + [vocab.eos] for line in lines]    array = torch.tensor([pad(line, max_len, vocab.pad) for line in lines])    valid_len = (array != vocab.pad).sum(1) #第一个维度    return array, valid_len</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/LW4YMRvUpCA9ul6.png" alt="TensorDataset"></p><pre><code class="lang-python">def load_data_nmt(batch_size, max_len): # This function is saved in d2l.    src_vocab, tgt_vocab = build_vocab(source), build_vocab(target)  # 先进行分词    src_array, src_valid_len = build_array(source, src_vocab, max_len, True)  # 构建源语言数组    tgt_array, tgt_valid_len = build_array(target, tgt_vocab, max_len, False) # 构建目标语言数组    train_data = data.TensorDataset(src_array, src_valid_len, tgt_array, tgt_valid_len)     # TensorDataset 利用 assert 判断 4 个参数是不是一一对应的    train_iter = data.DataLoader(train_data, batch_size, shuffle=True)    return src_vocab, tgt_vocab, train_iter</code></pre><p>X 是英语句子, Y 是法语句子</p><pre><code class="lang-python">src_vocab, tgt_vocab, train_iter = load_data_nmt(batch_size=2, max_len=8)for X, X_valid_len, Y, Y_valid_len, in train_iter:    print(&#39;X =&#39;, X.type(torch.int32), &#39;\nValid lengths for X =&#39;, X_valid_len,        &#39;\nY =&#39;, Y.type(torch.int32), &#39;\nValid lengths for Y =&#39;, Y_valid_len)    break</code></pre><pre><code>X = tensor([[   5,   24,    3,    4,    0,    0,    0,    0],        [  12, 1388,    7,    3,    4,    0,    0,    0]], dtype=torch.int32) Valid lengths for X = tensor([4, 5]) Y = tensor([[   1,   23,   46,    3,    3,    4,    2,    0],        [   1,   15,  137,   27, 4736,    4,    2,    0]], dtype=torch.int32) Valid lengths for Y = tensor([7, 7])</code></pre><h1 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a>Encoder-Decoder</h1><p> encoder：输入到隐藏状态<br> decoder：隐藏状态到输出</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/7aEyZcGDVS1zXPU.png" alt="Encoder-Decoder"></p><pre><code class="lang-python">class Encoder(nn.Module):    def __init__(self, **kwargs):        super(Encoder, self).__init__(**kwargs)    def forward(self, X, *args):        raise NotImplementedError</code></pre><pre><code class="lang-python">class Decoder(nn.Module):    def __init__(self, **kwargs):        super(Decoder, self).__init__(**kwargs)    def init_state(self, enc_outputs, *args):        raise NotImplementedError    def forward(self, X, state):        raise NotImplementedError</code></pre><pre><code class="lang-python">class EncoderDecoder(nn.Module):    def __init__(self, encoder, decoder, **kwargs):        super(EncoderDecoder, self).__init__(**kwargs)        self.encoder = encoder        self.decoder = decoder    def forward(self, enc_X, dec_X, *args):        enc_outputs = self.encoder(enc_X, *args)        dec_state = self.decoder.init_state(enc_outputs, *args)        return self.decoder(dec_X, dec_state)</code></pre><p>可以应用在对话系统、生成式任务中。</p><h1 id="Sequence-to-Sequence模型"><a href="#Sequence-to-Sequence模型" class="headerlink" title="Sequence to Sequence模型"></a>Sequence to Sequence模型</h1><h3 id="模型："><a href="#模型：" class="headerlink" title="模型："></a>模型：</h3><ul><li>训练（已知英语所对应的法语）</li><li>Encoder 是一个 RNN，可以是 LSTM 可以是 GRU</li><li>hidden state 就是 Encoder 得到的 ht，就是语义编码，也就是 Decoder 用来初始化的 $H_{-1}$</li><li>Decoder 就是一个生成语言模型，有了前几个生成后几个，类似于歌词生成<br><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/JL3VW7gznsGIStw.png" alt="生成"></li><li>预测（不知道对应的法语）</li></ul><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/YhcpTl6nQERLosu.png" alt="预测"></p><h3 id="具体结构："><a href="#具体结构：" class="headerlink" title="具体结构："></a>具体结构：</h3><ul><li>比如 apple 对应的 id，会把它翻译成<code>词向量</code>，代表 apple 的含义</li><li><code>dense()</code>多用于CNN网络搭建时搭建全连接层使用，在RNN网络搭建，特别是涉及到用RNN做分类时，dense()可以用作RNN<code>分类预测输出</code>, 每个输出也是一个分类问题</li><li><code>embedding层</code>的作用就是给每个单词，赋一个特定的词向量<br><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/5otbRSFCVqhZDUK.png" alt="具体结构"><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3></li></ul><pre><code class="lang-python">class Seq2SeqEncoder(d2l.Encoder):    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,                 dropout=0, **kwargs):        super(Seq2SeqEncoder, self).__init__(**kwargs)        self.num_hiddens=num_hiddens        self.num_layers=num_layers        self.embedding = nn.Embedding(vocab_size, embed_size) # embed_size 词向量维度，输入单词的形状        self.rnn = nn.LSTM(embed_size,num_hiddens, num_layers, dropout=dropout)    def begin_state(self, batch_size, device):  # 初始化        return [torch.zeros(size=(self.num_layers, batch_size, self.num_hiddens),  device=device),                torch.zeros(size=(self.num_layers, batch_size, self.num_hiddens),  device=device)]    def forward(self, X, *args):        X = self.embedding(X) # X shape: (batch_size, seq_len, embed_size)         # batch_size 有几句话，seq_len 一句话有几个单词，每个单词都变成了 embed_size 维的词向量        X = X.transpose(0, 1)  # RNN 第一个维度应该是时间（句子的顺序是第一个维度），所以把第 1、2 维度调换一下        # state = self.begin_state(X.shape[1], device=X.device)        out, state = self.rnn(X) # out 是每个单元的输出，state 是最后一个单元的输出        # out 的 shape 是 (seq_len, batch_size, num_hiddens).        # state 包含最后一个记忆细胞的状态，和隐层状态        # 在最后一步， state shape 是 (num_layers, batch_size, num_hiddens)        return out, state</code></pre><pre><code class="lang-python">encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8,num_hiddens=16, num_layers=2) #每个词用 8 维向量表示X = torch.zeros((4, 7),dtype=torch.long)  # 输入 4 句话，每句话 7 个单词output, state = encoder(X)output.shape, len(state), state[0].shape, state[1].shape  # len(state)=2 表示包含了记忆细胞和隐层状态两个部分</code></pre><pre><code>(torch.Size([7, 4, 16]), 2, torch.Size([2, 4, 16]), torch.Size([2, 4, 16]))</code></pre><h1 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h1><pre><code class="lang-python">class Seq2SeqDecoder(d2l.Decoder):    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,                 dropout=0, **kwargs):        super(Seq2SeqDecoder, self).__init__(**kwargs)        self.embedding = nn.Embedding(vocab_size, embed_size)        self.rnn = nn.LSTM(embed_size,num_hiddens, num_layers, dropout=dropout)        self.dense = nn.Linear(num_hiddens,vocab_size) # dense 用于输出结果        # 通过全连接层把16个隐藏单元映射到100个单词，在100 维度的向量里，选出评分最高的，作为结果输出    def init_state(self, enc_outputs, *args):        return enc_outputs[1]  # 用 Encoder 的输出作初始化    def forward(self, X, state):        X = self.embedding(X).transpose(0, 1)        out, state = self.rnn(X, state)        # Make the batch to be the first dimension to simplify loss computation.        out = self.dense(out).transpose(0, 1)        return out, state</code></pre><pre><code class="lang-python">decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8,num_hiddens=16, num_layers=2)state = decoder.init_state(encoder(X))out, state = decoder(X, state)out.shape, len(state), state[0].shape, state[1].shape  # out 4*7*10 10 代表单词表大小，在 10 个得分里选最高的</code></pre><pre><code>(torch.Size([4, 7, 10]), 2, torch.Size([2, 4, 16]), torch.Size([2, 4, 16]))</code></pre><h3 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h3><p>计算有效长度, 忽略 padding</p><pre><code class="lang-python">def SequenceMask(X, X_len,value=0):      # X 一个 batch 的输入，X_len 输入原本的有效长度（由于有 pad）    maxlen = X.size(1)    mask = torch.arange(maxlen)[None, :].to(X_len.device) &lt; X_len[:, None] # len 以后的部分都用 value 填充    X[~mask]=value    return X</code></pre><pre><code class="lang-python">X = torch.tensor([[1,2,3], [4,5,6]])SequenceMask(X,torch.tensor([1,2]))  # 第一句有效长度为 1，第二句有效长度为 2</code></pre><pre><code>tensor([[1, 0, 0],        [4, 5, 0]])</code></pre><pre><code class="lang-python">X = torch.ones((2,3, 4))SequenceMask(X, torch.tensor([1,2]),value=-1)</code></pre><pre><code>tensor([[[ 1.,  1.,  1.,  1.],         [-1., -1., -1., -1.],         [-1., -1., -1., -1.]],        [[ 1.,  1.,  1.,  1.],         [ 1.,  1.,  1.,  1.],         [-1., -1., -1., -1.]]])</code></pre><p>定义交叉熵损失函数, 继承<code>CrossEntropyLoss</code></p><pre><code class="lang-python">class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):    # pred shape: (batch_size, seq_len, vocab_size)    # label shape: (batch_size, seq_len)    # valid_length shape: (batch_size, )    def forward(self, pred, label, valid_length):        # the sample weights shape should be (batch_size, seq_len)        weights = torch.ones_like(label)        weights = SequenceMask(weights, valid_length).float()  # padding 部分的损失变成 0，保留有效长度部分的存世        self.reduction=&#39;none&#39;        output=super(MaskedSoftmaxCELoss, self).forward(pred.transpose(1,2), label)        return (output*weights).mean(dim=1)</code></pre><pre><code class="lang-python">loss = MaskedSoftmaxCELoss()loss(torch.ones((3, 4, 10)), torch.ones((3,4),dtype=torch.long), torch.tensor([4,3,0])) # label 3个句子，4 个正确单词； 有效长度[4, 3, 0]</code></pre><pre><code>tensor([2.3026, 1.7269, 0.0000])</code></pre><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><pre><code class="lang-python">def train_ch7(model, data_iter, lr, num_epochs, device):  # Saved in d2l 如果device是GPU，下面的device都要设定    model.to(device)    optimizer = optim.Adam(model.parameters(), lr=lr)    loss = MaskedSoftmaxCELoss()    tic = time.time()    for epoch in range(1, num_epochs+1):        l_sum, num_tokens_sum = 0.0, 0.0  # loss 的总和，单词数量的总和        for batch in data_iter:            optimizer.zero_grad()            X, X_vlen, Y, Y_vlen = [x.to(device) for x in batch]            Y_input, Y_label, Y_vlen = Y[:,:-1], Y[:,1:], Y_vlen-1 # Y_input decoder的输入            Y_hat, _ = model(X, Y_input, X_vlen, Y_vlen)            l = loss(Y_hat, Y_label, Y_vlen).sum()            l.backward()            with torch.no_grad():                d2l.grad_clipping_nn(model, 5, device)  #梯度裁剪            num_tokens = Y_vlen.sum().item()            optimizer.step()            l_sum += l.sum().item()            num_tokens_sum += num_tokens        if epoch % 50 == 0:            print(&quot;epoch {0:4d},loss {1:.3f}, time {2:.1f} sec&quot;.format(                   epoch, (l_sum/num_tokens_sum), time.time()-tic))            tic = time.time()</code></pre><pre><code class="lang-python">embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.0batch_size, num_examples, max_len = 64, 1e3, 10lr, num_epochs, ctx = 0.005, 300, d2l.try_gpu()src_vocab, tgt_vocab, train_iter = d2l.load_data_nmt(    batch_size, max_len,num_examples)encoder = Seq2SeqEncoder(    len(src_vocab), embed_size, num_hiddens, num_layers, dropout)decoder = Seq2SeqDecoder(    len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)model = d2l.EncoderDecoder(encoder, decoder)train_ch7(model, train_iter, lr, num_epochs, ctx)</code></pre><pre><code>epoch   50,loss 0.093, time 38.2 secepoch  100,loss 0.046, time 37.9 secepoch  150,loss 0.032, time 36.8 secepoch  200,loss 0.027, time 37.5 secepoch  250,loss 0.026, time 37.8 secepoch  300,loss 0.025, time 37.3 sec</code></pre><h3 id="测试-1"><a href="#测试-1" class="headerlink" title="测试"></a>测试</h3><pre><code class="lang-python">def translate_ch7(model, src_sentence, src_vocab, tgt_vocab, max_len, device):    src_tokens = src_vocab[src_sentence.lower().split(&#39; &#39;)]    src_len = len(src_tokens)    if src_len &lt; max_len:        src_tokens += [src_vocab.pad] * (max_len - src_len)  # 进行 padding    enc_X = torch.tensor(src_tokens, device=device)    enc_valid_length = torch.tensor([src_len], device=device)    # use expand_dim to add the batch_size dimension.    enc_outputs = model.encoder(enc_X.unsqueeze(dim=0), enc_valid_length) # unsqueeze 增加一个维度（因为有多个 batch_size)    dec_state = model.decoder.init_state(enc_outputs, enc_valid_length) # encoder 的输出作为 dec 的输入初始化    dec_X = torch.tensor([tgt_vocab.bos], device=device).unsqueeze(dim=0) # 设为 bos 表示开始    predict_tokens = []    for _ in range(max_len):        Y, dec_state = model.decoder(dec_X, dec_state) # 当前 RNN 单元的 output Y，给下一个单元的 dec_state        dec_X = Y.argmax(dim=2)  # 得分最高的单词，作为下一个单元的输入        py = dec_X.squeeze(dim=0).int().item()        if py == tgt_vocab.eos:  # 结束则跳出循环            break        predict_tokens.append(py)    return &#39; &#39;.join(tgt_vocab.to_tokens(predict_tokens))</code></pre><pre><code class="lang-python">for sentence in [&#39;Go .&#39;, &#39;Wow !&#39;, &quot;I&#39;m OK .&quot;, &#39;I won !&#39;]:    print(sentence + &#39; =&gt; &#39; + translate_ch7(        model, sentence, src_vocab, tgt_vocab, max_len, ctx))</code></pre><pre><code>Go . =&gt; va !Wow ! =&gt; &lt;unk&gt; !I&#39;m OK . =&gt; ça va .I won ! =&gt; j&#39;ai gagné !</code></pre><h2 id="Beam-Search-集束搜索"><a href="#Beam-Search-集束搜索" class="headerlink" title="Beam Search 集束搜索"></a>Beam Search 集束搜索</h2><p>简单greedy search：（贪心）只考虑了局部最优解</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/emJlxcIbrCVTS1o.png" alt="简单贪心"></p><p>维特比算法：选择整体分数最高的句子（搜索空间太大）<br>集束搜索：每次不只选一个，选择 top n，增加备选项但又不全局搜索</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/L5aI1irfAKm2znq.png" alt="集束搜索"></p><h2 id="练习题"><a href="#练习题" class="headerlink" title="练习题"></a>练习题</h2><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/BAHTqRNlPsiMdyr.png" alt="机器翻译-练习题"></p><ul><li>训练时下一个单词是已知的, 预测时需要作为下一个单词生成的输入</li><li>每个 batch 之内, size 是给定的, 句子长度都是一样的</li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习, NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动手深度学习(五):梯度消失、梯度爆炸</title>
      <link href="/2020/02/19/dl-notes5-grad/"/>
      <url>/2020/02/19/dl-notes5-grad/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动手深度学习(四):过拟合与欠拟合</title>
      <link href="/2020/02/14/dl-notes4-ov-und-fit/"/>
      <url>/2020/02/14/dl-notes4-ov-und-fit/</url>
      
        <content type="html"><![CDATA[<h1 id="过拟合、欠拟合及其解决方案"><a href="#过拟合、欠拟合及其解决方案" class="headerlink" title="过拟合、欠拟合及其解决方案"></a>过拟合、欠拟合及其解决方案</h1><ol><li>过拟合、欠拟合的概念</li><li>权重衰减</li><li>丢弃法</li></ol><h1 id="模型选择、过拟合和欠拟合"><a href="#模型选择、过拟合和欠拟合" class="headerlink" title="模型选择、过拟合和欠拟合"></a>模型选择、过拟合和欠拟合</h1><h2 id="训练误差和泛化误差"><a href="#训练误差和泛化误差" class="headerlink" title="训练误差和泛化误差"></a>训练误差和泛化误差</h2><p>在解释上述现象之前，我们需要区分训练误差（training error）和泛化误差（generalization error）。通俗来讲，前者指模型在训练数据集上表现出的误差，后者指模型在任意一个测试数据样本上表现出的误差的期望，并常常通过测试数据集上的误差来近似。计算训练误差和泛化误差可以使用之前介绍过的损失函数，例如线性回归用到的平方损失函数和softmax回归用到的交叉熵损失函数。</p><p>机器学习模型应关注降低泛化误差。</p><h2 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h2><h3 id="验证数据集"><a href="#验证数据集" class="headerlink" title="验证数据集"></a>验证数据集</h3><p>从严格意义上讲，测试集只能在所有超参数和模型参数选定后使用一次。不可以使用测试数据选择模型，如调参。由于无法从训练误差估计泛化误差，因此也不应只依赖训练数据选择模型。鉴于此，我们可以预留一部分在训练数据集和测试数据集以外的数据来进行模型选择。这部分数据被称为验证数据集，简称验证集（validation set）。例如，我们可以从给定的训练集中随机选取一小部分作为验证集，而将剩余部分作为真正的训练集。</p><h3 id="K折交叉验证"><a href="#K折交叉验证" class="headerlink" title="K折交叉验证"></a>K折交叉验证</h3><p>由于验证数据集不参与模型训练，当训练数据不够用时，预留大量的验证数据显得太奢侈。一种改善的方法是K折交叉验证（K-fold cross-validation）。在K折交叉验证中，我们把原始训练数据集分割成K个不重合的子数据集，然后我们做K次模型训练和验证。每一次，我们使用一个子数据集验证模型，并使用其他K-1个子数据集来训练模型。在这K次训练和验证中，每次用来验证模型的子数据集都不同。最后，我们对这K次训练误差和验证误差分别求平均。</p><h2 id="过拟合和欠拟合"><a href="#过拟合和欠拟合" class="headerlink" title="过拟合和欠拟合"></a>过拟合和欠拟合</h2><p>接下来，我们将探究模型训练中经常出现的两类典型问题：</p><ul><li>一类是模型无法得到较低的训练误差，我们将这一现象称作欠拟合（underfitting）；</li><li>另一类是模型的训练误差远小于它在测试数据集上的误差，我们称该现象为过拟合（overfitting）。<br>在实践中，我们要尽可能同时应对欠拟合和过拟合。虽然有很多因素可能导致这两种拟合问题，在这里我们重点讨论两个因素：模型复杂度和训练数据集大小。</li></ul><h3 id="模型复杂度"><a href="#模型复杂度" class="headerlink" title="模型复杂度"></a>模型复杂度</h3><p>为了解释模型复杂度，我们以多项式函数拟合为例。给定一个由标量数据特征$x$和对应的标量标签$y$组成的训练数据集，多项式函数拟合的目标是找一个$K$阶多项式函数</p><script type="math/tex; mode=display"> \hat{y} = b + \sum_{k=1}^K x^k w_k</script><p>来近似 $y$。在上式中，$w_k$是模型的权重参数，$b$是偏差参数。与线性回归相同，多项式函数拟合也使用平方损失函数。特别地，一阶多项式函数拟合又叫线性函数拟合。</p><p>给定训练数据集，模型复杂度和误差之间的关系：</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/upload/image/q5jc27wxoj.png?imageView2/0/w/960/h/960" alt="Image Name"></p><h3 id="训练数据集大小"><a href="#训练数据集大小" class="headerlink" title="训练数据集大小"></a>训练数据集大小</h3><p>影响欠拟合和过拟合的另一个重要因素是训练数据集的大小。一般来说，如果训练数据集中样本数过少，特别是比模型参数数量（按元素计）更少时，过拟合更容易发生。此外，泛化误差不会随训练数据集里样本数量增加而增大。因此，在计算资源允许的范围之内，我们通常希望训练数据集大一些，特别是在模型复杂度较高时，例如层数较多的深度学习模型。</p><h1 id="多项式函数拟合实验"><a href="#多项式函数拟合实验" class="headerlink" title="多项式函数拟合实验"></a>多项式函数拟合实验</h1><pre><code class="lang-python">%matplotlib inlineimport torchimport numpy as npimport syssys.path.append(&quot;/home/kesci/input&quot;)import d2lzh1981 as d2lprint(torch.__version__)</code></pre><pre><code>1.3.0</code></pre><h2 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h2><pre><code class="lang-python">n_train, n_test, true_w, true_b = 100, 100, [1.2, -3.4, 5.6], 5features = torch.randn((n_train + n_test, 1))poly_features = torch.cat((features, torch.pow(features, 2), torch.pow(features, 3)), 1) labels = (true_w[0] * poly_features[:, 0] + true_w[1] * poly_features[:, 1]          + true_w[2] * poly_features[:, 2] + true_b)labels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float)</code></pre><pre><code class="lang-python">features[:2], poly_features[:2], labels[:2]</code></pre><pre><code>(tensor([[-0.8589],         [-0.2534]]), tensor([[-0.8589,  0.7377, -0.6335],         [-0.2534,  0.0642, -0.0163]]), tensor([-2.0794,  4.4039]))</code></pre><h2 id="定义、训练和测试模型"><a href="#定义、训练和测试模型" class="headerlink" title="定义、训练和测试模型"></a>定义、训练和测试模型</h2><pre><code class="lang-python">def semilogy(x_vals, y_vals, x_label, y_label, x2_vals=None, y2_vals=None,             legend=None, figsize=(3.5, 2.5)):    # d2l.set_figsize(figsize)    d2l.plt.xlabel(x_label)    d2l.plt.ylabel(y_label)    d2l.plt.semilogy(x_vals, y_vals)    if x2_vals and y2_vals:        d2l.plt.semilogy(x2_vals, y2_vals, linestyle=&#39;:&#39;)        d2l.plt.legend(legend)</code></pre><pre><code class="lang-python">num_epochs, loss = 100, torch.nn.MSELoss()def fit_and_plot(train_features, test_features, train_labels, test_labels):    # 初始化网络模型    net = torch.nn.Linear(train_features.shape[-1], 1)    # 通过Linear文档可知，pytorch已经将参数初始化了，所以我们这里就不手动初始化了    # 设置批量大小    batch_size = min(10, train_labels.shape[0])        dataset = torch.utils.data.TensorDataset(train_features, train_labels)      # 设置数据集    train_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True) # 设置获取数据方式    optimizer = torch.optim.SGD(net.parameters(), lr=0.01)                      # 设置优化函数，使用的是随机梯度下降优化    train_ls, test_ls = [], []    for _ in range(num_epochs):        for X, y in train_iter:                                                 # 取一个批量的数据            l = loss(net(X), y.view(-1, 1))                                     # 输入到网络中计算输出，并和标签比较求得损失函数            optimizer.zero_grad()                                               # 梯度清零，防止梯度累加干扰优化            l.backward()                                                        # 求梯度            optimizer.step()                                                    # 迭代优化函数，进行参数优化        train_labels = train_labels.view(-1, 1)        test_labels = test_labels.view(-1, 1)        train_ls.append(loss(net(train_features), train_labels).item())         # 将训练损失保存到train_ls中        test_ls.append(loss(net(test_features), test_labels).item())            # 将测试损失保存到test_ls中    print(&#39;final epoch: train loss&#39;, train_ls[-1], &#39;test loss&#39;, test_ls[-1])        semilogy(range(1, num_epochs + 1), train_ls, &#39;epochs&#39;, &#39;loss&#39;,             range(1, num_epochs + 1), test_ls, [&#39;train&#39;, &#39;test&#39;])    print(&#39;weight:&#39;, net.weight.data,          &#39;\nbias:&#39;, net.bias.data)</code></pre><h2 id="三阶多项式函数拟合（正常）"><a href="#三阶多项式函数拟合（正常）" class="headerlink" title="三阶多项式函数拟合（正常）"></a>三阶多项式函数拟合（正常）</h2><pre><code class="lang-python">fit_and_plot(poly_features[:n_train, :], poly_features[n_train:, :], labels[:n_train], labels[n_train:])</code></pre><pre><code>final epoch: train loss 8887.298828125 test loss 1145.94287109375weight: tensor([[-8.5120, 19.0351, 12.8616]]) bias: tensor([-5.4607])</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/CD685B472B744329A1CFC47C9F0B5E89/q5jf5azjcn.png"></p><h2 id="线性函数拟合（欠拟合）"><a href="#线性函数拟合（欠拟合）" class="headerlink" title="线性函数拟合（欠拟合）"></a>线性函数拟合（欠拟合）</h2><pre><code class="lang-python">fit_and_plot(features[:n_train, :], features[n_train:, :], labels[:n_train], labels[n_train:])</code></pre><pre><code>final epoch: train loss 781.689453125 test loss 329.79852294921875weight: tensor([[26.8753]]) bias: tensor([6.1426])</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/33AD626DA0B94DB7A28D47697312B45D/q5jf5al2tv.png"></p><h2 id="训练样本不足（过拟合）"><a href="#训练样本不足（过拟合）" class="headerlink" title="训练样本不足（过拟合）"></a>训练样本不足（过拟合）</h2><pre><code class="lang-python">fit_and_plot(poly_features[0:2, :], poly_features[n_train:, :], labels[0:2], labels[n_train:])</code></pre><pre><code>final epoch: train loss 6.23520565032959 test loss 409.9844665527344weight: tensor([[ 0.9729, -0.9612,  0.7259]]) bias: tensor([1.6334])</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/AB13F65A70A9484788F8004E427EC290/q5jf5bd11u.png"></p><h1 id="权重衰减"><a href="#权重衰减" class="headerlink" title="权重衰减"></a>权重衰减</h1><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>权重衰减等价于 $L_2$ 范数正则化（regularization）。正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常用手段。</p><h2 id="L2-范数正则化（regularization）"><a href="#L2-范数正则化（regularization）" class="headerlink" title="L2 范数正则化（regularization）"></a>L2 范数正则化（regularization）</h2><p>$L_2$范数正则化在模型原损失函数基础上添加$L_2$范数惩罚项，从而得到训练所需要最小化的函数。$L_2$范数惩罚项指的是模型权重参数每个元素的平方和与一个正的常数的乘积。以线性回归中的线性回归损失函数为例</p><script type="math/tex; mode=display"> \ell(w_1, w_2, b) = \frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right)^2</script><p>其中$w_1, w_2$是权重参数，$b$是偏差参数，样本$i$的输入为$x_1^{(i)}, x_2^{(i)}$，标签为$y^{(i)}$，样本数为$n$。将权重参数用向量$\boldsymbol{w} = [w_1, w_2]$表示，带有$L_2$范数惩罚项的新损失函数为</p><script type="math/tex; mode=display">\ell(w_1, w_2, b) + \frac{\lambda}{2n} |\boldsymbol{w}|^2,</script><p>其中超参数$\lambda &gt; 0$。当权重参数均为0时，惩罚项最小。当$\lambda$较大时，惩罚项在损失函数中的比重较大，这通常会使学到的权重参数的元素较接近0。当$\lambda$设为0时，惩罚项完全不起作用。上式中$L_2$范数平方$|\boldsymbol{w}|^2$展开后得到$w_1^2 + w_2^2$。<br>有了$L_2$范数惩罚项后，在小批量随机梯度下降中，我们将线性回归一节中权重$w_1$和$w_2$的迭代方式更改为</p><script type="math/tex; mode=display"> \begin{aligned} w_1 &\leftarrow \left(1- \frac{\eta\lambda}{|\mathcal{B}|} \right)w_1 - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_1^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right),\\ w_2 &\leftarrow \left(1- \frac{\eta\lambda}{|\mathcal{B}|} \right)w_2 - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_2^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right). \end{aligned}</script><p>可见，$L_2$范数正则化令权重$w_1$和$w_2$先自乘小于1的数，再减去不含惩罚项的梯度。因此，$L_2$范数正则化又叫权重衰减。权重衰减通过惩罚绝对值较大的模型参数为需要学习的模型增加了限制，这可能对过拟合有效。</p><h2 id="高维线性回归实验从零开始的实现"><a href="#高维线性回归实验从零开始的实现" class="headerlink" title="高维线性回归实验从零开始的实现"></a>高维线性回归实验从零开始的实现</h2><p>下面，我们以高维线性回归为例来引入一个过拟合问题，并使用权重衰减来应对过拟合。设数据样本特征的维度为$p$。对于训练数据集和测试数据集中特征为$x_1, x_2, \ldots, x_p$的任一样本，我们使用如下的线性函数来生成该样本的标签：</p><script type="math/tex; mode=display"> y = 0.05 + \sum_{i = 1}^p 0.01x_i + \epsilon</script><p>其中噪声项$\epsilon$服从均值为0、标准差为0.01的正态分布。为了较容易地观察过拟合，我们考虑高维线性回归问题，如设维度$p=200$；同时，我们特意把训练数据集的样本数设低，如20。</p><pre><code class="lang-python">%matplotlib inlineimport torchimport torch.nn as nnimport numpy as npimport syssys.path.append(&quot;/home/kesci/input&quot;)import d2lzh1981 as d2lprint(torch.__version__)</code></pre><pre><code>1.3.0</code></pre><h2 id="初始化模型参数-1"><a href="#初始化模型参数-1" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h2><p>与前面观察过拟合和欠拟合现象的时候相似，在这里不再解释。</p><pre><code class="lang-python">n_train, n_test, num_inputs = 20, 100, 200true_w, true_b = torch.ones(num_inputs, 1) * 0.01, 0.05features = torch.randn((n_train + n_test, num_inputs))labels = torch.matmul(features, true_w) + true_blabels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float)train_features, test_features = features[:n_train, :], features[n_train:, :]train_labels, test_labels = labels[:n_train], labels[n_train:]</code></pre><pre><code class="lang-python"># 定义参数初始化函数，初始化模型参数并且附上梯度def init_params():    w = torch.randn((num_inputs, 1), requires_grad=True)    b = torch.zeros(1, requires_grad=True)    return [w, b]</code></pre><h2 id="定义L2范数惩罚项"><a href="#定义L2范数惩罚项" class="headerlink" title="定义L2范数惩罚项"></a>定义L2范数惩罚项</h2><pre><code class="lang-python">def l2_penalty(w):    return (w**2).sum() / 2</code></pre><h2 id="定义训练和测试"><a href="#定义训练和测试" class="headerlink" title="定义训练和测试"></a>定义训练和测试</h2><pre><code class="lang-python">batch_size, num_epochs, lr = 1, 100, 0.003net, loss = d2l.linreg, d2l.squared_lossdataset = torch.utils.data.TensorDataset(train_features, train_labels)train_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True)def fit_and_plot(lambd):    w, b = init_params()    train_ls, test_ls = [], []    for _ in range(num_epochs):        for X, y in train_iter:            # 添加了L2范数惩罚项            l = loss(net(X, w, b), y) + lambd * l2_penalty(w)            l = l.sum()            if w.grad is not None:                w.grad.data.zero_()                b.grad.data.zero_()            l.backward()            d2l.sgd([w, b], lr, batch_size)        train_ls.append(loss(net(train_features, w, b), train_labels).mean().item())        test_ls.append(loss(net(test_features, w, b), test_labels).mean().item())    d2l.semilogy(range(1, num_epochs + 1), train_ls, &#39;epochs&#39;, &#39;loss&#39;,                 range(1, num_epochs + 1), test_ls, [&#39;train&#39;, &#39;test&#39;])    print(&#39;L2 norm of w:&#39;, w.norm().item())</code></pre><h2 id="观察过拟合"><a href="#观察过拟合" class="headerlink" title="观察过拟合"></a>观察过拟合</h2><pre><code class="lang-python">fit_and_plot(lambd=0)</code></pre><pre><code>L2 norm of w: 11.6444091796875</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/C27406AAA0FD41C6801D55ED4B25D5EA/q5jf5cs7lp.svg"></p><h2 id="使用权重衰减"><a href="#使用权重衰减" class="headerlink" title="使用权重衰减"></a>使用权重衰减</h2><pre><code class="lang-python">fit_and_plot(lambd=3)</code></pre><pre><code>L2 norm of w: 0.04063604772090912</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/0770D8C23B8144C59D13D24390E471F0/q5jf5d4mwp.svg"></p><h2 id="简洁实现"><a href="#简洁实现" class="headerlink" title="简洁实现"></a>简洁实现</h2><pre><code class="lang-python">def fit_and_plot_pytorch(wd):    # 对权重参数衰减。权重名称一般是以weight结尾    net = nn.Linear(num_inputs, 1)    nn.init.normal_(net.weight, mean=0, std=1)    nn.init.normal_(net.bias, mean=0, std=1)    optimizer_w = torch.optim.SGD(params=[net.weight], lr=lr, weight_decay=wd) # 对权重参数衰减    optimizer_b = torch.optim.SGD(params=[net.bias], lr=lr)  # 不对偏差参数衰减    train_ls, test_ls = [], []    for _ in range(num_epochs):        for X, y in train_iter:            l = loss(net(X), y).mean()            optimizer_w.zero_grad()            optimizer_b.zero_grad()            l.backward()            # 对两个optimizer实例分别调用step函数，从而分别更新权重和偏差            optimizer_w.step()            optimizer_b.step()        train_ls.append(loss(net(train_features), train_labels).mean().item())        test_ls.append(loss(net(test_features), test_labels).mean().item())    d2l.semilogy(range(1, num_epochs + 1), train_ls, &#39;epochs&#39;, &#39;loss&#39;,                 range(1, num_epochs + 1), test_ls, [&#39;train&#39;, &#39;test&#39;])    print(&#39;L2 norm of w:&#39;, net.weight.data.norm().item())</code></pre><pre><code class="lang-python">fit_and_plot_pytorch(0)</code></pre><pre><code>L2 norm of w: 13.361410140991211</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/525D01167F0E40509495588D6B0A0FB9/q5jf5e5i21.svg"></p><pre><code class="lang-python">fit_and_plot_pytorch(3)</code></pre><pre><code>L2 norm of w: 0.051789578050374985</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/3FAACA854B9545A8ADADDEB6EE17A680/q5jf5fa51u.svg"></p><h1 id="丢弃法"><a href="#丢弃法" class="headerlink" title="丢弃法"></a>丢弃法</h1><p>多层感知机中神经网络图描述了一个单隐藏层的多层感知机。其中输入个数为4，隐藏单元个数为5，且隐藏单元$h_i$（$i=1, \ldots, 5$）的计算表达式为</p><script type="math/tex; mode=display"> h_i = \phi\left(x_1 w_{1i} + x_2 w_{2i} + x_3 w_{3i} + x_4 w_{4i} + b_i\right)</script><p>这里$\phi$是激活函数，$x_1, \ldots, x_4$是输入，隐藏单元$i$的权重参数为$w_{1i}, \ldots, w_{4i}$，偏差参数为$b_i$。当对该隐藏层使用丢弃法时，该层的隐藏单元将有一定概率被丢弃掉。设丢弃概率为$p$，那么有$p$的概率$h_i$会被清零，有$1-p$的概率$h_i$会除以$1-p$做拉伸。丢弃概率是丢弃法的超参数。具体来说，设随机变量$\xi_i$为0和1的概率分别为$p$和$1-p$。使用丢弃法时我们计算新的隐藏单元$h_i’$</p><script type="math/tex; mode=display"> h_i' = \frac{\xi_i}{1-p} h_i</script><p>由于$E(\xi_i) = 1-p$，因此</p><script type="math/tex; mode=display"> E(h_i') = \frac{E(\xi_i)}{1-p}h_i = h_i</script><p>即丢弃法不改变其输入的期望值。让我们对之前多层感知机的神经网络中的隐藏层使用丢弃法，一种可能的结果如图所示，其中$h_2$和$h_5$被清零。这时输出值的计算不再依赖$h_2$和$h_5$，在反向传播时，与这两个隐藏单元相关的权重的梯度均为0。由于在训练中隐藏层神经元的丢弃是随机的，即$h_1, \ldots, h_5$都有可能被清零，输出层的计算无法过度依赖$h_1, \ldots, h_5$中的任一个，从而在训练模型时起到正则化的作用，并可以用来应对过拟合。在测试模型时，我们为了拿到更加确定性的结果，一般不使用丢弃法</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/upload/image/q5jd69in3m.png?imageView2/0/w/960/h/960" alt="Image Name"></p><h2 id="丢弃法从零开始的实现"><a href="#丢弃法从零开始的实现" class="headerlink" title="丢弃法从零开始的实现"></a>丢弃法从零开始的实现</h2><pre><code class="lang-python">%matplotlib inlineimport torchimport torch.nn as nnimport numpy as npimport syssys.path.append(&quot;/home/kesci/input&quot;)import d2lzh1981 as d2lprint(torch.__version__)</code></pre><pre><code>1.3.0</code></pre><pre><code class="lang-python">def dropout(X, drop_prob):    X = X.float()    assert 0 &lt;= drop_prob &lt;= 1    keep_prob = 1 - drop_prob    # 这种情况下把全部元素都丢弃    if keep_prob == 0:        return torch.zeros_like(X)    mask = (torch.rand(X.shape) &lt; keep_prob).float()    return mask * X / keep_prob</code></pre><pre><code class="lang-python">X = torch.arange(16).view(2, 8)dropout(X, 0)</code></pre><pre><code>tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],        [ 8.,  9., 10., 11., 12., 13., 14., 15.]])</code></pre><pre><code class="lang-python">dropout(X, 0.5)</code></pre><pre><code>tensor([[ 0.,  0.,  0.,  6.,  8., 10.,  0., 14.],        [ 0.,  0., 20.,  0.,  0.,  0., 28.,  0.]])</code></pre><pre><code class="lang-python">dropout(X, 1.0)</code></pre><pre><code>tensor([[0., 0., 0., 0., 0., 0., 0., 0.],        [0., 0., 0., 0., 0., 0., 0., 0.]])</code></pre><pre><code class="lang-python"># 参数的初始化num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256W1 = torch.tensor(np.random.normal(0, 0.01, size=(num_inputs, num_hiddens1)), dtype=torch.float, requires_grad=True)b1 = torch.zeros(num_hiddens1, requires_grad=True)W2 = torch.tensor(np.random.normal(0, 0.01, size=(num_hiddens1, num_hiddens2)), dtype=torch.float, requires_grad=True)b2 = torch.zeros(num_hiddens2, requires_grad=True)W3 = torch.tensor(np.random.normal(0, 0.01, size=(num_hiddens2, num_outputs)), dtype=torch.float, requires_grad=True)b3 = torch.zeros(num_outputs, requires_grad=True)params = [W1, b1, W2, b2, W3, b3]</code></pre><pre><code class="lang-python">drop_prob1, drop_prob2 = 0.2, 0.5def net(X, is_training=True):    X = X.view(-1, num_inputs)    H1 = (torch.matmul(X, W1) + b1).relu()    if is_training:  # 只在训练模型时使用丢弃法        H1 = dropout(H1, drop_prob1)  # 在第一层全连接后添加丢弃层    H2 = (torch.matmul(H1, W2) + b2).relu()    if is_training:        H2 = dropout(H2, drop_prob2)  # 在第二层全连接后添加丢弃层    return torch.matmul(H2, W3) + b3</code></pre><pre><code class="lang-python">def evaluate_accuracy(data_iter, net):    acc_sum, n = 0.0, 0    for X, y in data_iter:        if isinstance(net, torch.nn.Module):            net.eval() # 评估模式, 这会关闭dropout            acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()            net.train() # 改回训练模式        else: # 自定义的模型            if(&#39;is_training&#39; in net.__code__.co_varnames): # 如果有is_training这个参数                # 将is_training设置成False                acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item()             else:                acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()         n += y.shape[0]    return acc_sum / n</code></pre><pre><code class="lang-python">num_epochs, lr, batch_size = 5, 100.0, 256  # 这里的学习率设置的很大，原因与之前相同。loss = torch.nn.CrossEntropyLoss()train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, root=&#39;/home/kesci/input/FashionMNIST2065&#39;)d2l.train_ch3(    net,    train_iter,    test_iter,    loss,    num_epochs,    batch_size,    params,    lr)</code></pre><pre><code>epoch 1, loss 0.0046, train acc 0.549, test acc 0.704epoch 2, loss 0.0023, train acc 0.785, test acc 0.737epoch 3, loss 0.0019, train acc 0.825, test acc 0.834epoch 4, loss 0.0017, train acc 0.842, test acc 0.763epoch 5, loss 0.0016, train acc 0.848, test acc 0.813</code></pre><h2 id="简洁实现-1"><a href="#简洁实现-1" class="headerlink" title="简洁实现"></a>简洁实现</h2><pre><code class="lang-python">net = nn.Sequential(        d2l.FlattenLayer(),        nn.Linear(num_inputs, num_hiddens1),        nn.ReLU(),        nn.Dropout(drop_prob1),        nn.Linear(num_hiddens1, num_hiddens2),         nn.ReLU(),        nn.Dropout(drop_prob2),        nn.Linear(num_hiddens2, 10)        )for param in net.parameters():    nn.init.normal_(param, mean=0, std=0.01)</code></pre><pre><code class="lang-python">optimizer = torch.optim.SGD(net.parameters(), lr=0.5)d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, optimizer)</code></pre><pre><code>epoch 1, loss 0.0046, train acc 0.553, test acc 0.736epoch 2, loss 0.0023, train acc 0.785, test acc 0.803epoch 3, loss 0.0019, train acc 0.818, test acc 0.756epoch 4, loss 0.0018, train acc 0.835, test acc 0.829epoch 5, loss 0.0016, train acc 0.848, test acc 0.851</code></pre><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul><li><p>欠拟合现象：模型无法达到一个较低的误差</p></li><li><p>过拟合现象：训练误差较低但是泛化误差依然较高，二者相差较大</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动手深度学习(三):多层感知机</title>
      <link href="/2020/02/14/dl-notes3-mlp/"/>
      <url>/2020/02/14/dl-notes3-mlp/</url>
      
        <content type="html"><![CDATA[<h1 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h1><ol><li>多层感知机的基本知识</li><li>使用多层感知机图像分类的从零开始的实现</li><li>使用pytorch的简洁实现</li></ol><h2 id="多层感知机的基本知识"><a href="#多层感知机的基本知识" class="headerlink" title="多层感知机的基本知识"></a>多层感知机的基本知识</h2><p>深度学习主要关注多层模型。在这里，我们将以多层感知机（multilayer perceptron，MLP）为例，介绍多层神经网络的概念。</p><h3 id="隐藏层"><a href="#隐藏层" class="headerlink" title="隐藏层"></a>隐藏层</h3><p>下图展示了一个多层感知机的神经网络图，它含有一个隐藏层，该层中有5个隐藏单元。</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/upload/image/q5ho684jmh.png" alt="Image Name"></p><h3 id="表达公式"><a href="#表达公式" class="headerlink" title="表达公式"></a>表达公式</h3><p>具体来说，给定一个小批量样本$\boldsymbol{X} \in \mathbb{R}^{n \times d}$，其批量大小为$n$，输入个数为$d$。假设多层感知机只有一个隐藏层，其中隐藏单元个数为$h$。记隐藏层的输出（也称为隐藏层变量或隐藏变量）为$\boldsymbol{H}$，有$\boldsymbol{H} \in \mathbb{R}^{n \times h}$。因为隐藏层和输出层均是全连接层，可以设隐藏层的权重参数和偏差参数分别为$\boldsymbol{W}_h \in \mathbb{R}^{d \times h}$和 $\boldsymbol{b}_h \in \mathbb{R}^{1 \times h}$，输出层的权重和偏差参数分别为$\boldsymbol{W}_o \in \mathbb{R}^{h \times q}$和$\boldsymbol{b}_o \in \mathbb{R}^{1 \times q}$。</p><p>我们先来看一种含单隐藏层的多层感知机的设计。其输出$\boldsymbol{O} \in \mathbb{R}^{n \times q}$的计算为</p><script type="math/tex; mode=display"> \begin{aligned} \boldsymbol{H} &= \boldsymbol{X} \boldsymbol{W}_h + \boldsymbol{b}_h,\\ \boldsymbol{O} &= \boldsymbol{H} \boldsymbol{W}_o + \boldsymbol{b}_o, \end{aligned}</script><p>也就是将隐藏层的输出直接作为输出层的输入。如果将以上两个式子联立起来，可以得到</p><script type="math/tex; mode=display"> \boldsymbol{O} = (\boldsymbol{X} \boldsymbol{W}_h + \boldsymbol{b}_h)\boldsymbol{W}_o + \boldsymbol{b}_o = \boldsymbol{X} \boldsymbol{W}_h\boldsymbol{W}_o + \boldsymbol{b}_h \boldsymbol{W}_o + \boldsymbol{b}_o.</script><p>从联立后的式子可以看出，虽然神经网络引入了隐藏层，却依然等价于一个单层神经网络：其中输出层权重参数为$\boldsymbol{W}_h\boldsymbol{W}_o$，偏差参数为$\boldsymbol{b}_h \boldsymbol{W}_o + \boldsymbol{b}_o$。不难发现，即便再添加更多的隐藏层，以上设计依然只能与仅含输出层的单层神经网络等价。</p><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>上述问题的根源在于全连接层只是对数据做仿射变换（affine transformation），而多个仿射变换的叠加仍然是一个仿射变换。解决问题的一个方法是引入非线性变换，例如对隐藏变量使用按元素运算的非线性函数进行变换，然后再作为下一个全连接层的输入。这个非线性函数被称为激活函数（activation function）。</p><p>下面我们介绍几个常用的激活函数：</p><h4 id="ReLU函数"><a href="#ReLU函数" class="headerlink" title="ReLU函数"></a>ReLU函数</h4><p>ReLU（rectified linear unit）函数提供了一个很简单的非线性变换。给定元素$x$，该函数定义为</p><script type="math/tex; mode=display">\text{ReLU}(x) = \max(x, 0).</script><p>可以看出，ReLU函数只保留正数元素，并将负数元素清零。为了直观地观察这一非线性变换，我们先定义一个绘图函数xyplot。</p><pre><code class="lang-python">%matplotlib inlineimport torchimport numpy as npimport matplotlib.pyplot as pltimport syssys.path.append(&quot;/home/kesci/input&quot;)import d2lzh1981 as d2lprint(torch.__version__)</code></pre><pre><code>1.3.0</code></pre><pre><code class="lang-python">def xyplot(x_vals, y_vals, name):    # d2l.set_figsize(figsize=(5, 2.5))    plt.plot(x_vals.detach().numpy(), y_vals.detach().numpy())    plt.xlabel(&#39;x&#39;)    plt.ylabel(name + &#39;(x)&#39;)</code></pre><pre><code class="lang-python">x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)y = x.relu()xyplot(x, y, &#39;relu&#39;)</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/070825B6A382411DA5BD7D14E67E8D54/q5hv7cdtna.png"></p><pre><code class="lang-python">y.sum().backward()xyplot(x, x.grad, &#39;grad of relu&#39;)</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/BFB05150DBD1474D9A9ECCB9CDF1DD39/q5hv7c3pxb.png"></p><h4 id="Sigmoid函数"><a href="#Sigmoid函数" class="headerlink" title="Sigmoid函数"></a>Sigmoid函数</h4><p>sigmoid函数可以将元素的值变换到0和1之间：</p><script type="math/tex; mode=display">\text{sigmoid}(x) = \frac{1}{1 + \exp(-x)}.</script><pre><code class="lang-python">y = x.sigmoid()xyplot(x, y, &#39;sigmoid&#39;)</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/68FCB4E8142144458F13128B370D1C91/q5hv7dor11.png"></p><p>依据链式法则，sigmoid函数的导数</p><script type="math/tex; mode=display">\text{sigmoid}'(x) = \text{sigmoid}(x)\left(1-\text{sigmoid}(x)\right).</script><p>下面绘制了sigmoid函数的导数。当输入为0时，sigmoid函数的导数达到最大值0.25；当输入越偏离0时，sigmoid函数的导数越接近0。</p><pre><code class="lang-python">x.grad.zero_()y.sum().backward()xyplot(x, x.grad, &#39;grad of sigmoid&#39;)</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/878C7B8823304F72860965E119A21412/q5hv7dpse9.png"></p><h4 id="tanh函数"><a href="#tanh函数" class="headerlink" title="tanh函数"></a>tanh函数</h4><p>tanh（双曲正切）函数可以将元素的值变换到-1和1之间：</p><script type="math/tex; mode=display">\text{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}.</script><p>我们接着绘制tanh函数。当输入接近0时，tanh函数接近线性变换。虽然该函数的形状和sigmoid函数的形状很像，但tanh函数在坐标系的原点上对称。</p><pre><code class="lang-python">y = x.tanh()xyplot(x, y, &#39;tanh&#39;)</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/92D16076309F42169482834C0B6ABB24/q5hv7dfeso.png"></p><p>依据链式法则，tanh函数的导数</p><script type="math/tex; mode=display">\text{tanh}'(x) = 1 - \text{tanh}^2(x).</script><p>下面绘制了tanh函数的导数。当输入为0时，tanh函数的导数达到最大值1；当输入越偏离0时，tanh函数的导数越接近0。</p><pre><code class="lang-python">x.grad.zero_()y.sum().backward()xyplot(x, x.grad, &#39;grad of tanh&#39;)</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/CB16F4B33E664E14BCE8E52D8B37C47F/q5hv7ejc8y.png"></p><h3 id="关于激活函数的选择"><a href="#关于激活函数的选择" class="headerlink" title="关于激活函数的选择"></a>关于激活函数的选择</h3><p>ReLu函数是一个通用的激活函数，目前在大多数情况下使用。但是，ReLU函数只能在隐藏层中使用。</p><p>用于分类器时，sigmoid函数及其组合通常效果更好。由于梯度消失问题，有时要避免使用sigmoid和tanh函数。  </p><p>在神经网络层数较多的时候，最好使用ReLu函数，ReLu函数比较简单计算量少，而sigmoid和tanh函数计算量大很多。</p><p>在选择激活函数的时候可以先选用ReLu函数如果效果不理想可以尝试其他激活函数。</p><h3 id="多层感知机-1"><a href="#多层感知机-1" class="headerlink" title="多层感知机"></a>多层感知机</h3><p>多层感知机就是含有至少一个隐藏层的由全连接层组成的神经网络，且每个隐藏层的输出通过激活函数进行变换。多层感知机的层数和各隐藏层中隐藏单元个数都是超参数。以单隐藏层为例并沿用本节之前定义的符号，多层感知机按以下方式计算输出：</p><script type="math/tex; mode=display"> \begin{aligned} \boldsymbol{H} &= \phi(\boldsymbol{X} \boldsymbol{W}_h + \boldsymbol{b}_h),\\ \boldsymbol{O} &= \boldsymbol{H} \boldsymbol{W}_o + \boldsymbol{b}_o, \end{aligned}</script><p>其中$\phi$表示激活函数。</p><h2 id="多层感知机从零开始的实现"><a href="#多层感知机从零开始的实现" class="headerlink" title="多层感知机从零开始的实现"></a>多层感知机从零开始的实现</h2><pre><code class="lang-python">import torchimport numpy as npimport syssys.path.append(&quot;/home/kesci/input&quot;)import d2lzh1981 as d2lprint(torch.__version__)</code></pre><pre><code>1.3.0</code></pre><h3 id="获取训练集"><a href="#获取训练集" class="headerlink" title="获取训练集"></a>获取训练集</h3><pre><code class="lang-python">batch_size = 256train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size,root=&#39;/home/kesci/input/FashionMNIST2065&#39;)</code></pre><h3 id="定义模型参数"><a href="#定义模型参数" class="headerlink" title="定义模型参数"></a>定义模型参数</h3><pre><code class="lang-python">num_inputs, num_outputs, num_hiddens = 784, 10, 256W1 = torch.tensor(np.random.normal(0, 0.01, (num_inputs, num_hiddens)), dtype=torch.float)b1 = torch.zeros(num_hiddens, dtype=torch.float)W2 = torch.tensor(np.random.normal(0, 0.01, (num_hiddens, num_outputs)), dtype=torch.float)b2 = torch.zeros(num_outputs, dtype=torch.float)params = [W1, b1, W2, b2]for param in params:    param.requires_grad_(requires_grad=True)</code></pre><h3 id="定义激活函数"><a href="#定义激活函数" class="headerlink" title="定义激活函数"></a>定义激活函数</h3><pre><code class="lang-python">def relu(X):    return torch.max(input=X, other=torch.tensor(0.0))</code></pre><h3 id="定义网络"><a href="#定义网络" class="headerlink" title="定义网络"></a>定义网络</h3><pre><code class="lang-python">def net(X):    X = X.view((-1, num_inputs))    H = relu(torch.matmul(X, W1) + b1)    return torch.matmul(H, W2) + b2</code></pre><h3 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h3><pre><code class="lang-python">loss = torch.nn.CrossEntropyLoss()</code></pre><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><pre><code class="lang-python">num_epochs, lr = 5, 100.0# def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,#               params=None, lr=None, optimizer=None):#     for epoch in range(num_epochs):#         train_l_sum, train_acc_sum, n = 0.0, 0.0, 0#         for X, y in train_iter:#             y_hat = net(X)#             l = loss(y_hat, y).sum()#             #             # 梯度清零#             if optimizer is not None:#                 optimizer.zero_grad()#             elif params is not None and params[0].grad is not None:#                 for param in params:#                     param.grad.data.zero_()#            #             l.backward()#             if optimizer is None:#                 d2l.sgd(params, lr, batch_size)#             else:#                 optimizer.step()  # “softmax回归的简洁实现”一节将用到#             #             #             train_l_sum += l.item()#             train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()#             n += y.shape[0]#         test_acc = evaluate_accuracy(test_iter, net)#         print(&#39;epoch %d, loss %.4f, train acc %.3f, test acc %.3f&#39;#               % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr)</code></pre><pre><code>epoch 1, loss 0.0030, train acc 0.712, test acc 0.806epoch 2, loss 0.0019, train acc 0.821, test acc 0.806epoch 3, loss 0.0017, train acc 0.847, test acc 0.825epoch 4, loss 0.0015, train acc 0.856, test acc 0.834epoch 5, loss 0.0015, train acc 0.863, test acc 0.847</code></pre><h2 id="多层感知机pytorch实现"><a href="#多层感知机pytorch实现" class="headerlink" title="多层感知机pytorch实现"></a>多层感知机pytorch实现</h2><pre><code class="lang-python">import torchfrom torch import nnfrom torch.nn import initimport numpy as npimport syssys.path.append(&quot;/home/kesci/input&quot;)import d2lzh1981 as d2lprint(torch.__version__)</code></pre><pre><code>1.3.0</code></pre><h3 id="初始化模型和各个参数"><a href="#初始化模型和各个参数" class="headerlink" title="初始化模型和各个参数"></a>初始化模型和各个参数</h3><pre><code class="lang-python">num_inputs, num_outputs, num_hiddens = 784, 10, 256net = nn.Sequential(        d2l.FlattenLayer(),        nn.Linear(num_inputs, num_hiddens),        nn.ReLU(),        nn.Linear(num_hiddens, num_outputs),         )for params in net.parameters():    init.normal_(params, mean=0, std=0.01)</code></pre><h3 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h3><pre><code class="lang-python">batch_size = 256train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size,root=&#39;/home/kesci/input/FashionMNIST2065&#39;)loss = torch.nn.CrossEntropyLoss()optimizer = torch.optim.SGD(net.parameters(), lr=0.5)num_epochs = 5d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, optimizer)</code></pre><pre><code>epoch 1, loss 0.0031, train acc 0.701, test acc 0.774epoch 2, loss 0.0019, train acc 0.821, test acc 0.806epoch 3, loss 0.0017, train acc 0.841, test acc 0.805epoch 4, loss 0.0015, train acc 0.855, test acc 0.834epoch 5, loss 0.0014, train acc 0.866, test acc 0.840</code></pre>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动手深度学习(二):Softmax</title>
      <link href="/2020/02/14/dl-notes2-softmax/"/>
      <url>/2020/02/14/dl-notes2-softmax/</url>
      
        <content type="html"><![CDATA[<h1 id="softmax和分类模型"><a href="#softmax和分类模型" class="headerlink" title="softmax和分类模型"></a>softmax和分类模型</h1><p>内容包含：</p><ol><li>softmax回归的基本概念</li><li>如何获取Fashion-MNIST数据集和读取数据</li><li>softmax回归模型的从零开始实现，实现一个对Fashion-MNIST训练集中的图像数据进行分类的模型</li><li>使用pytorch重新实现softmax回归模型</li></ol><h2 id="softmax的基本概念"><a href="#softmax的基本概念" class="headerlink" title="softmax的基本概念"></a>softmax的基本概念</h2><ul><li><p>分类问题<br>一个简单的图像分类问题，输入图像的高和宽均为2像素，色彩为灰度。<br>图像中的4像素分别记为$x_1, x_2, x_3, x_4$。<br>假设真实标签为狗、猫或者鸡，这些标签对应的离散值为$y_1, y_2, y_3$。<br>我们通常使用离散的数值来表示类别，例如$y_1=1, y_2=2, y_3=3$。</p></li><li><p>权重矢量  </p><script type="math/tex; mode=display">\begin{aligned} o_1 &= x_1 w_{11} + x_2 w_{21} + x_3 w_{31} + x_4 w_{41} + b_1 \end{aligned}</script></li></ul><script type="math/tex; mode=display"> \begin{aligned} o_2 &= x_1 w_{12} + x_2 w_{22} + x_3 w_{32} + x_4 w_{42} + b_2 \end{aligned}</script><script type="math/tex; mode=display"> \begin{aligned} o_3 &= x_1 w_{13} + x_2 w_{23} + x_3 w_{33} + x_4 w_{43} + b_3 \end{aligned}</script><ul><li>神经网络图<br>下图用神经网络图描绘了上面的计算。softmax回归同线性回归一样，也是一个单层神经网络。由于每个输出$o_1, o_2, o_3$的计算都要依赖于所有的输入$x_1, x_2, x_3, x_4$，softmax回归的输出层也是一个全连接层。</li></ul><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/upload/image/q5hmymezog.png" alt="Image Name"></p><script type="math/tex; mode=display">\begin{aligned}softmax回归是一个单层神经网络\end{aligned}</script><p>既然分类问题需要得到离散的预测输出，一个简单的办法是将输出值$o_i$当作预测类别是$i$的置信度，并将值最大的输出所对应的类作为预测输出，即输出 $\underset{i}{\arg\max} o_i$。例如，如果$o_1,o_2,o_3$分别为$0.1,10,0.1$，由于$o_2$最大，那么预测类别为2，其代表猫。</p><ul><li>输出问题<br>直接使用输出层的输出有两个问题：<ol><li>一方面，由于输出层的输出值的范围不确定，我们难以直观上判断这些值的意义。例如，刚才举的例子中的输出值10表示“很置信”图像类别为猫，因为该输出值是其他两类的输出值的100倍。但如果$o_1=o_3=10^3$，那么输出值10却又表示图像类别为猫的概率很低。</li><li>另一方面，由于真实标签是离散值，这些离散值与不确定范围的输出值之间的误差难以衡量。</li></ol></li></ul><p>softmax运算符（softmax operator）解决了以上两个问题。它通过下式将输出值变换成值为正且和为1的概率分布：</p><script type="math/tex; mode=display"> \hat{y}_1, \hat{y}_2, \hat{y}_3 = \text{softmax}(o_1, o_2, o_3)</script><p>其中</p><script type="math/tex; mode=display"> \hat{y}1 = \frac{ \exp(o_1)}{\sum_{i=1}^3 \exp(o_i)},\quad \hat{y}2 = \frac{ \exp(o_2)}{\sum_{i=1}^3 \exp(o_i)},\quad \hat{y}3 = \frac{ \exp(o_3)}{\sum_{i=1}^3 \exp(o_i)}.</script><p>容易看出$\hat{y}_1 + \hat{y}_2 + \hat{y}_3 = 1$且$0 \leq \hat{y}_1, \hat{y}_2, \hat{y}_3 \leq 1$，因此$\hat{y}_1, \hat{y}_2, \hat{y}_3$是一个合法的概率分布。这时候，如果$\hat{y}_2=0.8$，不管$\hat{y}_1$和$\hat{y}_3$的值是多少，我们都知道图像类别为猫的概率是80%。此外，我们注意到</p><script type="math/tex; mode=display"> \underset{i}{\arg\max} o_i = \underset{i}{\arg\max} \hat{y}_i</script><p>因此softmax运算不改变预测类别输出。</p><ul><li>计算效率<ul><li>单样本矢量计算表达式<br>为了提高计算效率，我们可以将单样本分类通过矢量计算来表达。在上面的图像分类问题中，假设softmax回归的权重和偏差参数分别为</li></ul></li></ul><script type="math/tex; mode=display"> \boldsymbol{W} = \begin{bmatrix} w_{11} & w_{12} & w_{13} \\ w_{21} & w_{22} & w_{23} \\ w_{31} & w_{32} & w_{33} \\ w_{41} & w_{42} & w_{43} \end{bmatrix},\quad \boldsymbol{b} = \begin{bmatrix} b_1 & b_2 & b_3 \end{bmatrix},</script><p>设高和宽分别为2个像素的图像样本$i$的特征为</p><script type="math/tex; mode=display">\boldsymbol{x}^{(i)} = \begin{bmatrix}x_1^{(i)} & x_2^{(i)} & x_3^{(i)} & x_4^{(i)}\end{bmatrix},</script><p>输出层的输出为</p><script type="math/tex; mode=display">\boldsymbol{o}^{(i)} = \begin{bmatrix}o_1^{(i)} & o_2^{(i)} & o_3^{(i)}\end{bmatrix},</script><p>预测为狗、猫或鸡的概率分布为</p><script type="math/tex; mode=display">\boldsymbol{\hat{y}}^{(i)} = \begin{bmatrix}\hat{y}_1^{(i)} & \hat{y}_2^{(i)} & \hat{y}_3^{(i)}\end{bmatrix}.</script><p>softmax回归对样本$i$分类的矢量计算表达式为</p><script type="math/tex; mode=display"> \begin{aligned} \boldsymbol{o}^{(i)} &= \boldsymbol{x}^{(i)} \boldsymbol{W} + \boldsymbol{b},\\ \boldsymbol{\hat{y}}^{(i)} &= \text{softmax}(\boldsymbol{o}^{(i)}). \end{aligned}</script><ul><li>小批量矢量计算表达式<br>  为了进一步提升计算效率，我们通常对小批量数据做矢量计算。广义上讲，给定一个小批量样本，其批量大小为$n$，输入个数（特征数）为$d$，输出个数（类别数）为$q$。设批量特征为$\boldsymbol{X} \in \mathbb{R}^{n \times d}$。假设softmax回归的权重和偏差参数分别为$\boldsymbol{W} \in \mathbb{R}^{d \times q}$和$\boldsymbol{b} \in \mathbb{R}^{1 \times q}$。softmax回归的矢量计算表达式为</li></ul><script type="math/tex; mode=display"> \begin{aligned} \boldsymbol{O} &= \boldsymbol{X} \boldsymbol{W} + \boldsymbol{b},\\ \boldsymbol{\hat{Y}} &= \text{softmax}(\boldsymbol{O}), \end{aligned}</script><p>其中的加法运算使用了广播机制，$\boldsymbol{O}, \boldsymbol{\hat{Y}} \in \mathbb{R}^{n \times q}$且这两个矩阵的第$i$行分别为样本$i$的输出$\boldsymbol{o}^{(i)}$和概率分布$\boldsymbol{\hat{y}}^{(i)}$。</p><h2 id="交叉熵损失函数"><a href="#交叉熵损失函数" class="headerlink" title="交叉熵损失函数"></a>交叉熵损失函数</h2><p>对于样本$i$，我们构造向量$\boldsymbol{y}^{(i)}\in \mathbb{R}^{q}$ ，使其第$y^{(i)}$（样本$i$类别的离散数值）个元素为1，其余为0。这样我们的训练目标可以设为使预测概率分布$\boldsymbol{\hat y}^{(i)}$尽可能接近真实的标签概率分布$\boldsymbol{y}^{(i)}$。</p><ul><li>平方损失估计  </li></ul><script type="math/tex; mode=display">\begin{aligned}Loss = |\boldsymbol{\hat y}^{(i)}-\boldsymbol{y}^{(i)}|^2/2\end{aligned}</script><p>然而，想要预测分类结果正确，我们其实并不需要预测概率完全等于标签概率。例如，在图像分类的例子里，如果$y^{(i)}=3$，那么我们只需要$\hat{y}^{(i)}_3$比其他两个预测值$\hat{y}^{(i)}_1$和$\hat{y}^{(i)}_2$大就行了。即使$\hat{y}^{(i)}_3$值为0.6，不管其他两个预测值为多少，类别预测均正确。而平方损失则过于严格，例如$\hat y^{(i)}_1=\hat y^{(i)}_2=0.2$比$\hat y^{(i)}_1=0, \hat y^{(i)}_2=0.4$的损失要小很多，虽然两者都有同样正确的分类预测结果。</p><p>改善上述问题的一个方法是使用更适合衡量两个概率分布差异的测量函数。其中，交叉熵（cross entropy）是一个常用的衡量方法：</p><script type="math/tex; mode=display">H\left(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}\right ) = -\sum_{j=1}^q y_j^{(i)} \log \hat y_j^{(i)},</script><p>其中带下标的$y_j^{(i)}$是向量$\boldsymbol y^{(i)}$中非0即1的元素，需要注意将它与样本$i$类别的离散数值，即不带下标的$y^{(i)}$区分。在上式中，我们知道向量$\boldsymbol y^{(i)}$中只有第$y^{(i)}$个元素$y^{(i)}{y^{(i)}}$为1，其余全为0，于是$H(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}) = -\log \hat y_{y^{(i)}}^{(i)}$。也就是说，交叉熵只关心对正确类别的预测概率，因为只要其值足够大，就可以确保分类结果正确。当然，遇到一个样本有多个标签时，例如图像里含有不止一个物体时，我们并不能做这一步简化。但即便对于这种情况，交叉熵同样只关心对图像中出现的物体类别的预测概率。</p><p>假设训练数据集的样本数为$n$，交叉熵损失函数定义为 </p><script type="math/tex; mode=display">\ell(\boldsymbol{\Theta}) = \frac{1}{n} \sum_{i=1}^n H\left(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}\right ),</script><p>其中$\boldsymbol{\Theta}$代表模型参数。同样地，如果每个样本只有一个标签，那么交叉熵损失可以简写成$\ell(\boldsymbol{\Theta}) = -(1/n) \sum_{i=1}^n \log \hat y_{y^{(i)}}^{(i)}$。从另一个角度来看，我们知道最小化$\ell(\boldsymbol{\Theta})$等价于最大化$\exp(-n\ell(\boldsymbol{\Theta}))=\prod_{i=1}^n \hat y_{y^{(i)}}^{(i)}$，即最小化交叉熵损失函数等价于最大化训练数据集所有标签类别的联合预测概率。</p><h2 id="模型训练和预测"><a href="#模型训练和预测" class="headerlink" title="模型训练和预测"></a>模型训练和预测</h2><p>在训练好softmax回归模型后，给定任一样本特征，就可以预测每个输出类别的概率。通常，我们把预测概率最大的类别作为输出类别。如果它与真实类别（标签）一致，说明这次预测是正确的。在3.6节的实验中，我们将使用准确率（accuracy）来评价模型的表现。它等于正确预测数量与总预测数量之比。</p><h1 id="获取Fashion-MNIST训练集和读取数据"><a href="#获取Fashion-MNIST训练集和读取数据" class="headerlink" title="获取Fashion-MNIST训练集和读取数据"></a>获取Fashion-MNIST训练集和读取数据</h1><p>在介绍softmax回归的实现前我们先引入一个多类图像分类数据集。它将在后面的章节中被多次使用，以方便我们观察比较算法之间在模型精度和计算效率上的区别。图像分类数据集中最常用的是手写数字识别数据集MNIST[1]。但大部分模型在MNIST上的分类精度都超过了95%。为了更直观地观察算法之间的差异，我们将使用一个图像内容更加复杂的数据集Fashion-MNIST[2]。</p><p>我这里我们会使用torchvision包，它是服务于PyTorch深度学习框架的，主要用来构建计算机视觉模型。torchvision主要由以下几部分构成：</p><ol><li>torchvision.datasets: 一些加载数据的函数及常用的数据集接口；</li><li>torchvision.models: 包含常用的模型结构（含预训练模型），例如AlexNet、VGG、ResNet等；</li><li>torchvision.transforms: 常用的图片变换，例如裁剪、旋转等；</li><li>torchvision.utils: 其他的一些有用的方法。</li></ol><pre><code class="lang-python"># import needed package%matplotlib inlinefrom IPython import displayimport matplotlib.pyplot as pltimport torchimport torchvisionimport torchvision.transforms as transformsimport timeimport syssys.path.append(&quot;/home/kesci/input&quot;)import d2lzh1981 as d2lprint(torch.__version__)print(torchvision.__version__)</code></pre><pre><code>---------------------------------------------------------------------------ModuleNotFoundError                       Traceback (most recent call last)&lt;ipython-input-1-e3b06ab588d2&gt; in &lt;module&gt;      4 import matplotlib.pyplot as plt      5 ----&gt; 6 import torch      7 import torchvision      8 import torchvision.transforms as transformsModuleNotFoundError: No module named &#39;torch&#39;</code></pre><h2 id="get-dataset"><a href="#get-dataset" class="headerlink" title="get dataset"></a>get dataset</h2><pre><code class="lang-python">mnist_train = torchvision.datasets.FashionMNIST(root=&#39;/home/kesci/input/FashionMNIST2065&#39;, train=True, download=True, transform=transforms.ToTensor())mnist_test = torchvision.datasets.FashionMNIST(root=&#39;/home/kesci/input/FashionMNIST2065&#39;, train=False, download=True, transform=transforms.ToTensor())</code></pre><p>class torchvision.datasets.FashionMNIST(root, train=True, transform=None, target_transform=None, download=False)</p><ul><li>root（string）– 数据集的根目录，其中存放processed/training.pt和processed/test.pt文件。</li><li>train（bool, 可选）– 如果设置为True，从training.pt创建数据集，否则从test.pt创建。</li><li>download（bool, 可选）– 如果设置为True，从互联网下载数据并放到root文件夹下。如果root目录下已经存在数据，不会再次下载。</li><li>transform（可被调用 , 可选）– 一种函数或变换，输入PIL图片，返回变换之后的数据。如：transforms.RandomCrop。</li><li>target_transform（可被调用 , 可选）– 一种函数或变换，输入目标，进行变换。</li></ul><pre><code class="lang-python"># show result print(type(mnist_train))print(len(mnist_train), len(mnist_test))</code></pre><pre><code>&lt;class &#39;torchvision.datasets.mnist.FashionMNIST&#39;&gt;60000 10000</code></pre><pre><code class="lang-python"># 我们可以通过下标来访问任意一个样本feature, label = mnist_train[0]print(feature.shape, label)  # Channel x Height x Width</code></pre><pre><code>torch.Size([1, 28, 28]) 9</code></pre><p>如果不做变换输入的数据是图像，我们可以看一下图片的类型参数：</p><pre><code class="lang-python">mnist_PIL = torchvision.datasets.FashionMNIST(root=&#39;/home/kesci/input/FashionMNIST2065&#39;, train=True, download=True)PIL_feature, label = mnist_PIL[0]print(PIL_feature)</code></pre><pre><code>&lt;PIL.Image.Image image mode=L size=28x28 at 0x7F54A41612E8&gt;</code></pre><pre><code class="lang-python"># 本函数已保存在d2lzh包中方便以后使用def get_fashion_mnist_labels(labels):    text_labels = [&#39;t-shirt&#39;, &#39;trouser&#39;, &#39;pullover&#39;, &#39;dress&#39;, &#39;coat&#39;,                   &#39;sandal&#39;, &#39;shirt&#39;, &#39;sneaker&#39;, &#39;bag&#39;, &#39;ankle boot&#39;]    return [text_labels[int(i)] for i in labels]</code></pre><pre><code class="lang-python">def show_fashion_mnist(images, labels):    d2l.use_svg_display()    # 这里的_表示我们忽略（不使用）的变量    _, figs = plt.subplots(1, len(images), figsize=(12, 12))    for f, img, lbl in zip(figs, images, labels):        f.imshow(img.view((28, 28)).numpy())        f.set_title(lbl)        f.axes.get_xaxis().set_visible(False)        f.axes.get_yaxis().set_visible(False)    plt.show()</code></pre><pre><code class="lang-python">X, y = [], []for i in range(10):    X.append(mnist_train[i][0]) # 将第i个feature加到X中    y.append(mnist_train[i][1]) # 将第i个label加到y中show_fashion_mnist(X, get_fashion_mnist_labels(y))</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/056F457B00454FFD81A3CB6AD966C508/q5j7ehijw7.svg"></p><pre><code class="lang-python"># 读取数据batch_size = 256num_workers = 4train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)</code></pre><pre><code class="lang-python">start = time.time()for X, y in train_iter:    continueprint(&#39;%.2f sec&#39; % (time.time() - start))</code></pre><pre><code>4.95 sec</code></pre><h1 id="softmax从零开始的实现"><a href="#softmax从零开始的实现" class="headerlink" title="softmax从零开始的实现"></a>softmax从零开始的实现</h1><pre><code class="lang-python">import torchimport torchvisionimport numpy as npimport syssys.path.append(&quot;/home/kesci/input&quot;)import d2lzh1981 as d2lprint(torch.__version__)print(torchvision.__version__)</code></pre><pre><code>1.3.00.4.1a0+d94043a</code></pre><h2 id="获取训练集数据和测试集数据"><a href="#获取训练集数据和测试集数据" class="headerlink" title="获取训练集数据和测试集数据"></a>获取训练集数据和测试集数据</h2><pre><code class="lang-python">batch_size = 256train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, root=&#39;/home/kesci/input/FashionMNIST2065&#39;)</code></pre><h2 id="模型参数初始化"><a href="#模型参数初始化" class="headerlink" title="模型参数初始化"></a>模型参数初始化</h2><pre><code class="lang-python">num_inputs = 784print(28*28)num_outputs = 10W = torch.tensor(np.random.normal(0, 0.01, (num_inputs, num_outputs)), dtype=torch.float)b = torch.zeros(num_outputs, dtype=torch.float)</code></pre><pre><code>784</code></pre><pre><code class="lang-python">W.requires_grad_(requires_grad=True)b.requires_grad_(requires_grad=True)</code></pre><pre><code>tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)</code></pre><h2 id="对多维Tensor按维度操作"><a href="#对多维Tensor按维度操作" class="headerlink" title="对多维Tensor按维度操作"></a>对多维Tensor按维度操作</h2><pre><code class="lang-python">X = torch.tensor([[1, 2, 3], [4, 5, 6]])print(X.sum(dim=0, keepdim=True))  # dim为0，按照相同的列求和，并在结果中保留列特征print(X.sum(dim=1, keepdim=True))  # dim为1，按照相同的行求和，并在结果中保留行特征print(X.sum(dim=0, keepdim=False)) # dim为0，按照相同的列求和，不在结果中保留列特征print(X.sum(dim=1, keepdim=False)) # dim为1，按照相同的行求和，不在结果中保留行特征</code></pre><pre><code>tensor([[5, 7, 9]])tensor([[ 6],        [15]])tensor([5, 7, 9])tensor([ 6, 15])</code></pre><h2 id="定义softmax操作"><a href="#定义softmax操作" class="headerlink" title="定义softmax操作"></a>定义softmax操作</h2><script type="math/tex; mode=display"> \hat{y}_j = \frac{ \exp(o_j)}{\sum_{i=1}^3 \exp(o_i)}</script><pre><code class="lang-python">def softmax(X):    X_exp = X.exp()    partition = X_exp.sum(dim=1, keepdim=True)    # print(&quot;X size is &quot;, X_exp.size())    # print(&quot;partition size is &quot;, partition, partition.size())    return X_exp / partition  # 这里应用了广播机制</code></pre><pre><code class="lang-python">X = torch.rand((2, 5))X_prob = softmax(X)print(X_prob, &#39;\n&#39;, X_prob.sum(dim=1))</code></pre><pre><code>tensor([[0.2253, 0.1823, 0.1943, 0.2275, 0.1706],        [0.1588, 0.2409, 0.2310, 0.1670, 0.2024]])  tensor([1.0000, 1.0000])</code></pre><h2 id="softmax回归模型"><a href="#softmax回归模型" class="headerlink" title="softmax回归模型"></a>softmax回归模型</h2><script type="math/tex; mode=display"> \begin{aligned} \boldsymbol{o}^{(i)} &= \boldsymbol{x}^{(i)} \boldsymbol{W} + \boldsymbol{b},\\ \boldsymbol{\hat{y}}^{(i)} &= \text{softmax}(\boldsymbol{o}^{(i)}). \end{aligned}</script><pre><code class="lang-python">def net(X):    return softmax(torch.mm(X.view((-1, num_inputs)), W) + b)</code></pre><h2 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h2><script type="math/tex; mode=display">H\left(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}\right ) = -\sum_{j=1}^q y_j^{(i)} \log \hat y_j^{(i)},</script><script type="math/tex; mode=display">\ell(\boldsymbol{\Theta}) = \frac{1}{n} \sum_{i=1}^n H\left(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}\right ),</script><script type="math/tex; mode=display">\ell(\boldsymbol{\Theta}) = -(1/n) \sum_{i=1}^n \log \hat y_{y^{(i)}}^{(i)}</script><pre><code class="lang-python">y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])y = torch.LongTensor([0, 2])y_hat.gather(1, y.view(-1, 1))</code></pre><pre><code>tensor([[0.1000],        [0.5000]])</code></pre><pre><code class="lang-python">def cross_entropy(y_hat, y):    return - torch.log(y_hat.gather(1, y.view(-1, 1)))</code></pre><h2 id="定义准确率"><a href="#定义准确率" class="headerlink" title="定义准确率"></a>定义准确率</h2><p>我们模型训练完了进行模型预测的时候，会用到我们这里定义的准确率。</p><pre><code class="lang-python">def accuracy(y_hat, y):    return (y_hat.argmax(dim=1) == y).float().mean().item()</code></pre><pre><code class="lang-python">print(accuracy(y_hat, y))</code></pre><pre><code>0.5</code></pre><pre><code class="lang-python"># 本函数已保存在d2lzh_pytorch包中方便以后使用。该函数将被逐步改进：它的完整实现将在“图像增广”一节中描述def evaluate_accuracy(data_iter, net):    acc_sum, n = 0.0, 0    for X, y in data_iter:        acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()        n += y.shape[0]    return acc_sum / n</code></pre><pre><code class="lang-python">print(evaluate_accuracy(test_iter, net))</code></pre><pre><code>0.1445</code></pre><h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><pre><code class="lang-python">num_epochs, lr = 5, 0.1# 本函数已保存在d2lzh_pytorch包中方便以后使用def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,              params=None, lr=None, optimizer=None):    for epoch in range(num_epochs):        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0        for X, y in train_iter:            y_hat = net(X)            l = loss(y_hat, y).sum()            # 梯度清零            if optimizer is not None:                optimizer.zero_grad()            elif params is not None and params[0].grad is not None:                for param in params:                    param.grad.data.zero_()            l.backward()            if optimizer is None:                d2l.sgd(params, lr, batch_size)            else:                optimizer.step()             train_l_sum += l.item()            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()            n += y.shape[0]        test_acc = evaluate_accuracy(test_iter, net)        print(&#39;epoch %d, loss %.4f, train acc %.3f, test acc %.3f&#39;              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, batch_size, [W, b], lr)</code></pre><pre><code>epoch 1, loss 0.7851, train acc 0.750, test acc 0.791epoch 2, loss 0.5704, train acc 0.814, test acc 0.810epoch 3, loss 0.5258, train acc 0.825, test acc 0.819epoch 4, loss 0.5014, train acc 0.832, test acc 0.824epoch 5, loss 0.4865, train acc 0.836, test acc 0.827</code></pre><h2 id="模型预测"><a href="#模型预测" class="headerlink" title="模型预测"></a>模型预测</h2><p>现在我们的模型训练完了，可以进行一下预测，我们的这个模型训练的到底准确不准确。<br>现在就可以演示如何对图像进行分类了。给定一系列图像（第三行图像输出），我们比较一下它们的真实标签（第一行文本输出）和模型预测结果（第二行文本输出）。</p><pre><code class="lang-python">X, y = iter(test_iter).next()true_labels = d2l.get_fashion_mnist_labels(y.numpy())pred_labels = d2l.get_fashion_mnist_labels(net(X).argmax(dim=1).numpy())titles = [true + &#39;\n&#39; + pred for true, pred in zip(true_labels, pred_labels)]d2l.show_fashion_mnist(X[0:9], titles[0:9])</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/1DA8927186304BEBA2B3DCC4A9E027DD/q5j7fq2jer.svg"></p><h1 id="softmax的简洁实现"><a href="#softmax的简洁实现" class="headerlink" title="softmax的简洁实现"></a>softmax的简洁实现</h1><pre><code class="lang-python"># 加载各种包或者模块import torchfrom torch import nnfrom torch.nn import initimport numpy as npimport syssys.path.append(&quot;/home/kesci/input&quot;)import d2lzh1981 as d2lprint(torch.__version__)</code></pre><pre><code>1.3.0</code></pre><h2 id="初始化参数和获取数据"><a href="#初始化参数和获取数据" class="headerlink" title="初始化参数和获取数据"></a>初始化参数和获取数据</h2><pre><code class="lang-python">batch_size = 256train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, root=&#39;/home/kesci/input/FashionMNIST2065&#39;)</code></pre><h2 id="定义网络模型"><a href="#定义网络模型" class="headerlink" title="定义网络模型"></a>定义网络模型</h2><pre><code class="lang-python">num_inputs = 784num_outputs = 10class LinearNet(nn.Module):    def __init__(self, num_inputs, num_outputs):        super(LinearNet, self).__init__()        self.linear = nn.Linear(num_inputs, num_outputs)    def forward(self, x): # x 的形状: (batch, 1, 28, 28)        y = self.linear(x.view(x.shape[0], -1))        return y# net = LinearNet(num_inputs, num_outputs)class FlattenLayer(nn.Module):    def __init__(self):        super(FlattenLayer, self).__init__()    def forward(self, x): # x 的形状: (batch, *, *, ...)        return x.view(x.shape[0], -1)from collections import OrderedDictnet = nn.Sequential(        # FlattenLayer(),        # LinearNet(num_inputs, num_outputs)         OrderedDict([           (&#39;flatten&#39;, FlattenLayer()),           (&#39;linear&#39;, nn.Linear(num_inputs, num_outputs))]) # 或者写成我们自己定义的 LinearNet(num_inputs, num_outputs) 也可以        )</code></pre><h2 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h2><pre><code class="lang-python">init.normal_(net.linear.weight, mean=0, std=0.01)init.constant_(net.linear.bias, val=0)</code></pre><pre><code>Parameter containing:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)</code></pre><h2 id="定义损失函数-1"><a href="#定义损失函数-1" class="headerlink" title="定义损失函数"></a>定义损失函数</h2><pre><code class="lang-python">loss = nn.CrossEntropyLoss() # 下面是他的函数原型# class torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction=&#39;mean&#39;)</code></pre><h2 id="定义优化函数"><a href="#定义优化函数" class="headerlink" title="定义优化函数"></a>定义优化函数</h2><pre><code class="lang-python">optimizer = torch.optim.SGD(net.parameters(), lr=0.1) # 下面是函数原型# class torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False)</code></pre><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><pre><code class="lang-python">num_epochs = 5d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, optimizer)</code></pre><pre><code>epoch 1, loss 0.0031, train acc 0.751, test acc 0.795epoch 2, loss 0.0022, train acc 0.813, test acc 0.809epoch 3, loss 0.0021, train acc 0.825, test acc 0.806epoch 4, loss 0.0020, train acc 0.833, test acc 0.813epoch 5, loss 0.0019, train acc 0.837, test acc 0.822</code></pre>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动手深度学习(一):线性回归</title>
      <link href="/2020/02/14/dl-notes1-lin-re/"/>
      <url>/2020/02/14/dl-notes1-lin-re/</url>
      
        <content type="html"><![CDATA[<h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><p>主要内容包括：</p><ol><li>线性回归的基本要素</li><li>线性回归模型从零开始的实现</li><li>线性回归模型使用pytorch的简洁实现</li></ol><h2 id="线性回归的基本要素"><a href="#线性回归的基本要素" class="headerlink" title="线性回归的基本要素"></a>线性回归的基本要素</h2><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>为了简单起见，这里我们假设价格只取决于房屋状况的两个因素，即面积（平方米）和房龄（年）。接下来我们希望探索价格与这两个因素的具体关系。线性回归假设输出与各个输入之间是线性关系:</p><script type="math/tex; mode=display">\mathrm{price} = w_{\mathrm{area}} \cdot \mathrm{area} + w_{\mathrm{age}} \cdot \mathrm{age} + b</script><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>我们通常收集一系列的真实数据，例如多栋房屋的真实售出价格和它们对应的面积和房龄。我们希望在这个数据上面寻找模型参数来使模型的预测价格与真实价格的误差最小。在机器学习术语里，该数据集被称为训练数据集（training data set）或训练集（training set），一栋房屋被称为一个样本（sample），其真实售出价格叫作标签（label），用来预测标签的两个因素叫作特征（feature）。特征用来表征样本的特点。</p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>在模型训练中，我们需要衡量价格预测值与真实值之间的误差。通常我们会选取一个非负数作为误差，且数值越小表示误差越小。一个常用的选择是平方函数。 它在评估索引为 $i$ 的样本误差的表达式为</p><script type="math/tex; mode=display">l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2,</script><script type="math/tex; mode=display">L(\mathbf{w}, b) =\frac{1}{n}\sum_{i=1}^n l^{(i)}(\mathbf{w}, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2.</script><h3 id="优化函数-随机梯度下降"><a href="#优化函数-随机梯度下降" class="headerlink" title="优化函数 - 随机梯度下降"></a>优化函数 - 随机梯度下降</h3><p>当模型和损失函数形式较为简单时，上面的误差最小化问题的解可以直接用公式表达出来。这类解叫作解析解（analytical solution）。本节使用的线性回归和平方误差刚好属于这个范畴。然而，大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。这类解叫作数值解（numerical solution）。</p><p>在求数值解的优化算法中，小批量随机梯度下降（mini-batch stochastic gradient descent）在深度学习中被广泛使用。它的算法很简单：先选取一组模型参数的初始值，如随机选取；接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量（mini-batch）$\mathcal{B}$，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后用此结果与预先设定的一个正数的乘积作为模型参数在本次迭代的减小量。   </p><script type="math/tex; mode=display">(\mathbf{w},b) \leftarrow (\mathbf{w},b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(\mathbf{w},b)} l^{(i)}(\mathbf{w},b)</script><p>学习率: $\eta$代表在每次优化中，能够学习的步长的大小<br>批量大小: $\mathcal{B}$是小批量计算中的批量大小batch size   </p><p>总结一下，优化函数的有以下两个步骤：</p><ul><li>(i)初始化模型参数，一般来说使用随机初始化；</li><li>(ii)我们在数据上迭代多次，通过在负梯度方向移动参数来更新每个参数。</li></ul><h2 id="矢量计算"><a href="#矢量计算" class="headerlink" title="矢量计算"></a>矢量计算</h2><p>在模型训练或预测时，我们常常会同时处理多个数据样本并用到矢量计算。在介绍线性回归的矢量计算表达式之前，让我们先考虑对两个向量相加的两种方法。</p><ol><li>向量相加的一种方法是，将这两个向量按元素逐一做标量加法。</li><li>向量相加的另一种方法是，将这两个向量直接做矢量加法。</li></ol><pre><code class="lang-python">import torchimport time# init variable a, b as 1000 dimension vectorn = 1000a = torch.ones(n)b = torch.ones(n)</code></pre><pre><code class="lang-python"># define a timer class to record timeclass Timer(object):    &quot;&quot;&quot;Record multiple running times.&quot;&quot;&quot;    def __init__(self):        self.times = []        self.start()    def start(self):        # start the timer        self.start_time = time.time()    def stop(self):        # stop the timer and record time into a list        self.times.append(time.time() - self.start_time)        return self.times[-1]    def avg(self):        # calculate the average and return        return sum(self.times)/len(self.times)    def sum(self):        # return the sum of recorded time        return sum(self.times)</code></pre><p>现在我们可以来测试了。首先将两个向量使用for循环按元素逐一做标量加法。</p><pre><code class="lang-python">timer = Timer()c = torch.zeros(n)for i in range(n):    c[i] = a[i] + b[i]&#39;%.5f sec&#39; % timer.stop()</code></pre><pre><code>&#39;0.01162 sec&#39;</code></pre><p>另外是使用torch来将两个向量直接做矢量加法：</p><pre><code class="lang-python">timer.start()d = a + b&#39;%.5f sec&#39; % timer.stop()</code></pre><pre><code>&#39;0.00027 sec&#39;</code></pre><p>结果很明显,后者比前者运算速度更快。因此，我们应该尽可能采用矢量计算，以提升计算效率。</p><h2 id="线性回归模型从零开始的实现"><a href="#线性回归模型从零开始的实现" class="headerlink" title="线性回归模型从零开始的实现"></a>线性回归模型从零开始的实现</h2><pre><code class="lang-python"># import packages and modules%matplotlib inlineimport torchfrom IPython import displayfrom matplotlib import pyplot as pltimport numpy as npimport randomprint(torch.__version__)</code></pre><pre><code>1.3.0</code></pre><h3 id="生成数据集"><a href="#生成数据集" class="headerlink" title="生成数据集"></a>生成数据集</h3><p>使用线性模型来生成数据集，生成一个1000个样本的数据集，下面是用来生成数据的线性关系：</p><script type="math/tex; mode=display">\mathrm{price} = w_{\mathrm{area}} \cdot \mathrm{area} + w_{\mathrm{age}} \cdot \mathrm{age} + b</script><pre><code class="lang-python"># set input feature number num_inputs = 2# set example numbernum_examples = 1000# set true weight and bias in order to generate corresponded labeltrue_w = [2, -3.4]true_b = 4.2features = torch.randn(num_examples, num_inputs,                      dtype=torch.float32)labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_blabels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()),   # 不可能严格线性分布，加一个偏差                       dtype=torch.float32)</code></pre><h3 id="使用图像来展示生成的数据"><a href="#使用图像来展示生成的数据" class="headerlink" title="使用图像来展示生成的数据"></a>使用图像来展示生成的数据</h3><pre><code class="lang-python">plt.scatter(features[:, 1].numpy(), labels.numpy(), 1)</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/8E2E1E16060241C6A33E4CF1EC65DF1D/q5mgua7bvt.png"></p><h3 id="读取数据集"><a href="#读取数据集" class="headerlink" title="读取数据集"></a>读取数据集</h3><pre><code class="lang-python">def data_iter(batch_size, features, labels):    num_examples = len(features)    indices = list(range(num_examples))    random.shuffle(indices)  # random read 10 samples 顺序打乱读取    for i in range(0, num_examples, batch_size):        # the last time may be not enough for a whole batch 防止越界        j = torch.LongTensor(indices[i: min(i + batch_size, num_examples)]) #每次跳跃 b_size        yield features.index_select(0, j), labels.index_select(0, j)   # 返回一个就退出</code></pre><pre><code class="lang-python">batch_size = 10for X, y in data_iter(batch_size, features, labels):    print(X, &#39;\n&#39;, y)    break</code></pre><pre><code>tensor([[ 0.6802, -0.1529],        [-1.1139, -1.0506],        [ 1.1355,  0.6313],        [ 0.9057,  1.4527],        [-0.7477, -0.4656],        [-0.4012, -0.1810],        [ 0.0038, -1.3701],        [-0.9438, -1.6571],        [ 0.4016,  0.4233],        [ 0.8824, -1.0067]])  tensor([6.0793, 5.5343, 4.3112, 1.0657, 4.2856, 4.0040, 8.8709, 7.9642, 3.5696,        9.3887])</code></pre><h3 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><pre><code class="lang-python">w = torch.tensor(np.random.normal(0, 0.01, (num_inputs, 1)), dtype=torch.float32)b = torch.zeros(1, dtype=torch.float32)w.requires_grad_(requires_grad=True)  # 要求梯度b.requires_grad_(requires_grad=True)</code></pre><pre><code>tensor([0.], requires_grad=True)</code></pre><h3 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h3><p>定义用来训练参数的训练模型：</p><script type="math/tex; mode=display">\mathrm{price} = w_{\mathrm{area}} \cdot \mathrm{area} + w_{\mathrm{age}} \cdot \mathrm{age} + b</script><pre><code class="lang-python">def linreg(X, w, b):    return torch.mm(X, w) + b   # mm 相乘</code></pre><h3 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h3><p>我们使用的是均方误差损失函数：</p><script type="math/tex; mode=display">l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2,</script><pre><code class="lang-python">def squared_loss(y_hat, y):     return (y_hat - y.view(y_hat.size())) ** 2 / 2   # view 表示数据相同，大小不同# s = squared_loss(2.33,3.14) + squared_loss(1.07,0.98) + squared_loss(1.23,1.32)y_hat = torch.tensor([[2.33],        [ 1.07],        [ 1.23]])y = torch.tensor([3.14, 0.98, 1.32])s = squared_loss(y_hat, y)print(s.mean())</code></pre><pre><code>tensor(0.1121)</code></pre><h3 id="定义优化函数"><a href="#定义优化函数" class="headerlink" title="定义优化函数"></a>定义优化函数</h3><p>在这里优化函数使用的是小批量随机梯度下降：</p><script type="math/tex; mode=display">(\mathbf{w},b) \leftarrow (\mathbf{w},b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(\mathbf{w},b)} l^{(i)}(\mathbf{w},b)</script><pre><code class="lang-python">def sgd(params, lr, batch_size):     # params : [w, b]    for param in params:        # 在梯度的负方向累加一个值，达到优化的效果（趋于谷底）        param.data -= lr * param.grad / batch_size # ues .data to operate param without gradient track</code></pre><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>当数据集、模型、损失函数和优化函数定义完了之后就可来准备进行模型的训练了。</p><pre><code class="lang-python"># super parameters initlr = 0.03  # lr 学习率num_epochs = 5net = linregloss = squared_loss# trainingfor epoch in range(num_epochs):  # training repeats num_epochs times    # in each epoch, all the samples in dataset will be used once    # X is the feature and y is the label of a batch sample    for X, y in data_iter(batch_size, features, labels):        l = loss(net(X, w, b), y).sum()          # calculate the gradient of batch sample loss         l.backward()          # using small batch random gradient descent to iter model parameters        sgd([w, b], lr, batch_size)          # reset parameter gradient        w.grad.data.zero_()  # 每一次运算后都需要将上一次的梯度记录清空        b.grad.data.zero_()    train_l = loss(net(features, w, b), labels)    print(&#39;epoch %d, loss %f&#39; % (epoch + 1, train_l.mean().item()))</code></pre><h2 id="线性回归模型使用pytorch的简洁实现"><a href="#线性回归模型使用pytorch的简洁实现" class="headerlink" title="线性回归模型使用pytorch的简洁实现"></a>线性回归模型使用pytorch的简洁实现</h2><pre><code class="lang-python">import torchfrom torch import nnimport numpy as nptorch.manual_seed(1)   # 同样的随机初始化种子,保证结果可以复现print(torch.__version__)torch.set_default_tensor_type(&#39;torch.FloatTensor&#39;)</code></pre><pre><code>1.3.0</code></pre><h3 id="生成数据集-1"><a href="#生成数据集-1" class="headerlink" title="生成数据集"></a>生成数据集</h3><p>在这里生成数据集跟从零开始的实现中是完全一样的。</p><pre><code class="lang-python">num_inputs = 2num_examples = 1000true_w = [2, -3.4]true_b = 4.2features = torch.tensor(np.random.normal(0, 1, (num_examples, num_inputs)), dtype=torch.float)labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_blabels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float)</code></pre><h3 id="读取数据集-1"><a href="#读取数据集-1" class="headerlink" title="读取数据集"></a>读取数据集</h3><pre><code class="lang-python">import torch.utils.data as Databatch_size = 10# combine featues and labels of datasetdataset = Data.TensorDataset(features, labels)# put dataset into DataLoaderdata_iter = Data.DataLoader(    dataset=dataset,            # torch TensorDataset format    batch_size=batch_size,      # mini batch size    shuffle=True,               # whether shuffle the data or not    num_workers=2,              # read data in multithreading)</code></pre><pre><code class="lang-python">for X, y in data_iter:    print(X, &#39;\n&#39;, y)    break</code></pre><pre><code>tensor([[ 0.2134,  0.7981],        [-0.3899, -0.4544],        [ 1.4472, -1.2160],        [-0.7354,  1.2216],        [-1.3233,  0.6937],        [-0.2810,  0.5505],        [-1.6620, -0.1457],        [-0.7635,  0.7058],        [ 0.6079, -0.7497],        [ 0.4924, -0.1376]])  tensor([ 1.9045,  4.9770, 11.2503, -1.4180, -0.8159,  1.7737,  1.3910,  0.2721,         7.9648,  5.6408])</code></pre><h3 id="定义模型-1"><a href="#定义模型-1" class="headerlink" title="定义模型"></a>定义模型</h3><pre><code class="lang-python">class LinearNet(nn.Module):    def __init__(self, n_feature):        super(LinearNet, self).__init__()      # call father function to init         self.linear = nn.Linear(n_feature, 1)  # function prototype: `torch.nn.Linear(in_features, out_features, bias=True)`    def forward(self, x):        y = self.linear(x)        return ynet = LinearNet(num_inputs)print(net)</code></pre><pre><code>LinearNet(  (linear): Linear(in_features=2, out_features=1, bias=True))</code></pre><pre><code class="lang-python"># ways to init a multilayer network# method onenet = nn.Sequential(   # 一个时序容器，Modules 会以他们传入的顺序被添加到容器中    nn.Linear(num_inputs, 1)    # other layers can be added here    )# method twonet = nn.Sequential()net.add_module(&#39;linear&#39;, nn.Linear(num_inputs, 1))# net.add_module ......# method threefrom collections import OrderedDictnet = nn.Sequential(OrderedDict([          (&#39;linear&#39;, nn.Linear(num_inputs, 1))          # ......        ]))print(net[0].weight)print(net[0].bias)# print(net)</code></pre><pre><code>Parameter containing:tensor([[0.6652, 0.1260]], requires_grad=True)Parameter containing:tensor([0.5704], requires_grad=True)</code></pre><h3 id="初始化模型参数-1"><a href="#初始化模型参数-1" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><pre><code class="lang-python">from torch.nn import initinit.normal_(net[0].weight, mean=0.0, std=0.01)  # weight重新生成init.constant_(net[0].bias, val=0.0)  # bias[ 被赋值为 0print(net[0].weight)print(net[0].bias)</code></pre><pre><code>Parameter containing:tensor([[0.0016, 0.0029]], requires_grad=True)Parameter containing:tensor([0.], requires_grad=True)</code></pre><pre><code class="lang-python">for param in net.parameters():    print(param)  # param 就是上面的 weight 和 bias</code></pre><pre><code>Parameter containing:tensor([[0.0016, 0.0029]], requires_grad=True)Parameter containing:tensor([0.], requires_grad=True)</code></pre><h3 id="定义损失函数-1"><a href="#定义损失函数-1" class="headerlink" title="定义损失函数"></a>定义损失函数</h3><pre><code class="lang-python">loss = nn.MSELoss()    # nn built-in squared loss function   采用计算 MSE 的 loss                       # function prototype: `torch.nn.MSELoss(size_average=None, reduce=None, reduction=&#39;mean&#39;)`</code></pre><h3 id="定义优化函数-1"><a href="#定义优化函数-1" class="headerlink" title="定义优化函数"></a>定义优化函数</h3><pre><code class="lang-python">import torch.optim as optimoptimizer = optim.SGD(net.parameters(), lr=0.03)   # built-in random gradient descent functionprint(optimizer)  # function prototype: `torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False)`</code></pre><pre><code>SGD (Parameter Group 0    dampening: 0    lr: 0.03    momentum: 0    nesterov: False    weight_decay: 0)</code></pre><h3 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h3><pre><code class="lang-python">num_epochs = 3for epoch in range(1, num_epochs + 1):    for X, y in data_iter:        output = net(X)        l = loss(output, y.view(-1, 1))        optimizer.zero_grad() # reset gradient, equal to net.zero_grad()        l.backward()        optimizer.step()    print(&#39;epoch %d, loss: %f&#39; % (epoch, l.item()))</code></pre><pre><code>epoch 1, loss: 0.000299epoch 2, loss: 0.000058epoch 3, loss: 0.000051</code></pre><pre><code class="lang-python"># result comparisiondense = net[0]print(true_w, dense.weight.data)print(true_b, dense.bias.data)</code></pre><pre><code>[2, -3.4] tensor([[ 2.0007, -3.3992]])4.2 tensor([4.1989])</code></pre>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习基础</title>
      <link href="/2020/02/14/dl-notes/"/>
      <url>/2020/02/14/dl-notes/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo 踩坑记录</title>
      <link href="/2020/02/09/hexo-experience/"/>
      <url>/2020/02/09/hexo-experience/</url>
      
        <content type="html"><![CDATA[<h1 id="Hexo-踩坑记录"><a href="#Hexo-踩坑记录" class="headerlink" title="Hexo 踩坑记录"></a>Hexo 踩坑记录</h1><p>Hexo 作一个优秀的博客框架，本人也是一直参考着网上零零碎碎的博文一步步搭建的，中间遇到不少坑。每次做一个稍大的项目都要经历这种折腾，今后决定把这些经验记录下来沉淀，有未完成的也留给以后思考。</p><h3 id="1-git-分支未知出错"><a href="#1-git-分支未知出错" class="headerlink" title="1. git 分支未知出错"></a>1. git 分支未知出错</h3><p>修改 matery.css 的时候总是没法同步到 <a href="https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io" target="_blank" rel="noopener">https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io</a>… 的远端，后来发现存在分支的问题，于是在主题的 config.yml 中添加一个 <code>@master</code>:</p><pre><code class="lang-yml">jsDelivr:  url: https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io@master</code></pre><p>更改的内容部分可以生效，但仍然有一些冥顽不化的配色，本地已经渲染出来了，远端 jsdelivr 死活不改变，加了 @master 也不变，于是直接在 post/post-detail.ejs 页面中添加 css 代码强行修改:</p><pre><code class="lang-css">#articleContent a {    color: #267871; !important;}#artDetail .post-cate a {    color: #267871; !important;}blockquote {    border-left: 5px solid #267871; !important;}#artDetail .reprint a {    color: #267871; !important;}</code></pre><p>挖个坑，希望今后可以好好研究一下 git</p><h3 id="2-Hexo-无法解析代码块"><a href="#2-Hexo-无法解析代码块" class="headerlink" title="2. Hexo 无法解析代码块"></a>2. Hexo 无法解析代码块</h3><p>比如上面的 css 代码块，Hexo 在解析的时候会把它识别成网页 css 的内容，最后没有显示，只能主动适应 Hexo 这种方式：</p><ol><li>不添加 <code>&lt;style type=&quot;text/css&quot;&gt;</code>这种会与网页混淆的内容</li><li>代码块中不添加(1) (2) 序号</li></ol><h3 id="3-Hexo-代码块无法高亮的问题"><a href="#3-Hexo-代码块无法高亮的问题" class="headerlink" title="3. Hexo 代码块无法高亮的问题"></a>3. Hexo 代码块无法高亮的问题</h3><ol><li><p>主要矛盾：<br>hexo 对于 mathjax 的显示支持有问题，需要安装新的 kmarked 插件，修改对于<code>{, (</code>等等的解析规则，而修改规则支持 Mathjax 后，prism 的高亮功能又会失效，简而言之不可兼得</p></li><li><p>解决：<br>我使用的是<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank" rel="noopener">hexo-matery</a>主题，使用了大家推荐的 hexo-prism-plugin 主题，但是出现和 prism 高亮和 Mathjax 语法不兼容的问题。代码不能高亮简直失去了博客写代码的意义。最后使用 <a href="https://highlightjs.org/" target="_blank" rel="noopener">highlight.js</a>成功实现代码高亮，参考以下这位博主的设置：<a href="http://cps.ninja/2019/03/25/add-highlightjs-to-hexo-blog/" target="_blank" rel="noopener">使用 Highlight.js 优化代码块高亮效果</a></p></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数学基础备忘录</title>
      <link href="/2020/02/07/math-notes/"/>
      <url>/2020/02/07/math-notes/</url>
      
        <content type="html"><![CDATA[<h1 id="数学基础备忘录"><a href="#数学基础备忘录" class="headerlink" title="数学基础备忘录"></a>数学基础备忘录</h1><h2 id="排列组合公式"><a href="#排列组合公式" class="headerlink" title="排列组合公式"></a>排列组合公式</h2><p>(A_{n}^{m}=n(n-1) \cdots(n-m+1))  m个数递减相乘<br>(C_{n}^{m}=\frac{A_{n}^{m}}{m !}=\frac{n !}{m !(n-m) !}=C_{n}^{n-m})</p><p>$A^4_4 = 4\times3\times2\times1 = 24$</p><p>$A^4_4$ 计算方法：从4 开始，从 4 到 3 到 2…连续乘 4 个数。<br>$C^3_4$ 计算方法：（先计算 $A^3_4$ 再得到 $C^3_4$ ）$C^3_4 = \frac{A^3_4}{3!} = \frac{4\times3\times2}{3\times2\times1} = 24\div6 = 4$</p><p>比如 $A^2_4$ 表示从 4 个东西东西中抽取任意 2 样，一共有 $4\times3 = 12$ <code>排列</code>方式。<br>$C^3_4$ 表示从 4 个东西东西中抽取任意 3 样，一共有 $\frac{4\times3\times2}{3\times2\times1} = 24\div6 = 4$ 两种<code>组成</code>方式。</p><p>A 表示排列，是有序的，而 C 表示组合，表示有多少种<code>组成</code>方式，只看成员，不看顺序。</p><p>对于 A（排列）来说，<code>4 3 1</code>和<code>1 3 4</code>是两种构成，而对于 C（组合）来说，<code>4 3 1</code>和<code>1 3 4</code>就是同一回事。</p><p>$C^3_5$ = 多少？ </p>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> math </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习概述</title>
      <link href="/2020/02/06/ml-study-notes/"/>
      <url>/2020/02/06/ml-study-notes/</url>
      
        <content type="html"><![CDATA[<h1 id="机器学习概述"><a href="#机器学习概述" class="headerlink" title="机器学习概述"></a>机器学习概述</h1><h2 id="机器学习分类"><a href="#机器学习分类" class="headerlink" title="机器学习分类"></a>机器学习分类</h2><ol><li>监督学习：已经有数据，和数据对应的标签。</li><li>非监督学习：给定的样本无需输出/标签，让机器自己学习样本中隐含的内部结构。</li><li>半监督学习：二者结合。</li><li>强化学习：通过打分/评价的形式，类似于监督学习中的标签。</li></ol><h2 id="机器学习模型"><a href="#机器学习模型" class="headerlink" title="机器学习模型"></a>机器学习模型</h2><p>机器学习 = 数据 data + 模型 model + 优化方法 optimal strategy</p><h2 id="偏差-方差权衡"><a href="#偏差-方差权衡" class="headerlink" title="偏差/方差权衡"></a>偏差/方差权衡</h2><p>variance 和 bias，分别对应过拟合和欠拟合</p><p>来自 Wikipedia：</p><blockquote><p>在监督学习中，如果能将模型的方差与误差权衡好，那么可以认为该模型的泛化性能（对于新数据）将会表现出好的结果。</p><p>偏差刻画的是算法本身的性能。高偏差将会造成欠拟合(Underfitting) [miss the relevant relations between features and target outputs]。换句话说，模型越复杂偏差就越小；而模型越简单，偏差就越大。</p><p>方差用来衡量因训练集数据波动(fluctuations)而造成的误差影响。高方差将会造成过拟合(Overfitting)。</p></blockquote><p>在周志华老师&lt;机器学习&gt;书中是这样阐述的：</p><blockquote><p><em>偏差</em> 度量了学习算法的期望预测与真实结果的偏离程度，即刻画了算法本身的拟合能力；</p><p><em>方差</em> 度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响；</p><p><em>噪声</em> 则表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题的本身难度</p><p>偏差-方差分解说明，泛化性能是由学习算法的能力、数据的充分性以及学习任务本身的难度所共同决定的。给定的学习任务，为了取得好的泛化性能，则需使偏差较小，即能够充分拟合数据，并且使方差较小，即使数据扰动产生的影响小。一般来说方差与偏差是有冲突的，这称为方差-偏差窘境。</p></blockquote><h2 id="常见机器学习算法概览"><a href="#常见机器学习算法概览" class="headerlink" title="常见机器学习算法概览"></a>常见机器学习算法概览</h2><h3 id="1-Linear-Algorithm-线性算法"><a href="#1-Linear-Algorithm-线性算法" class="headerlink" title="1. Linear Algorithm 线性算法"></a>1. Linear Algorithm 线性算法</h3><ol><li><p><strong>Linear Regression 线性回归</strong>：使用最小二乘法 Least Squares 拟合一条直线 → 计算 R<sup>2</sup> → 计算 R<sup>2</sup> 的 p 值。R<sup>2</sup> 表示 x 能多大程度反映 y 的变化，p 值表示可靠程度。拟合直线的过程使用「随机梯度下降」（SGD）</p></li><li><p><strong>Lasso 回归 和 Ridge 回归</strong>：都可以减少共线性带来的影响，即 X 自变量之间有相互关联。区别可以归结为L2和L1正则化的性质差异。</p></li><li><p><strong>Polynomial Regression 多项式回归</strong>：能够模拟非线性可分的数据（曲线），线性回归不能做到这一点。但容易过拟合。</p></li><li><p><strong>Logistic Regression 逻辑回归</strong>：判断 True or False，Y 值为 0-1 表示概率，用于分类。线性回归使用「Residual 偏差」，而逻辑回归使用「maximum likelihood 最大似然」</p></li></ol><h3 id="2-Decision-Tree-决策树"><a href="#2-Decision-Tree-决策树" class="headerlink" title="2. Decision Tree 决策树"></a>2. Decision Tree 决策树</h3><ol><li><p><strong>ID3</strong>: 计算「信息熵」 $Entropy(D)$，值越小，说明样本集合D的纯度就越高，进而选择用样本的某一个属性a来划分样本集合D时，就可以得出用属性a对样本D进行划分所带来的「信息增益」 $Gain(D, a)$，值越大，说明如果用属性a来划分样本集合D，那么纯度会提升。 <script type="math/tex">Entropy(t)=-\sum_{k} p\left(c_{k} | t\right) \log p\left(c_{k} | t\right)</script>  <script type="math/tex">Classificationerror (t)=1-\max _{k}\left[p\left(c_{k} | t\right)\right]</script></p></li><li><p><strong>C4.5</strong>: 提出Gainratio 「增益率」，解决ID3决策树的一个缺点，当一个属性的可取值数目较多时，那么可能在这个属性对应的可取值下的样本只有一个或者是很少个，那么这个时候它的信息增益是非常高的，这个时候纯度很高，ID3决策树会认为这个属性很适合划分，但是较多取值的属性来进行划分带来的问题是它的泛化能力比较弱。用 $I(·)$ 表示不纯度——可以是熵可以是基尼，信息增益：<script type="math/tex">\Delta=I(\text { parent })-\sum_{i=1}^{n} \frac{N\left(a_{i}\right)}{N} I\left(a_{i}\right)</script>信息增益率：<script type="math/tex">Gainratio =\frac{\Delta}{Entropy({parent})}</script></p></li><li><p><strong>CART(Classification and Regression Tree)</strong>: 通过计算 Gini 基尼系数（尽可能小），判断 impurity 不纯洁度。离散数据用「是否」划分子树，连续数据可以用「两两之间平均值」划分子树。<script type="math/tex">{Gini}(t)=1-\sum_{k}\left[p\left(c_{k} | t\right)\right]^{2}</script>D 分裂为 DL 和 DR，分裂后的信息增益<script type="math/tex">Gain(D, A)=\frac{\left|D_{L}\right|}{|D|} \operatorname{Gini}\left(D_{L}\right)+\frac{\left|D_{R}\right|}{|D|} \operatorname{Gini}\left(D_{R}\right)</script></p></li></ol><h3 id="3-SVM-支持向量机"><a href="#3-SVM-支持向量机" class="headerlink" title="3. SVM 支持向量机"></a>3. SVM 支持向量机</h3><p>SVM：<a href="https://blog.csdn.net/liugan528/article/details/79448379" target="_blank" rel="noopener">https://blog.csdn.net/liugan528/article/details/79448379</a></p><p>KKT：<a href="https://blog.csdn.net/qq_32763149/article/details/81055062" target="_blank" rel="noopener">https://blog.csdn.net/qq_32763149/article/details/81055062</a></p><p><strong>SVM 分类</strong>：</p><ol><li>硬间隔支持向量机（线性可分支持向量机）：当训练数据线性可分时，可通过硬间隔最大化学得一个线性可分支持向量机。</li><li>软间隔支持向量机：当训练数据近似线性可分时，可通过软间隔最大化得到一个线性支持向量机。</li><li>非线性支持向量机：当训练数据线性不可分时，可通过核方法以及软间隔最大化得一个非线性支持向量机。</li></ol><p><strong>基本原理</strong>：</p><ol><li><p>Maximum Margin Classifier：只看边界。</p></li><li><p>Soft Margin Classifier（即 Support Vector Classifier）：允许 misclassification误分类，寻找两个支撑向量来确定分类边界。</p></li><li><p>Kernel Function：非线性SVM，从低维数据开始，通过「核函数」给数据升维，然后找到一个 Support Vector Classifier 将数据分成两组。核函数的选择，支撑向量的选择，都用 cross validation 交叉验证。</p></li><li><p>Kernel Trick: 根据升维的距离进行计算，但是不进行实际的升维。</p></li></ol><p><strong>具体过程</strong>：</p><ol><li><p>线性可分的情况：对于超平面 $w \cdot x+b=0$ 和 $margin$ 有关系<script type="math/tex">{margin}=\frac{2}{\|w\|}</script><br> 最大化 $margin$ 等效于最小化 $\frac{1}{2}|w|^{2}$</p><p> 形成一个拉格朗日乘子α的约束问题 <script type="math/tex">\begin{array}{ll}{\min _{w, b}} & {\frac{1}{2}|w|^{2}} {\text {s.t.}} & {y_{i}\left(w \cdot x_{i}+b\right)-1 \geq 0}\end{array}</script><br> 可以列式 <script type="math/tex">L(w, b, \alpha)=\frac{1}{2}|w|^{2}-\sum_{i=1}^{N} \alpha_{i}\left[y_{i}\left(w \cdot x_{i}+b\right)-1\right]</script><br> 拉格朗日对偶性：解决「凸二次规划」（convex quadratic propgramming）问题，即将原始的约束最优化问题可等价于极大极小的对偶问题（以 w,b 作参数时的最小值，以α作参数时的最大值）<br> <script type="math/tex">\max _{\alpha} \min _{w, b} \quad L(w, b, \alpha)</script>通过求导一系列步骤，转换成\begin{array}{ll}<br>{\min _{\alpha}} &amp; {\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i}} \\<br>{\text { s.t. }} &amp; {\sum_{i=1}^{N} \alpha_{i} y_{i}=0} \\<br>{} &amp; {\alpha_{i} \geq 0, \quad i=1,2, \cdots, N}<br>\end{array}</p></li><li><p>线性不可分的情况：对每个样本引入一个松弛变量 $\xi_{i} \geq 0$, 约束条件和目标函数变为</p><script type="math/tex; mode=display">\begin{aligned}&y_{i}\left(w \cdot x_{i}+b\right) \geq 1-\xi_{i}\\&\min _{w, b, \xi} \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i}\end{aligned}</script></li></ol><p><strong>部分术语</strong>：</p><ol><li><p>KKT 条件：是拉格朗日乘子的泛化，把所有的不等式约束、等式约束和目标函数全部写为一个式子L(a, b, x)= f(x) + a<em>g(x) + b</em>h(x)，KKT条件是说最优值必须满足以下条件：（1）L(a, b, x)对x求导为零；（2）h(x) =0; （3）a*g(x) = 0;</p></li><li><p>SMO：Sequential Minimal Optimization用二次规划来求解α，要用到 KKT</p></li><li><p>SVR：支持向量回归</p></li></ol><p><strong>优点</strong>：<br>SVM在中小量样本规模的时候容易得到数据和特征之间的非线性关系，可以避免使用神经网络结构选择和局部极小值问题，可解释性强，可以解决高维问题。</p><p><strong>缺点</strong>：<br>SVM对缺失数据敏感，对非线性问题没有通用的解决方案，核函数的正确选择不容易，计算复杂度高，主流的算法可以达到O(n2)O(n2)的复杂度，这对大规模的数据是吃不消的。</p><h3 id="4-Naive-Bayes-Algorithms-朴素贝叶斯"><a href="#4-Naive-Bayes-Algorithms-朴素贝叶斯" class="headerlink" title="4. Naive Bayes Algorithms 朴素贝叶斯"></a>4. Naive Bayes Algorithms 朴素贝叶斯</h3><ol><li>Naive Bayes</li><li>Gaussian Naive Bayes</li><li>Multinomial Naive Bayes</li><li>Bayesian Belief Network (BBN)</li><li>Bayesian Network (BN)</li></ol><p>朴素贝叶斯基本公式：$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$<br><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/09/J46IjSoqFuD1vtW.png" alt="J46IjSoqFuD1vtW"></p><h3 id="5-KNN-k-NearestNeighbor-K-最邻近算法"><a href="#5-KNN-k-NearestNeighbor-K-最邻近算法" class="headerlink" title="5. KNN (k-NearestNeighbor) K 最邻近算法"></a>5. KNN (k-NearestNeighbor) K 最邻近算法</h3><p>用于分类</p><ol><li>计算测试数据与各个训练数据之间的距离；</li><li>按照距离的递增关系进行排序；</li><li>选取距离最小的K个点；</li><li>确定前K个点所在类别的出现频率；</li><li>返回前K个点中出现频率最高的类别作为测试数据的预测分类</li></ol><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/09/ScAOKyPqMGWdrLH.png" alt="ScAOKyPqMGWdrLH"></p><h3 id="6-Clustering-Algorithm-聚类算法"><a href="#6-Clustering-Algorithm-聚类算法" class="headerlink" title="6. Clustering Algorithm 聚类算法"></a>6. Clustering Algorithm 聚类算法</h3><ol><li>k-Means：选取平均值</li><li>k-Medians：由选取平均值改为选取中位数</li><li>Expectation Maximisation (EM)：有隐含随机变量的概率模型的参数的估计方法，它是一种无监督的算法</li><li><p>Hierarchical Clustering 层次聚类：<br>算法如下：</p><p>(1) 将每个对象看作一类，计算两两之间的最小距离；</p><p>(2) 将距离最小的两个类合并成一个新类；</p><p>(3) 重新计算新类与所有类之间的距离；</p><p>(4) 重复(2)、(3)，直到所有类最后合并成一类。</p></li></ol><h3 id="7-K-means-算法"><a href="#7-K-means-算法" class="headerlink" title="7. K-means 算法"></a>7. K-means 算法</h3><p>算法如下：</p><pre><code>选取k个初始质心(作为初始cluster);repeat:    对每个样本点，计算得到距其最近的质心，将其类别标为该质心所对应的cluster;    重新计算k个cluser对应的质心;until 质心不再发生变化</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/09/VLCReWo92QXSNP4.jpg" alt="VLCReWo92QXSNP4"></p><h3 id="8-Random-Forest-随机森林"><a href="#8-Random-Forest-随机森林" class="headerlink" title="8. Random Forest 随机森林"></a>8. Random Forest 随机森林</h3><h3 id="9-Dimensionality-Reduction-Algorithms-降维算法"><a href="#9-Dimensionality-Reduction-Algorithms-降维算法" class="headerlink" title="9. Dimensionality Reduction Algorithms 降维算法"></a>9. Dimensionality Reduction Algorithms 降维算法</h3><h3 id="10-Gradient-Boosting-algorithms-梯度提升算法"><a href="#10-Gradient-Boosting-algorithms-梯度提升算法" class="headerlink" title="10. Gradient Boosting algorithms 梯度提升算法"></a>10. Gradient Boosting algorithms 梯度提升算法</h3><ol><li>GBM</li><li>XGBoost</li><li>LightGBM</li><li>CatBoost</li></ol><h3 id="11-Deep-Learning-Algorithms-深度学习"><a href="#11-Deep-Learning-Algorithms-深度学习" class="headerlink" title="11. Deep Learning Algorithms 深度学习"></a>11. Deep Learning Algorithms 深度学习</h3><ol><li>Convolutional Neural Network (CNN)</li><li>Recurrent Neural Networks (RNNs)</li><li>Long Short-Term Memory Networks (LSTMs)</li><li>Stacked Auto-Encoders</li><li>Deep Boltzmann Machine (DBM)</li><li>Deep Belief Networks (DBN)</li></ol><hr><h2 id="机器学习损失函数"><a href="#机器学习损失函数" class="headerlink" title="机器学习损失函数"></a>机器学习损失函数</h2><ol><li>0-1损失函数<script type="math/tex; mode=display">L(y,f(x)) =\begin{cases}0, & \text{y = f(x)}  \\1, & \text{y $\neq$ f(x)}\end{cases}</script></li><li>绝对值损失函数<script type="math/tex; mode=display">L(y,f(x))=|y-f(x)|</script></li><li>平方损失函数<script type="math/tex; mode=display">L(y,f(x))=(y-f(x))^2</script></li><li>log对数损失函数<script type="math/tex; mode=display">L(y,f(x))=log(1+e^{-yf(x)})</script></li><li>指数损失函数<script type="math/tex; mode=display">L(y,f(x))=exp(-yf(x))</script></li><li>Hinge损失函数<script type="math/tex; mode=display">L(w,b)=max\{0,1-yf(x)\}</script></li></ol><hr><h2 id="机器学习优化方法"><a href="#机器学习优化方法" class="headerlink" title="机器学习优化方法"></a>机器学习优化方法</h2><p>梯度下降是最常用的优化方法之一，它使用梯度的反方向 $ \nabla_\theta J(\theta) $ 更新参数 $ \theta $，使得目标函数$J(\theta)$达到最小化的一种优化方法，这种方法我们叫做梯度更新. </p><ol><li>(全量)梯度下降<script type="math/tex; mode=display">\theta=\theta-\eta\nabla_\theta J(\theta)</script></li><li>随机梯度下降<script type="math/tex; mode=display">\theta=\theta-\eta\nabla_\theta J(\theta;x^{(i)},y^{(i)})</script></li><li>小批量梯度下降<script type="math/tex; mode=display">\theta=\theta-\eta\nabla_\theta J(\theta;x^{(i:i+n)},y^{(i:i+n)})</script></li><li>引入动量的梯度下降<script type="math/tex; mode=display">\begin{cases}v_t=\gamma v_{t-1}+\eta \nabla_\theta J(\theta)  \\\theta=\theta-v_t\end{cases}</script></li><li>自适应学习率的Adagrad算法<script type="math/tex; mode=display">\begin{cases}g_t= \nabla_\theta J(\theta)  \\\theta_{t+1}=\theta_{t,i}-\frac{\eta}{\sqrt{G_t+\varepsilon}} \cdot g_t\end{cases}</script></li><li><p>牛顿法</p><script type="math/tex; mode=display">\theta_{t+1}=\theta_t-H^{-1}\nabla_\theta J(\theta_t)</script><p> 其中:<br> $t$: 迭代的轮数</p><p> $\eta$: 学习率</p><p> $G_t$: 前t次迭代的梯度和</p><p> $\varepsilon:$很小的数,防止除0错误</p><p> $H$: 损失函数相当于$\theta$的Hession矩阵在$\theta_t$处的估计</p></li></ol><hr><h2 id="机器学习的评价指标"><a href="#机器学习的评价指标" class="headerlink" title="机器学习的评价指标"></a>机器学习的评价指标</h2><ol><li>MSE(Mean Squared Error)<script type="math/tex; mode=display">MSE(y,f(x))=\frac{1}{N}\sum_{i=1}^{N}(y-f(x))^2</script></li><li>MAE(Mean Absolute Error)<script type="math/tex; mode=display">MSE(y,f(x))=\frac{1}{N}\sum_{i=1}^{N}|y-f(x)|</script></li><li>RMSE(Root Mean Squard Error)<script type="math/tex; mode=display">RMSE(y,f(x))=\frac{1}{1+MSE(y,f(x))}</script></li><li>Top-k准确率<script type="math/tex; mode=display">Top_k(y,pre_y)=\begin{cases}1, {y \in pre_y}  \\0, {y \notin pre_y}\end{cases}</script></li><li>混淆矩阵</li></ol><div class="table-container"><table><thead><tr><th style="text-align:center">混淆矩阵</th><th style="text-align:center">Predicted as Positive</th><th style="text-align:center">Predicted as Negative</th></tr></thead><tbody><tr><td style="text-align:center">Labeled as Positive</td><td style="text-align:center">True Positive(TP)</td><td style="text-align:center">False Negative(FN)</td></tr><tr><td style="text-align:center">Labeled as Negative</td><td style="text-align:center">False Positive(FP)</td><td style="text-align:center">True Negative(TN)</td></tr></tbody></table></div><ul><li>真正例(True Positive, TP):真实类别为正例, 预测类别为正例</li><li>假负例(False Negative, FN): 真实类别为正例, 预测类别为负例</li><li>假正例(False Positive, FP): 真实类别为负例, 预测类别为正例 </li><li><p>真负例(True Negative, TN): 真实类别为负例, 预测类别为负例</p></li><li><p>真正率(True Positive Rate, TPR): 被预测为正的正样本数 / 正样本实际数</p><script type="math/tex; mode=display">TPR=\frac{TP}{TP+FN}</script></li><li><p>假负率(False Negative Rate, FNR): 被预测为负的正样本数/正样本实际数</p><script type="math/tex; mode=display">FNR=\frac{FN}{TP+FN}</script></li><li><p>假正率(False Positive Rate, FPR): 被预测为正的负样本数/负样本实际数，</p><script type="math/tex; mode=display">FPR=\frac{FP}{FP+TN}</script></li><li>真负率(True Negative Rate, TNR): 被预测为负的负样本数/负样本实际数，<script type="math/tex; mode=display">TNR=\frac{TN}{FP+TN}</script></li><li>准确率(Accuracy)<script type="math/tex; mode=display">ACC=\frac{TP+TN}{TP+FN+FP+TN}</script></li><li>精准率<script type="math/tex; mode=display">P=\frac{TP}{TP+FP}</script></li><li>召回率<script type="math/tex; mode=display">R=\frac{TP}{TP+FN}</script></li><li>F1-Score<script type="math/tex; mode=display">\frac{2}{F_1}=\frac{1}{P}+\frac{1}{R}</script></li><li><strong>ROC</strong></li></ul><p>ROC曲线的横轴为“假正例率”，纵轴为“真正例率”. 以FPR为横坐标，TPR为纵坐标，那么ROC曲线就是改变各种阈值后得到的所有坐标点 (FPR,TPR) 的连线，画出来如下。红线是随机乱猜情况下的ROC，曲线越靠左上角，分类器越佳. </p><ul><li><strong>AUC(Area Under Curve)</strong></li></ul><p>AUC就是ROC曲线下的面积. 真实情况下，由于数据是一个一个的，阈值被离散化，呈现的曲线便是锯齿状的，当然数据越多，阈值分的越细，”曲线”越光滑. </p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/09/Ky4FT1MVe3PUYai.jpg" alt="Ky4FT1MVe3PUYai"></p><p>用AUC判断分类器（预测模型）优劣的标准:</p><ul><li>AUC = 1 是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数预测的场合，不存在完美分类器.</li><li>0.5 &lt; AUC &lt; 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值.</li><li>AUC &lt; 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测.</li></ul><h2 id="机器学习模型选择"><a href="#机器学习模型选择" class="headerlink" title="机器学习模型选择"></a>机器学习模型选择</h2><ol><li>交叉验证</li></ol><p>所有数据分为三部分：训练集、交叉验证集和测试集。交叉验证集不仅在选择模型时有用，在超参数选择、正则项参数 [公式] 和评价模型中也很有用。</p><ol><li>k-折叠交叉验证</li></ol><ul><li>假设训练集为S ，将训练集等分为k份:$\{S_1, S_2, …, S_k\}$. </li><li>然后每次从集合中拿出k-1份进行训练</li><li>利用集合中剩下的那一份来进行测试并计算损失值</li><li>最后得到k次测试得到的损失值，并选择平均损失值最小的模型</li></ul><ol><li>Bias与Variance，欠拟合与过拟合</li></ol><p><strong>欠拟合</strong>一般表示模型对数据的表现能力不足，通常是模型的复杂度不够，并且Bias高，训练集的损失值高，测试集的损失值也高.</p><p><strong>过拟合</strong>一般表示模型对数据的表现能力过好，通常是模型的复杂度过高，并且Variance高，训练集的损失值低，测试集的损失值高.</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/09/dIiSlJVkjFL6EpB.jpg" alt="dIiSlJVkjFL6EpB"></p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/09/Dn5bB8wUVgmsIzC.jpg" alt="Dn5bB8wUVgmsIzC"></p><ol><li>解决方法</li></ol><ul><li>增加训练样本: 解决高Variance情况</li><li>减少特征维数: 解决高Variance情况</li><li>增加特征维数: 解决高Bias情况</li><li>增加模型复杂度: 解决高Bias情况</li><li>减小模型复杂度: 解决高Variance情况</li></ul><h2 id="机器学习参数调优"><a href="#机器学习参数调优" class="headerlink" title="机器学习参数调优"></a>机器学习参数调优</h2><ol><li>网格搜索</li></ol><p>一种调参手段；穷举搜索：在所有候选的参数选择中，通过循环遍历，尝试每一种可能性，表现最好的参数就是最终的结果</p><ol><li>随机搜索</li></ol><p>与网格搜索相比，随机搜索并未尝试所有参数值，而是从指定的分布中采样固定数量的参数设置。它的理论依据是，如果随即样本点集足够大，那么也可以找到全局的最大或最小值，或它们的近似值。通过对搜索范围的随机取样，随机搜索一般会比网格搜索要快一些。</p><ol><li>贝叶斯优化算法</li></ol><p>贝叶斯优化用于机器学习调参由J. Snoek(2012)提出，主要思想是，给定优化的目标函数(广义的函数，只需指定输入和输出即可，无需知道内部结构以及数学性质)，通过不断地添加样本点来更新目标函数的后验分布(高斯过程,直到后验分布基本贴合于真实分布。简单的说，就是考虑了上一次参数的信息，从而更好的调整当前的参数。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PAT-甲级 做题笔记</title>
      <link href="/2020/02/06/pat-advance/"/>
      <url>/2020/02/06/pat-advance/</url>
      
        <content type="html"><![CDATA[<h1 id="PAT-甲级-做题笔记"><a href="#PAT-甲级-做题笔记" class="headerlink" title="PAT-甲级 做题笔记"></a>PAT-甲级 做题笔记</h1><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/06/Y49WNI6DpA3hSKX.png" alt="甲级题目分类"></p><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><p>0000 <a href="#0000">做题 Tips 基本经验</a><br>1003 <a href="#1003">Emergency (Dijkstra 算法)</a><br>1004 <a href="#1004">Counting Leaves (计算叶节点数，DFS/BFS 树算法)</a><br>1007 <a href="#1007">Maximum Subsequence Sum(最大子序列和)</a><br>1010 <a href="#1010">Radix (进制转换/二分法)</a><br>1012 <a href="#1012">The Best Rank (应用问题，数据结构设计，多维度排序)</a><br>1013 <a href="#1013">Battle Over Cities (图的遍历，统计强连通分量的个数，DFS)</a><br>1014 <a href="#1014">Waiting in Line (队列应用，排队问题)</a><br>1015 <a href="https://pintia.cn/problem-sets/994805342720868352/problems/994805495863296000" target="_blank" rel="noopener">Reversible Primes (进制转换+判断素数：可取用函数)</a><br>1016 <a href="#1016">Phone Bills (日期类计算)</a><br>1018 <a href="#1018">Public Bike Management (图最短路径 Dijkstra + DFS)</a><br>1020 <a href="#1020">Tree Traversals (已知后序和中序，转前序/层序)</a><br>1021 <a href="#1021">Deepest Root (求树中最长的路径，DFS，连通分量)</a><br>1040 <a href="#1040">Longest Symmetric String (求最长对称子串, DP)</a><br>1045 <a href="#1045">Favorite Color Stripe (最长不下降子序列, DP)</a><br>1068 <a href="#1068">Find More Coins (0-1 背包问题, DP)</a><br>1044 <a href="#1044">Shopping in Mars (特定值子序列和, 二分查找)</a><br>1085 <a href="#1085">Perfect Sequence (二分查找, upper_bound, 双指针)</a></p><hr><h2 id="0000-做题-Tips-基本经验"><a href="#0000-做题-Tips-基本经验" class="headerlink" title="0000 做题 Tips 基本经验"></a>0000 做题 Tips 基本经验<span id="0000"></span></h2><ol><li>最后千万别栽在头文件上，比如 reverse() 属于 &lt; algorithm&gt;</li><li><p>输出多行的时候，最后有一个多余的空行也没问题<br>比如使用 <code>cout &lt;&lt; a &lt;&lt; endl</code> 三次，样例输出：</p><pre><code> 3 4 5</code></pre><p> 实际输出：最后有一个多余空行没关系（实际上是需要一个\n 或者 endl 才正确）</p><pre><code> 3 4 5</code></pre></li><li>int 最大范围 $-2^{31}$=-2147483648 到 $2^{31}$-1=2147483647  (10 位数)</li></ol><hr><p><span id="1003"></span></p><h2 id="1003-Emergency-Dijkstra-算法"><a href="#1003-Emergency-Dijkstra-算法" class="headerlink" title="1003 Emergency (Dijkstra 算法)"></a>1003 Emergency (Dijkstra 算法)</h2><h3 id="1-题目大意"><a href="#1-题目大意" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>n个城市m条路，每个城市有救援小组，所有的边的边权已知。给定起点和终点，求从起点到终点的最短路径条数以及最短路径上的救援小组数目之和。如果有多条就输出点权（城市救援小组数目）最大的那个</p><h3 id="2-分析"><a href="#2-分析" class="headerlink" title="2. 分析"></a>2. 分析</h3><p>用一遍Dijkstra算法～救援小组个数相当于点权，用Dijkstra求边权最小的最短路径的条数，以及这些最短路径中点权最大的值～dis[i]表示从出发点到i结点最短路径的路径长度，num[i]表示从出发点到i结点最短路径的条数，w[i]表示从出发点到i点救援队的数目之和～当判定dis[u] + e[u][v] &lt; dis[v]的时候，不仅仅要更新dis[v]，还要更新num[v] = num[u], w[v] = weight[v] + w[u]; 如果dis[u] + e[u][v] == dis[v]，还要更新num[v] += num[u]，而且判断一下是否权重w[v]更小，如果更小了就更新w[v] = weight[v] + w[u]</p><h3 id="3-个人代码"><a href="#3-个人代码" class="headerlink" title="3. 个人代码"></a>3. 个人代码</h3><p><a href="https://pintia.cn/problem-sets/994805342720868352/problems/994805523835109376" target="_blank" rel="noopener">PTA-1003</a></p><pre><code class="lang-c++">#include &lt;iostream&gt;#include &lt;algorithm&gt;using namespace std;//Dijkstra 算法：单源最短路径int n, m, c1, c2;int e[510][510], weight[510], dis[510], num[510], w[510];//边的邻接矩阵，每个点的权值，从出发点到i的距离，最短距离边数，最大权重和const int Inf = 99999999;bool visit[510];int main(){    cin &gt;&gt; n &gt;&gt; m &gt;&gt; c1 &gt;&gt; c2;    for (int i = 0; i &lt; n; ++i) {        cin &gt;&gt; weight[i];    }    fill(e[0], e[0]+510*510, Inf);  //整个邻接矩阵填正无穷，默认不可达    fill(dis, dis+510, Inf);    int a,b,c;    for (int j = 0; j &lt; m; ++j) {        cin &gt;&gt; a &gt;&gt; b &gt;&gt; c;        e[a][b] = e[b][a] = c;  //一定要对称    }    dis[c1] = 0;    num[c1] = 1;    w[c1] = weight[c1];  //初始化    for (int i = 0; i &lt; n; ++i) {    //每个点都要作为起始点 开始一遍        int u = -1, min_d = Inf;        //每次循环重置        //本次循环表示从 i 出发, i -&gt; j        for (int j = 0; j &lt; n; ++j) {            if(!visit[j] &amp;&amp; dis[j] &lt; min_d){   //找从本点出发的最小边                u = j;                min_d = dis[j];            }        }        if(u == -1) break;        visit[u] = true;        //得到一条 i-&gt;u, 即 dis[], 用来更新最短路径        //之前没更新的、在后续得到更新，达到最优        for (int v = 0; v &lt; n; ++v) {            if(!visit[v] &amp;&amp; e[u][v] != Inf){                if(dis[u] + e[u][v] &lt; dis[v]){                    dis[v] = dis[u] + e[u][v];                    w[v] = w[u] + weight[v];                    num[v] = num[u];                } else if(dis[u] + e[u][v] == dis[v]){                    num[v] += num[u];                    if(w[v] &lt; w[u]+weight[v])                        w[v] = w[u]+weight[v];   //携带尽可能多的人                }            }        }    }    cout &lt;&lt; num[c2] &lt;&lt; &quot; &quot; &lt;&lt; w[c2];   //从出发点到c2    return 0;}</code></pre><h3 id="4-学习要点"><a href="#4-学习要点" class="headerlink" title="4. 学习要点"></a>4. 学习要点</h3><blockquote><p>1.真正理解单源最短路径，每一次选择的 dis 都是相对于源点的最短路径<br>2.大数组要设为全局变量<br>3.外层循环 i：n 次循环，每次访问一个新点，保证 n 个点全访问到；内层循环 j：继续寻找还未访问的点，找 dis 最小的访问；内层循环 k：更新所有能够更新的 dis</p></blockquote><hr><p><span id="1004"></span></p><h2 id="1004-Counting-Leaves-dfs-bfs-树算法"><a href="#1004-Counting-Leaves-dfs-bfs-树算法" class="headerlink" title="1004 Counting Leaves (dfs/bfs 树算法)"></a>1004 Counting Leaves (dfs/bfs 树算法)</h2><h3 id="1-题目大意-1"><a href="#1-题目大意-1" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>给出一棵树，问每一层各有多少个叶子结点</p><h3 id="2-坑"><a href="#2-坑" class="headerlink" title="2. 坑"></a>2. 坑</h3><pre><code>list[tmp].lev = list[fa].lev + 1;  //出问题,不一定按照顺序输入</code></pre><p>有可能先 03 后 01，fa 的 lev 还没确定，不能给孩子节点 +1</p><pre><code>3 203 1 0201 1 03</code></pre><h3 id="3-个人代码-1"><a href="#3-个人代码-1" class="headerlink" title="3. 个人代码"></a>3. 个人代码</h3><p><a href="https://pintia.cn/problem-sets/994805342720868352/problems/994805521431773184" target="_blank" rel="noopener">PTA-1004</a></p><h3 id="4-正确方法"><a href="#4-正确方法" class="headerlink" title="4. 正确方法"></a>4. 正确方法</h3><pre><code class="lang-cpp">void dfs(int fa){    for (auto &amp;ch : list[fa].v) {        list[ch].lev = list[fa].lev + 1;        dfs(ch);    }}dfs(1);   //通过 DFS 一层层给叶子节点 lev+1</code></pre><hr><p><span id="1007"></span></p><h2 id="1007-Maximum-Subsequence-Sum-最大子序列和"><a href="#1007-Maximum-Subsequence-Sum-最大子序列和" class="headerlink" title="1007 Maximum Subsequence Sum (最大子序列和)"></a>1007 Maximum Subsequence Sum (最大子序列和)</h2><h3 id="1-题目大意-2"><a href="#1-题目大意-2" class="headerlink" title="1.题目大意"></a>1.题目大意</h3><p>求最大连续子序列和，输出最大的和以及这个子序列的开始值和结束值。如果所有数都小于0，那么认为最大的和为0，并且输出首尾元素</p><h3 id="2-分析-1"><a href="#2-分析-1" class="headerlink" title="2.分析"></a>2.分析</h3><p>本质上是<strong>动态规划</strong>的思想，数组为<code>vec[]</code>，设<code>dp[i]</code> 是以<code>vec[i]</code>结尾的子数组的最大和，对于元素<code>vec[i+1]</code>, 它有两种选择：<code>vec[i+1]</code>接着前面的子数组构成最大和; <code>vec[i+1]</code>自己单独构成子数组。则<code>dp[i+1] = max{dp[i]+vec[i+1],  vec[i+1]}</code></p><p>简化则用一个 temp_sum 和一个 temp_first 解决，真正最大和为 sum，起始点为 first 和 last，建立局部和全局的关系。如果 <code>temp_sum &lt; 0</code>, 说明目前这一段对后续的序列和已经没有加成作用，可以舍弃另立门户，令<code>temp_sum = 0</code></p><h3 id="3-个人代码-2"><a href="#3-个人代码-2" class="headerlink" title="3. 个人代码"></a>3. 个人代码</h3><pre><code class="lang-cpp">//初始值设置容易坑：sum要为-1 才能开始int sum=-1, temp_sum=0, first=0, last=k-1, temp_first=0;for (int j = 0; j &lt; k; ++j) {    temp_sum += a[j];    if(temp_sum &lt; 0){        temp_sum = 0;        temp_first = j+1;   //本段已经可以舍弃，开始新一段    } else if(temp_sum &gt; sum){        sum = temp_sum;        first = temp_first;        last = j;    }}if(sum &lt; 0) sum = 0;  //如果全负则为 0，根据题目要求不要漏情况</code></pre><h3 id="4-类似题"><a href="#4-类似题" class="headerlink" title="4. 类似题"></a>4. 类似题</h3><p><a href="https://www.cnblogs.com/grandyang/p/4480780.html" target="_blank" rel="noopener">最长无重复子串</a></p><hr><p>)<span id="1010"></span></p><h2 id="1010-Radix-进制转换-二分法"><a href="#1010-Radix-进制转换-二分法" class="headerlink" title="1010 Radix (进制转换/二分法"></a>1010 Radix (进制转换/二分法</h2><h3 id="1-题目大意-3"><a href="#1-题目大意-3" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>给定两个相等数，已知一个数的进制，求另外一个数的进制（radix-&gt;基数）</p><h3 id="2-分析-2"><a href="#2-分析-2" class="headerlink" title="2. 分析"></a>2. 分析</h3><p>通用函数<code>cal(string str, int radix)</code>把任意进制的数转换为 10 进制数<br>在使用搜索遍历，找到另一个数对应的进制</p><h3 id="3-个人代码-3"><a href="#3-个人代码-3" class="headerlink" title="3.个人代码"></a>3.个人代码</h3><p><a href="https://pintia.cn/problem-sets/994805342720868352/problems/994805507225665536" target="_blank" rel="noopener">PTA-1010</a></p><p>转换函数</p><pre><code class="lang-cpp">long long cal(string str, long long radix){    long long target=0, bak = 1;    reverse(str.begin(), str.end());    for (char c : str) {        if(isdigit(c))            target += (c - &#39;0&#39;) * bak;        else            target += (c - &#39;a&#39; + 10) * bak;        bak *= radix;  //也可以使用&lt;cmath&gt;的 pow()函数    }    return target;}</code></pre><p>搜索函数</p><pre><code class="lang-cpp">long long find_radix(string str, long long tar){    char c = *max_element(str.begin(), str.end());  //最大字母    //进制至少比最大字母要大    long long low = isdigit(c) ? c-&#39;0&#39; : c-&#39;a&#39;+10;    low += 1;  //必须要加一,至少多1    long long high = max(low, tar);  //进制最大不会大于目标 tar    while (low &lt;= high){        long long mid = (low+high)/2;        long long tmp = cal(str, mid);        if(tmp &lt; 0 || tmp &gt; tar) high = mid - 1;  //小于 0 也是进制太大        else if(tmp == tar) return mid;        else low = mid + 1;    }    return -1;}</code></pre><h3 id="4-学习要点-1"><a href="#4-学习要点-1" class="headerlink" title="4. 学习要点"></a>4. 学习要点</h3><blockquote><ol><li>进制转换这种大数字的乘法很容易溢出，必须要用 <code>long long</code></li><li>暴力搜索（从 2 到 ∞ 容易超时）最好用二分法，并且限定范围：最小是 <code>low</code> 至少是最大的那个字母+1，比如 fff 最少是 15+1=16 进制，最大是<code>high</code>不超过目标 <code>tar</code></li><li>可以使用反向迭代器 <code>it = n.rbegin(); it != n.rend()</code>代替<code>reverse()</code></li></ol></blockquote><hr><h2 id="1012-The-Best-Rank-数据结构设计-多维度排序"><a href="#1012-The-Best-Rank-数据结构设计-多维度排序" class="headerlink" title="1012 The Best Rank (数据结构设计/多维度排序)"></a>1012 The Best Rank (数据结构设计/多维度排序)<span id="1012"></span></h2><h3 id="1-题目大意-4"><a href="#1-题目大意-4" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>找出每个人自己最优势的科目，也就是单独排名最好的科目，优先级 A &gt; C &gt; M &gt; E</p><ul><li>Sample Input:<pre><code>5 6310101 98 85 88310102 70 95 88310103 82 87 94310104 91 91 91310105 85 90 90310101310102310103310104310105999999</code></pre></li><li>Sample Output:<pre><code>1 C1 M1 E1 A3 AN/A</code></pre></li></ul><h3 id="2-个人代码"><a href="#2-个人代码" class="headerlink" title="2. 个人代码"></a>2. 个人代码</h3><p><a href="https://pintia.cn/problem-sets/994805342720868352/problems/994805502658068480" target="_blank" rel="noopener">PTA-1012</a></p><h3 id="3-学习要点"><a href="#3-学习要点" class="headerlink" title="3. 学习要点"></a>3. 学习要点</h3><p>数据结构设计</p><pre><code class="lang-cpp">struct node{    //一定要用数组形式，避免过多变量 int c, m, e, c_r, m_r, e_r, a_r;    int id, best;    int score[4], rank[4];}stu[2001];  //输入学生序列，非学号排序int exist[1000000], flag=-1; //exist 至少 &gt;0, 顺便建立 id-输入序号 映射//使用 flag 配置作用，避免写过多重复的 cmp 函数bool cmp(node &amp;a, node &amp;b){ return a.score[flag] &gt; b.score[flag];}</code></pre><p>输入后，利用<code>exist[stu[k].id] = k + 1; k++</code> 依次建立映射<br>输出时，利用<code>cin &gt;&gt; tmp; ex = exist[tmp]</code>和<code>stu[ex-1]</code>实现映射，无需 find<br>处理时，使用<code>sort(stu, stu+n, cmp)</code> 循环 <code>flag++</code>，实现分学科多次排序，确定 <code>rank[]</code></p><hr><p><span id="1013"></span></p><h2 id="1013-Battle-Over-Cities（图的遍历，统计强连通分量的个数，dfs）"><a href="#1013-Battle-Over-Cities（图的遍历，统计强连通分量的个数，dfs）" class="headerlink" title="1013 Battle Over Cities（图的遍历，统计强连通分量的个数，dfs）"></a>1013 Battle Over Cities（图的遍历，统计强连通分量的个数，dfs）</h2><h3 id="1-题目大意-5"><a href="#1-题目大意-5" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>给出n个城市之间有相互连接的m条道路，当删除一个城市和其连接的道路的时候，问其他几个剩余的城市至少要添加多少个路线，才能让它们重新变为连通图</p><h3 id="2-分析-3"><a href="#2-分析-3" class="headerlink" title="2. 分析"></a>2. 分析</h3><p>n 连通分量，最少需要 n-1 条边相连，所以本题实质就是求「去除某点及其相连的边」后，剩余的「连通分量个数」是一个图算法，需要用图的遍历来解决 (DFS)</p><blockquote><p>求连通分量依据：一次 <code>dfs()</code>走到底，可以访问完一个连通分量</p></blockquote><p>数据结构：</p><blockquote><p>涉及图的遍历，要考虑 <code>visit[]</code>数组<br>图的路径存储，使用二维矩阵</p></blockquote><h3 id="3-个人代码-4"><a href="#3-个人代码-4" class="headerlink" title="3. 个人代码"></a>3. 个人代码</h3><p><a href="https://pintia.cn/problem-sets/994805342720868352/problems/994805500414115840" target="_blank" rel="noopener">PTA-1013</a></p><pre><code class="lang-cpp">for (int i = 0; i &lt; k; ++i) {        int cur, cnt=0;        cin &gt;&gt; cur;        fill(visited, visited+n, false);        visited[cur] = true;  //相当于把本点去除        for (int j = 1; j &lt;= n; ++j) {  //依次计算所有连通分量，而非从本点出发            if(!visited[j]){                dfs(j);                cnt++;            }        }        cout &lt;&lt; cnt-1 &lt;&lt; endl;    }</code></pre><pre><code class="lang-cpp">void dfs(int st){   //只用来 visit，不负责计数    visited[st] = true;    for (int i = 0; i &lt;= n; ++i)        if(!visited[i] &amp;&amp; e[st][i]){   //未访问且有路            dfs(i);        }}</code></pre><hr><p><span id="1014"></span></p><h2 id="1014-Reversible-Primes（队列应用-排队问题"><a href="#1014-Reversible-Primes（队列应用-排队问题" class="headerlink" title="1014 Reversible Primes（队列应用 排队问题"></a>1014 Reversible Primes（队列应用 排队问题</h2><h3 id="1-题目大意-6"><a href="#1-题目大意-6" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>n个窗口，每个窗口可以排队m人。有k位用户需要服务，给出了每位用户需要的minute数，所有客户在8点开始服务，如果有窗口还没排满就入队，否则就在黄线外等候。如果有某一列有一个用户走了服务完毕了，黄线外的人就进来一个。如果同时就选窗口数小的。求q个人的服务结束时间。<br>如果一个客户在17:00以及以后还没有开始服务（此处不是结束服务是开始17:00）就不再服务输出sorry；如果这个服务已经开始了，无论时间多长都要等他服务完毕。</p><h3 id="2-学习要点"><a href="#2-学习要点" class="headerlink" title="2. 学习要点"></a>2. 学习要点</h3><p>面对此类题目，总想按照 timeline 每分每秒去决策，其实队列的问题用队列解决。</p><p>以每个窗口为单位设计结构体，<code>poptime</code> 用于刚入黄线的人决策，<code>endtime</code> 用于已经在队内的人是否会 sorry 决策，<code>q</code> 表示本队列内排队情况。</p><p>给定客户序列，则以每一个客户为循环单位<code>(index++)</code>，队列有一个进就会有一个出，每次变动一个单位，更新各个队列的<code>poptime endtime q</code>等变量。</p><pre><code class="lang-cpp">struct node {    int poptime, endtime;  //队首的人出队（结束）的时间, 队尾的人结束的时间    queue&lt;int&gt; q;};int index = 1;    //第 1 步：n*m 个客户先抢占黄线内, 直接瞬间涌入，不存在选择    for (int i = 1; i &lt;= cap; ++i) {    //看做行和列，先抢第一行        for (int j = 1; j &lt;= n; ++j) {            if(index &lt;= k){                win[j].q.push(time[index]);                if(win[j].endtime &gt;= 540)                    sorry[index] = true;                win[j].endtime += time[index];                if(i == 1)  //对于第一行，出队时间即本队结束时间，做一个初始化                    win[j].poptime = win[j].endtime;                res[index] = win[j].endtime;  //刚进来的，自己是队尾，队伍结束就是自己结束                index++;            }        }    }    //第 2 步：后续的客户    while (index &lt;= k){   //while一次循环表示一个客户的选择        int temp_min = win[1].poptime, temp_win = 1;  //初始化        for (int i = 2; i &lt;= n; ++i) {   //找最早出队的            if(win[i].poptime &lt; temp_min){                temp_win = i;                temp_min = win[i].poptime;            }        }        win[temp_win].q.pop();        win[temp_win].q.push(time[index]);        win[temp_win].poptime += win[temp_win].q.front(); //更新：最前面那个人完成的时间就是出队时间        if(win[temp_win].endtime &gt;= 540)  //endtime 还未更新，是看的前一个人的完成时间，是否超时            sorry[index] = true;        win[temp_win].endtime += time[index];  //更新：刚进来的这个人的完成时间就是本队的完成时间        res[index] = win[temp_win].endtime;        index++;    }</code></pre><hr><p><span id="1016"></span></p><h2 id="1016-Phone-Bills（日期类计算"><a href="#1016-Phone-Bills（日期类计算" class="headerlink" title="1016 Phone Bills（日期类计算"></a>1016 Phone Bills（日期类计算</h2><h3 id="1-题目大意-7"><a href="#1-题目大意-7" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>给定固定格式的日期<code>mm:dd:hh:mm</code>计算各个时间的差值，以及每个时间段有不同的费率，计算总的账单费用。</p><h3 id="2-学习要点-1"><a href="#2-学习要点-1" class="headerlink" title="2. 学习要点"></a>2. 学习要点</h3><p>1.数据结构设计<br>因为输入是给定一个个 call 序列，所以便于存储，也以每个 call 为单位，统一放在一个数组里，按照名字、时间排序，计算的时候，只要考虑 i 和 i-1 前后两个元素是否有同一个 name 以及 status 一个为 0 一个为 1 即可。</p><pre><code class="lang-cpp">struct node {    string name;    int status, month, time, day, hour, minute;};bool cmp(node a, node b) {    return a.name != b.name ? a.name &lt; b.name : a.time &lt; b.time;}</code></pre><p>2.日期差值计算<br>直接”01:05:02:24“-”01:04:23:59“ 难以计算，每个小时还有不同的费率，所以 <strong>统一从本月 0 天 0 点开始计算</strong>，以<code>dd:hh:mm</code>格式再相减即可，隔天的情况则加上这一整天</p><pre><code class="lang-cpp">double billFromZero(node call, int *rate) {    double total;    total = rate[call.hour];  //本小时的费用    total += call.minute + rate[24] * 60 * call.day;  //从本月第 0 天到今天    for (int i = 0; i &lt; call.hour; i++)        total += rate[i] * 60;  //本天内 0 点到现在累加    return total / 100.0;}</code></pre><hr><h2 id="1018-Public-Bike-Management（图最短路径-Dijkstra-DFS）"><a href="#1018-Public-Bike-Management（图最短路径-Dijkstra-DFS）" class="headerlink" title="1018 Public Bike Management（图最短路径 Dijkstra + DFS）"></a>1018 Public Bike Management（图最短路径 Dijkstra + DFS）<span id="1018"></span></h2><p>图例经过 Dijkstra 之后的结果</p><pre><code class="lang-cpp">dis[1] = 1      pre[1] = {0}dis[2] = 1      pre[2] = {0}dis[3] = 2      pre[3] = {1, 2}</code></pre><hr><p><span id="1020"></span></p><h2 id="1020-Tree-Traversals-已知后序和中序，转前序-层序"><a href="#1020-Tree-Traversals-已知后序和中序，转前序-层序" class="headerlink" title="1020 Tree Traversals (已知后序和中序，转前序/层序)"></a>1020 Tree Traversals (已知后序和中序，转前序/层序)</h2><h3 id="1-输入格式"><a href="#1-输入格式" class="headerlink" title="1. 输入格式"></a>1. 输入格式</h3><pre><code>72 3 1 5 7 6 41 2 3 4 5 6 7</code></pre><h3 id="2-分析-4"><a href="#2-分析-4" class="headerlink" title="2. 分析"></a>2. 分析</h3><p>和手动模拟差不多，找到根节点 (pre 的第一个，post 的最后一个) -&gt; 找到左右两段的端点，分别作为左右子树 -&gt; 递归重复。关键在于递归的写法，端点的寻找。</p><h3 id="3-个人代码-5"><a href="#3-个人代码-5" class="headerlink" title="3. 个人代码"></a>3. 个人代码</h3><p><code>level</code> 的作用即按照 <code>index</code> 存储，利用 2n+1 2n+2 的公式可以定位左右孩子，而索引的顺序恰好是层序遍历的顺序（具体第几层不知道）<br>转为前序遍历，只需在 pre 递归前打印 root 即可</p><pre><code class="lang-cpp">vector&lt;int&gt; in, post, level(100000, -1);void pre(int root, int start, int end, int index){      //start end 都是相对于中序序列, 但是 root 是相对于 post 序列的！    if(start &gt; end) return;    level[index] = post[root];    int i = start;    while (i &lt; end &amp;&amp; in[i] != post[root]) i++;  //找到中序 in 序列中的根节点    int left_size = i-start, right_size = end-i;    pre(root-right_size-1, start, i-1, index*2+1);  //此 root 即为左孩子节点，可以确定为 index*2+1    pre(root-1, i+1, end, index*2+2);    //找到根节点 i，左子树 start ~ i-1, 右子树 i+1 ~ end}pre(n-1, 0, n-1, 0);</code></pre><h3 id="4-类题"><a href="#4-类题" class="headerlink" title="4. 类题"></a>4. 类题</h3><p><a href="https://pintia.cn/problem-sets/994805342720868352/problems/994805380754817024" target="_blank" rel="noopener">PTA-1086</a>（已知前序和中序遍历，求后序遍历）<br>给出用栈进行中序遍历的过程，求后序遍历。有一个隐含条件，push 的顺序即为前序遍历，相对于已知 pre 和 in，求 post，同样找 root, start, end 即可。</p><hr><p><span id="1021"></span></p><h2 id="1021-Deepest-Root-求树中最长的路径，DFS，连通分量"><a href="#1021-Deepest-Root-求树中最长的路径，DFS，连通分量" class="headerlink" title="1021 Deepest Root (求树中最长的路径，DFS，连通分量)"></a>1021 Deepest Root (求树中最长的路径，DFS，连通分量)</h2><h3 id="1-题目大意-8"><a href="#1-题目大意-8" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>给出n个结点（1~n）之间的n条边，问是否能构成一棵树，如果不能构成则输出它有的连通分量个数；如果能构成一棵树，输出能构成最深的树的高度时，树的根结点。如果有多个，按照从小到大输出。</p><h3 id="2-分析-5"><a href="#2-分析-5" class="headerlink" title="2. 分析"></a>2. 分析</h3><p>连通分量的个数：基本套路，一次 DFS 走完一个连通分量，统计循环中 DFS 开启的次数即可<br>本题关键在于，两次 DFS 即可求出所有最远端点（即所有最长路径中的所有端点）如果是一棵树的话，线段无非是两个端点，第一次 DFS 能找到一端，在这些最远端点中（可能有多个同样最远的，可能只有一个最远的）随便选一个作为起点，再来一次 DFS，即可找到另一端的最远端点。</p><h3 id="3-个人代码-6"><a href="#3-个人代码-6" class="headerlink" title="3. 个人代码"></a>3. 个人代码</h3><p><a href="https://pintia.cn/problem-sets/994805342720868352/problems/994805482919673856" target="_blank" rel="noopener">PTA-1003</a></p><pre><code class="lang-cpp">void dfs(int node, int height) {    if(height &gt; maxheight) {        temp.clear();        temp.push_back(node);        maxheight = height;    } else if(height == maxheight){        temp.push_back(node);    }    visit[node] = true;    for(int i = 0; i &lt; v[node].size(); i++) {        if(visit[v[node][i]] == false)            dfs(v[node][i], height + 1);    }</code></pre><h3 id="4-做题-Tips"><a href="#4-做题-Tips" class="headerlink" title="4. 做题 Tips"></a>4. 做题 Tips</h3><ol><li>本题不用考虑权重，所以无需开一个二维数组用邻接矩阵存储<br><code>e[10010][10010]; e[a]=e[b]=weight;</code><br>只需使用邻接表，把和自己直接相连的点存起来即可<br><code>v[node][i]; resize(n+1);</code><br><br></li><li>DFS 基本套路<pre><code class="lang-cpp"> dfs(int node, ...){   //遍历过程中顺带做其它操作     visit[node] = true;     for 与本点直接相连的 i:         if(!visit[i])             dfs(i, ...) }</code></pre></li></ol><hr><p><span id="1102"></span></p><h2 id="1102-Invert-a-Binary-Tree-翻转二叉树"><a href="#1102-Invert-a-Binary-Tree-翻转二叉树" class="headerlink" title="1102 Invert a Binary Tree (翻转二叉树)"></a>1102 Invert a Binary Tree (翻转二叉树)</h2><h3 id="1-题目大意-9"><a href="#1-题目大意-9" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>给定一棵二叉树，先翻转好，输出层序遍历和中序遍历</p><h3 id="2-分析-6"><a href="#2-分析-6" class="headerlink" title="2. 分析"></a>2. 分析</h3><p>可以写一个 invert 函数，交换本节点左右元素，然后对左右孩子递归即可。</p><pre><code class="lang-cpp">struct node{    int left=-1, right=-1;  //-1 表示 NULL};void invert(int r){    int a = v[r].left, b = v[r].right;    if(a == -1 &amp;&amp; b == -1)        return;    v[r].left = b;    v[r].right = a;    if(v[r].left != -1) invert(v[r].left);    if(v[r].right != -1) invert(v[r].right);}</code></pre><p>也可以不用写，正常遍历即可，只是先遍历右孩子，再遍历左孩子即可。</p><pre><code class="lang-cpp">//层序遍历标准模式：void level(int r){    queue&lt;int&gt; q;    q.push(r);    while (!q.empty()){        r = q.front();        q.pop();        // if(q.empty()) break;        le_ans.push_back(r);        if(v[r].left != -1) q.push(v[r].left);        if(v[r].right != -1) q.push(v[r].right);    }}</code></pre><h3 id="3-坑点："><a href="#3-坑点：" class="headerlink" title="3. 坑点："></a>3. 坑点：</h3><ol><li>一般的题，都要考虑元素可能有重复，可能不按顺序。本题使用    <code>mark</code>数组确定<code>root</code>是最好的方式</li><li><code>queue</code>和一般的<code>vector</code>有本质的不同，不能用迭代器遍历，不能索引，只能通过<code>front(), back()</code>访问。<code>quque</code>进行<code>pop(),front()</code>等操作时，一定要检查<code>q.empty()</code></li><li>当<code>queue</code>中只有一个元素时，<code>pop()</code>后，在访问<code>front()</code>可能会出现异常数字</li></ol><hr><p><span id="1040"></span></p><h2 id="1040-Longest-Symmetric-String-求最长对称子串-DP"><a href="#1040-Longest-Symmetric-String-求最长对称子串-DP" class="headerlink" title="1040 Longest Symmetric String (求最长对称子串 DP)"></a>1040 Longest Symmetric String (求最长对称子串 DP)</h2><h3 id="1-题目大意-10"><a href="#1-题目大意-10" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>给定一个字符串，找到最长的、对称的子串，输出长度<br>比如<code>Is PAT&amp;TAP symmetric?</code> 输出 11, 即<code>s PAT&amp;TAP s</code>的长度</p><h3 id="2-分析-7"><a href="#2-分析-7" class="headerlink" title="2. 分析"></a>2. 分析</h3><ol><li><p>基本递归的思路，大问题拆成小问题，拆成一个已知条件下的具体问题。对于 子串/子序列 问题，要么用 <code>dp[i][j]</code>表示 i 和 j 的子串，要么用 <code>dp[i]</code>表示 0 到 i 之间的子串.</p></li><li><p>本题要求每一对<code>dp[i][j]</code>就要已知<code>dp[i+1][j-1]</code>, 再根据<code>str[i]</code>和<code>str[j]</code>来判断本段是否对称, 复杂问题被简化, 逐层逐层抛弃.</p></li><li><p>dp 的另外一个关键就在于初始化, 本题将所有<code>dp[i][i]</code>和相邻相等的所有<code>dp[i][i-1]</code>赋值为 1</p></li><li><p>本题一个难点在于, 普通的 dp 都可以从序列开头递归即可, 本题要根据子串的长度, 从短到长进行循环, 从中沉淀出最长的 length</p></li></ol><h3 id="3-个人代码-7"><a href="#3-个人代码-7" class="headerlink" title="3. 个人代码"></a>3. 个人代码</h3><p><a href="https://pintia.cn/problem-sets/994805342720868352/problems/994805446102073344" target="_blank" rel="noopener">PTA-1040</a></p><pre><code class="lang-c++">for (int L = 3; L &lt;= len; ++L) {   //线段长度由小到大        for (int i = 0; i &lt; len - L + 1; ++i) {  //能够到达的最右端点            int j = i + L - 1;  //本次 i 对应的右端点 j            if(str[i] == str[j] &amp;&amp; dp[i+1][j-1]){  //更新dp                dp[i][j] = 1;                ans = L;            }        }    }</code></pre><hr><p><span id="1045"></span></p><h2 id="1045-Favorite-Color-Stripe-最长不下降子序列-DP"><a href="#1045-Favorite-Color-Stripe-最长不下降子序列-DP" class="headerlink" title="1045 Favorite Color Stripe (最长不下降子序列 DP)"></a>1045 Favorite Color Stripe (最长不下降子序列 DP)</h2><h3 id="1-题目大意-11"><a href="#1-题目大意-11" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>给出m中颜色作为喜欢的颜色（同时也给出顺序），然后给出一串长度为L的颜色序列，现在要去掉这个序列中的不喜欢的颜色，然后求剩下序列的一个子序列，使得这个子序列表示的颜色顺序符合自己喜欢的颜色的顺序，不一定要所有喜欢的颜色都出现。<br>1 - N 表示 N 种颜色, 比如给出 2 3 1 5 6 表示自己喜欢的颜色序列</p><h3 id="2-分析-8"><a href="#2-分析-8" class="headerlink" title="2. 分析"></a>2. 分析</h3><p>在原串中要找出符合 2 3 1 5 6 顺序的所有子串, 可能需要多层循环遍历, 还要用额外数组 tmp 临时记录, 数据难以比较和判断, 很麻烦. 所以考虑转化成有序数组, 一一对应成 1 2 3 4 5, 通过比大小即可判断, 等价于转化成一个<strong>最长不下降子序列</strong>问题.</p><p>维护单调数组 dp, <code>dp[i]</code>表示从 0 - i 最长不下降子序列的长度, 可以 DP 的精髓就在于, 本位置 i 对应的<code>dp[i]</code>可以由前面的<code>dp[0]</code> — <code>dp[i-1]</code> 直接推出.</p><p>举例:</p><pre><code>i  2 1 5 3 6 4 8 9 7dp 1 1 2 2 3 3 4 5 4j  2 1 5 3 6 4 8 9 7</code></pre><p>此过程中沉淀出的 maxn 就是 5</p><h3 id="3-个人代码-8"><a href="#3-个人代码-8" class="headerlink" title="3. 个人代码"></a>3. 个人代码</h3><pre><code class="lang-c++">int book[201], a[10001], dp[10001];//dp[i]内存储的是, 从 0 到 i 为止最长的长度, 与 book 无关int main(){    int n, m, l, tmp, maxn = -1;    cin &gt;&gt; n &gt;&gt; m;    for (int i = 1; i &lt;= m; ++i) {  //颜色为 1-n 将颜色序列映射为 idx 递增序列        cin &gt;&gt; tmp;        book[tmp] = i;    }    cin &gt;&gt; l;    int num = 0;    for (int i = 0; i &lt; l; ++i) {  //关键一步，把所有颜色转化为序号，依次存入 a 中        cin &gt;&gt; tmp;        if(book[tmp] &gt;= 1){            a[num++] = book[tmp];  //a 存储 idx        }    }    for (int i = 0; i &lt; num; ++i) {   //不喜欢的颜色直接忽略了,不计入考察范围        dp[i] = 1;   //每次初始赋值为 1, 检查 0-i        for (int j = 0; j &lt; i; ++j) {            if(a[i] &gt;= a[j]){   //以 a 为评判标准, 大于等于就能做下一步                dp[i] = max(dp[i], dp[j]+1);                //核心思想，已知0 ~ i-1 的最大值，新来的 i 最多比0 ~ i-1 多 1            }        }        maxn = max(maxn, dp[i]);    }    cout &lt;&lt; maxn;    return 0;}</code></pre><hr><p><span id="1068"></span></p><h2 id="1068-Find-More-Coins-01-背包问题-DP"><a href="#1068-Find-More-Coins-01-背包问题-DP" class="headerlink" title="1068 Find More Coins (01 背包问题 DP)"></a>1068 Find More Coins (01 背包问题 DP)</h2><h3 id="1-题目大意-12"><a href="#1-题目大意-12" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>用n个硬币买价值为m的东西，输出使用方案，使得正好几个硬币加起来价值为m。从小到大排列，输出最小的那个排列方案</p><h3 id="2-尝试-DFS"><a href="#2-尝试-DFS" class="headerlink" title="2. 尝试(DFS)"></a>2. 尝试(DFS)</h3><p>本题可以采用递归方案, DFS 递归到底, 直到刚好<code>target==0</code>即可, 类似于二叉树的寻找路径和. 但是某个测试点会超时.</p><p>核心代码(DFS 递归):</p><pre><code class="lang-c++">bool sear(int st, int target){   //搜索 st ~ n, 每次从 st 搜到末位    if(0 == target)        return true;    for (int i = st; i &lt; n; ++i) {        if(target &lt; a[i])   //后面的面额只会越来越大,不用搜了(剪枝)            return false;        tmp.push_back(a[i]);        if (sear(i+1, target-a[i]))  //target减去当前,大目标拆成小目标            return true;   //找到一个为 true 就不用再搜了,返回        else tmp.pop_back();  //sear()==false 表示走不通, pop出来换下一个    }    return false;}sort(a, a+n);  //从小到大排序, 便于输出最小的方案if(a[0] &gt; m){ printf(&quot;No Solution\n&quot;); return 0; } //面额太大,付不了小钱for (int i = 0; i &lt; n; ++i) {  //逐个作为起点进行搜索    tmp.clear();    if(sear(i, m))        break;}</code></pre><h3 id="3-0-1背包问题-DP"><a href="#3-0-1背包问题-DP" class="headerlink" title="3. 0-1背包问题(DP)"></a>3. 0-1背包问题(DP)</h3><p>给定 n 种物品和一个容量为 C 的背包，物品 i 的重量是 wi，其价值为 vi 。</p><p>问：应该如何选择装入背包的物品，使得装入背包中的物品的总价值最大？<br>DP 的题还是要用 DP 来做, 关键在于搞清楚每一个数组的作用</p><p>🎒背包问题的经典转移公式</p><pre><code class="lang-c++">if(j &gt;= w[i])    dp[i][j] = max(dp[i-1][j], dp[i-1][j-w[i]]+v[i]);else    dp[i][j] = dp[i-1][j];  //面对这个 i 物品, 不拿(或装不下), 就等同于 i-1 的情况</code></pre><p><code>dp[i][j]</code> 表示 在面对第 i 件物品，且背包容量为 j 时所能获得的最大价值<br><code>dp[i-1][j-w[i]]+v[i]</code>则是容量空出<code>w[i]</code>的情况下( i-1表示回溯一个序号), 装入<code>v[i]</code>所能得到的价值.</p><h3 id="4-正解-DP"><a href="#4-正解-DP" class="headerlink" title="4. 正解(DP)"></a>4. 正解(DP)</h3><pre><code>8 95 9 8 7 2 3 4 1</code></pre><pre><code>1 3 5</code></pre><pre><code class="lang-c++">int dp[101], choice[10001][101], w[10001];//dp[j] 表示容量为 j 的时候, 最多能装多少钱数; w为每个硬币的面额//choice[i][j]=true 表示总目标钱数为 j 时, i 可以作为一个选择</code></pre><p>本题不是求最大价值, 只要求刚好装满, 只需要加一个判断<code>if(dp[m] != m)</code>, 然后以 m 为目标, 添加所有 choice 筛选的 [<strong>总目标钱数为 m 时, 可以作为一个选择</strong>] 的序列</p><p>本题的判断精髓就在这一句<code>if(dp[j] &lt;= dp[j-w[i]] + w[i])</code> 含义就是, 选择 w[i] 之后, 也能凑得和<code>dp[j]</code>之前 <strong>一样的, 甚至更多的</strong> 钱数.</p><p>本题实际发挥作用的时候, 小于号<code>&lt;</code>实际发挥的作用就是初始化, 让<code>dp[j]</code>从0变成一个数值, 比如<code>dp[9]=9</code>, 9 的面额直接满足; 等于号<code>=</code>实际发挥的作用就是寻找新的路径, 更新<code>choice[i][j]</code> 让<code>1 3 5</code>也组成进来.</p><p>比如, i=5 的时候, w[i]=4 已经更新了 dp[9]=9, dp[5]=5, 此时 j 从 m 遍历递减, 又从 9 开始, 有<code>dp[9] = dp[5] + w[i]</code>, 就把 i=5, 即 w[i]=4 添加进<code>choice[9][5] = true</code>, 作为路径之一</p><pre><code class="lang-c++">sort(w + 1, w + n + 1, cmp);  //从大的面额找起for (int i = 1; i &lt;= n; ++i) {    for (int j = m; j &gt;= w[i]; --j) {    //容量从 m 递减,直到比 w[j] 小则装不下,忽略        if(dp[j] &lt;= dp[j-w[i]] + w[i]){            choice[i][j] = true;            dp[j] = dp[j-w[i]] + w[i];        }    }}if(dp[m] != m) cout &lt;&lt; &quot;No Solution&quot;;else{    int v = m, index = n;  //v 表示目标容量, n 表示初始序号在末尾(从最小的选起)    while (v &gt; 0){        if(choice[index][v]){  //目标为 v 的时候, w[index] 可以作为选项            arr.push_back(w[index]);            v -= w[index];        }        index--;    }    //输出 arr}</code></pre><h3 id="5-后记"><a href="#5-后记" class="headerlink" title="5. 后记"></a>5. 后记</h3><p>一道基础的背包问题, 居然思考了我一整个下午, 算法基础真的薄弱. 鉴于网上的题解要么不够大佬, 代码不够最优; 要么太过于大佬, 解释得很简略. 于是我吧最大佬的代码, 用最清楚的语言解释, 是对自己思路的一个锻炼, 也希望帮助后来的萌新们.</p><hr><p><span id="1044"></span></p><h2 id="1044-Shopping-in-Mars-特定值子序列和-二分查找"><a href="#1044-Shopping-in-Mars-特定值子序列和-二分查找" class="headerlink" title="1044 Shopping in Mars (特定值子序列和, 二分查找)"></a>1044 Shopping in Mars (特定值子序列和, 二分查找)</h2><h3 id="1-题目大意-13"><a href="#1-题目大意-13" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>求一串的数字中连续的一段，使得这个连续的段内数字的和恰好等于所期望的值m. 如果不能找到恰好等于，就找让自己付出最少的价格（总和必须大于等于所给值）的那段区间, 求所有可能的结果.<br>简而言之, 就是找一个连续子序列, 恰好等于 target</p><h3 id="2-个人代码-1"><a href="#2-个人代码-1" class="headerlink" title="2. 个人代码"></a>2. 个人代码</h3><p>因为给定的金币序列是乱序的, 这题一开始没想到可以二分法. 其实题目要求的是连续的子段, 那么就可以逐级叠加求<code>sum</code>, <code>a[j] - a[i-1]</code>即是 i~j 这一段的子段和. 而这个逐级叠加的<code>sum</code>数组, 就是一个完美的递增序列.</p><p>注意解题技巧, 需要多个参数时, 可以采用<code>int &amp;j, int &amp;temp_sum</code>引用传参.</p><pre><code class="lang-c++">#include &lt;iostream&gt;#include &lt;stdlib.h&gt;#include &lt;vector&gt;#include &lt;map&gt;#include &lt;algorithm&gt;using namespace std;vector&lt;int&gt; sum, ans;int n, m;// 从 i 开始, 以 n 结尾的序列, 二分查找// 目标: 找到右端点, 刚好比左端点大 mvoid search(int i, int &amp;j, int &amp;temp_sum){    int left = i, right = n;    int target = sum[left-1] + m;    while (left &lt; right){        // 保留右侧的二分查找, 不求找到而退出        // 最后让 left==right 时退出, 此时是比 target 大的最小数        // 因为 mid 小了的时候, 总是舍弃左端点, mid 大了的时候, 右端点被保留        int mid = (left+right)/2;        if(sum[mid] &lt; target)            left = mid + 1;        else            right = mid;    }    j = right;    temp_sum = sum[right] - sum[i-1];}int main(){    cin &gt;&gt; n &gt;&gt; m;    sum.resize(n+1);    sum[0] = 0;    for (int i = 1; i &lt;= n; ++i) {  //转化为递增序列, 得以应用二分, 提高效率        cin &gt;&gt; sum[i];        sum[i] += sum[i-1];    }    int min_ans = sum[n];  //求比 m 大的最小和    for (int i = 1; i &lt;= n; ++i) {        int j=0, temp_sum=0;  //仅仅作为参数传回来而已        search(i, j, temp_sum);        if(temp_sum &lt; m) continue;        else{            if(temp_sum &lt; min_ans){                min_ans = temp_sum;                ans.clear();                ans.push_back(i);                ans.push_back(j);            }            else if (temp_sum == min_ans){                ans.push_back(i);                ans.push_back(j);            }        }    }    for (int i = 0; i &lt; ans.size(); i += 2) {        cout &lt;&lt; ans[i] &lt;&lt; &quot;-&quot; &lt;&lt; ans[i+1] &lt;&lt; endl;    }    return 0;}</code></pre><hr><p><span id="1085"></span></p><h2 id="1085-Perfect-Sequence-二分查找-upper-bound-双指针"><a href="#1085-Perfect-Sequence-二分查找-upper-bound-双指针" class="headerlink" title="1085 Perfect Sequence (二分查找, upper_bound, 双指针)"></a>1085 Perfect Sequence (二分查找, upper_bound, 双指针)</h2><h3 id="1-题目大意-14"><a href="#1-题目大意-14" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>设这个数列中的最大值是M，最小值是m，如果M &lt;= m * p，则称这个数列是完美数列. 现在给定一个序列, 求满足完美序列的最长子序列长度</p><h3 id="2-二分法"><a href="#2-二分法" class="headerlink" title="2.二分法"></a>2.二分法</h3><p>本题可以应用 upper_bound 来查找, 既然要求满足最大的<code>max &lt;= p*min</code>, 那么找到第一个<code>max &gt; p*min</code>即可, 即找到第一个大于target 的元素.</p><p><strong>upper_bound 代码</strong></p><pre><code class="lang-c++">int bin_sear_upper(int left, int right, int target){    int mid;    while (left &lt; right){        mid = (left+right)/2;        if(a[mid] &gt; target)            right = mid;        else            left = mid + 1;    }    return right;}</code></pre><p>关键点 1: <code>left &lt; right</code>到最后必然会夹出 left==right 返回即可.<br>而普通的查找定值<code>if(a[mid]==target)</code>, 需要<code>left &lt;= right</code>作为条件, 找不到就返回-1</p><p>关键点 2: 先判断<code>&gt;</code>, 让<code>right = mid</code>目的是尽可能保留大数,<code>left = mid + 1</code>小数不保留无所谓.</p><p><strong>lower_bound 代码</strong></p><pre><code class="lang-c++">if(a[mid] &gt;= target)    right = mid;else    left = mid + 1;</code></pre><p>只需换为<code>&gt;=</code>即可, 其他完全一样, 也是保大.</p><p>比如 3 4 5 5 7 8 8 8 9, target 为 8.<br>upper_bound 找到的为 3 4 5 5 7 [8] 8 8 9, lower_bound 找到的为 3 4 5 5 7 8 8 8 [9].</p><p>这三种二分法基本可以涵盖所有二分查找的应用.</p><h3 id="3-个人代码-9"><a href="#3-个人代码-9" class="headerlink" title="3. 个人代码"></a>3. 个人代码</h3><pre><code class="lang-c++">int search(int left, long long target){  // 寻找比 target 小的最大数 upper_bound    int right = n-1, mid;    if(a[right] &lt;= target){  //最大的比 target 小        return right;    }    while (left &lt; right){        mid = (left + right)/2;        if(a[mid] &gt; target)            right = mid;        else            left = mid + 1;    }    return right-1;  // 最后 left==right 找到的是第一个比 tar 大的数, -1 即可}</code></pre><p>应用的是 upper_bound, <code>right-1</code>即可得到满足<code>max &lt;= p*min</code>的最大数字.</p><h3 id="4-坑点"><a href="#4-坑点" class="headerlink" title="4. 坑点"></a>4. 坑点</h3><ol><li><code>段错误</code> 数组开的不够大, 递归调用过多</li><li><code>答案错误</code> 发现存在溢出, int 最大管到 $10^9$, 此题乘起来之后会超过, 要用 long long !</li></ol><h3 id="5-Two-points-双指针法"><a href="#5-Two-points-双指针法" class="headerlink" title="5. Two points 双指针法"></a>5. Two points 双指针法</h3><p>瞬间非常简单</p><pre><code class="lang-c++">    sort(v.begin(), v.end());    int result = 0, temp = 0;    for (int i = 0; i &lt; n; i++) {        for (int j = i + result; j &lt; n; j++) {          //result 已经是最长了, 之后的直接从 result 以上的找起            if (v[j] &lt;= v[i] * p) {                temp = j - i + 1;                if (temp &gt; result)                    result = temp;            } else {                break;            }        }    }    cout &lt;&lt; result;</code></pre><p>效率对比差距明显</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/12/Buth7AGTcCpPonS.png" alt="图片替换文本" width="140" height="180"/></p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PAT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>欢迎来到 Hexo</title>
      <link href="/2020/02/02/hello-world/"/>
      <url>/2020/02/02/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><p>本博客采用 Hexo 快速搭建，也欢迎大家上手使用 Hexo，选择自己喜欢的主题，省时省力，工具齐全，生态优越，可扩展性好</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre><code class="lang-bash">$ hexo new &quot;My New Post&quot;</code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre><code class="lang-bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre><code class="lang-bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre><code class="lang-bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
