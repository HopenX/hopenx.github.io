<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>推荐系统与信息检索入门</title>
      <link href="/2020/05/12/rec-sys-notes/"/>
      <url>/2020/05/12/rec-sys-notes/</url>
      
        <content type="html"><![CDATA[<h1 id="推荐系统与信息检索入门"><a href="#推荐系统与信息检索入门" class="headerlink" title="推荐系统与信息检索入门"></a>推荐系统与信息检索入门</h1><h2 id="学习排序方法"><a href="#学习排序方法" class="headerlink" title="学习排序方法"></a>学习排序方法</h2><h3 id="Pointwise"><a href="#Pointwise" class="headerlink" title="Pointwise"></a>Pointwise</h3><ul><li>预测绝对相关度 (比如 RMSE), 是预测专家系统, 人为地给定相关度评估, 提取一系列的特征, 当做<code>回归问题</code>处理.</li><li>存在的问题: 单点精度 $\ne$ 排序精度, 在 $(\hat{y}-y)^2$ 相同的情况下, 排序顺序可能发生变化.</li></ul><h3 id="Pairwise"><a href="#Pairwise" class="headerlink" title="Pairwise"></a>Pairwise</h3><ul><li>不关心绝对相关性, 关注文档的相对偏好, 转化为一系列<code>二分类问题</code>. 建立 RankNet</li><li>每个文档的相对的目标概率<script type="math/tex; mode=display">y_{i, j}=f(x)=\left\{\begin{array}{lr}1, & \text { if } i排在j前 \\0, & \text { otherwise }\end{array}\right.</script>i 排在 j 前面则为 1, 否则为 0.<br>使用 sigmoid 构造逻辑回归 —-&gt; 使用交叉熵损失函数作为 loss —-&gt; 计算梯度 —-&gt; 在神经网络中 BP<br><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/29/EVvyXZgjtsnaM4k.png" alt="Pairwise"></li><li>排序评估指标:<br>查询 q 的标准折扣累积增益 (NDGC@k)<br><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/29/PpWRifaGJ3koHZ7.png" alt="NDGC"></li><li>缺点: 对于搜索引擎, 排在前列的文档是否排序正确, 更为重要. 各个文档之间相关性排序是否正确的重要性不同. 相同的排序 pair 错误, 即两对排序交叉熵相同, 但对于整个排序列的影响不同. 比如 &lt; 1, 6 &gt; 和 &lt; 3, 4 &gt; 排序错误的影响不同.</li></ul><h3 id="Listwise"><a href="#Listwise" class="headerlink" title="Listwise"></a>Listwise</h3><p>对于一个 list, 即<code>预测排序列表</code>和<code>真实排序列表</code>之间的差异直接简历损失函数</p><ul><li>ListNet<br>对于 n 个文档, 就是一个 n! 分类的问题. 对整个列表计算 softmax, 然后计算交叉熵 loss.<br><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/29/Q9l37MiZzuUxocn.png" alt="ListNet"></li><li>KL 散度<br>计算不同分布之间的 distance</li></ul><h3 id="LambdaRank"><a href="#LambdaRank" class="headerlink" title="LambdaRank"></a>LambdaRank</h3><ul><li>Pairwise 损失了 list-level 列表级评估, Listwise 要求较高的模型复杂度. 于是产生了结合方案: 利用 listwise 的信息, 做 pairwise 的二分类训练, 损失仍然建立在 pair 上, 但是利用 list 的信息, 做一个 weighting 操作</li><li>LambdaRank 优化 NDGC<br>交换 i 和 j 的排序, 看看$\Delta$NDGC 的值, 即变化情况. 将外部排序的信息, 作为 $h\left(\lambda_{i, j}, g_{q}\right)$ 注入.<br><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/29/YpdGTvMh9ilo6gW.png" alt="LambdaRank"></li><li>Pairwise 方法仍然是最受欢迎最广泛应用的学习排序算法<br>排序精度和训练效率的平衡              </li><li>LambdaRank 是一种具有排序列表级信息的 pairwise 方法<br>容易实现,易于改进和调整</li></ul><h2 id="矩阵分解的协同过滤算法"><a href="#矩阵分解的协同过滤算法" class="headerlink" title="矩阵分解的协同过滤算法"></a>矩阵分解的协同过滤算法</h2><p>用户-物品矩阵进行预测, 都可以通过分解为两个低秩矩阵, 进行相乘, 从而可以得知矩阵空白处的预测值. 比如分解电影网站的用户打分矩阵, 和电影得分矩阵, 可以预测用户 p 对于一个新的物品 i 的喜好程度.</p><h3 id="基本模型-Matrix-Factorization"><a href="#基本模型-Matrix-Factorization" class="headerlink" title="基本模型 Matrix Factorization"></a>基本模型 Matrix Factorization</h3><ul><li>双线性模型, 参数与参数相乘, pu 为用户偏好矩阵, qi 为物品评价矩阵, 用户 u 在物品 i 上的预测为<script type="math/tex; mode=display">\hat{r}_{u, i}=p_{u}^{T} q_{i}</script></li><li>均方误差构造损失函数<script type="math/tex; mode=display">\mathcal{L}\left(u, i, r_{u, i}\right)=\frac{1}{2}\left(r_{u, i}-p_{u}^{T} q_{i}\right)^{2}</script></li><li>训练目标: 均方误差+L2范式<script type="math/tex; mode=display">\min _{P, Q} \sum_{r_{u, i} \in D} \frac{1}{2}\left(r_{u, i}-p_{u}^{T} q_{i}\right)^{2}+\frac{\lambda}{2}\left(\left\|p_{u}\right\|^{2}+\left\|q_{i}\right\|^{2}\right)</script></li><li>梯度更新: pu 在 qi 方向上变大/变小, qi 在 pu 的方向上变大/变小<script type="math/tex; mode=display">\begin{array}{l}\frac{\partial \mathcal{L}\left(u, i, r_{u, i}\right)}{\partial p_{u}}=\left(p_{u}^{T} q_{i}-r_{u, i}\right) q_{i}+\lambda p_{u} \\\\\frac{\partial \mathcal{L}\left(u, i, r_{u, i}\right)}{\partial q_{i}}=\left(p_{u}^{T} q_{i}-r_{u, i}\right) p_{u}+\lambda q_{i}\end{array}</script></li></ul><h3 id="带偏差的矩阵分解模型"><a href="#带偏差的矩阵分解模型" class="headerlink" title="带偏差的矩阵分解模型"></a>带偏差的矩阵分解模型</h3><ul><li><p>解决动态变化问题<br>因子模型方便地允许分别处理不同原因, 可以通过大量参数化函数来获得灵活性. 我们观察到的变化: 个人用户的评分等级 $b_u(t)$  个人物品的受欢迎程度 $b_i(t)$  用户偏好 $p_u(t)$  全局bias $\mu$ </p></li><li><p>考虑多种偏差, 用户 u 在物品 i 上的预测为<br><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/29/IovOtfdJp2lPKUR.png" alt=""></p></li><li><p>训练目标</p><script type="math/tex; mode=display">\min _{P, Q} \sum_{r_{u, i} \in D} \frac{1}{2}\left(r_{u, i}-\left(\mu+b_{u}+b_{i}+p_{u}^{\top} q_{i}\right)\right)^{2}+\frac{\lambda}{2}\left(\left\|p_{u}\right\|^{2}+\left\|q_{i}\right\|^{2}+b_{u}^{2}+b_{i}^{2}\right)</script></li><li><p>梯度更新 导数$\times$步长</p><script type="math/tex; mode=display">\begin{aligned}\delta &=r_{u, i}-\left(\mu+b_{u}+b_{i}+p_{u}^{T} q_{i}\right) \\\mu & \leftarrow \mu+\eta \delta \\b_{u} & \leftarrow(1-\eta \lambda) b_{u}+\eta \delta \\b_{i} & \leftarrow(1-\eta \lambda) b_{i}+\eta \delta \\p_{u} & \leftarrow(1-\eta \lambda) p_{u}+\eta \delta q_{i} \\q_{i} & \leftarrow(1-\eta \lambda) q_{i}+\eta \delta p_{u}\end{aligned}</script></li><li><p>发展: 深度学习大行其道</p></li><li>代码实践: <a href="https://www.kaggle.com/chenhaokun/matrix-factorization" target="_blank" rel="noopener">👆矩阵分解</a><br>向量即 embedding<br>embedding_size 每个用户的向量维度<br>user_embeddings 和 item_embeddings 即 p 和 q 的矩阵</li></ul><h3 id="常见的几种协同过滤"><a href="#常见的几种协同过滤" class="headerlink" title="常见的几种协同过滤"></a>常见的几种协同过滤</h3><p><strong>以使用者为基础（User-based）的协同过滤</strong><br>用相似统计的方法得到具有相似爱好或者兴趣的相邻用户，所以称之为以用户为基础（User-based）的协同过滤或基于邻居的协同过滤(Neighbor-based Collaborative Filtering)。 </p><p>方法步骤：</p><ol><li>收集用户信息：收集可以代表用户兴趣的信息。一般的网站系统使用评分的方式或是给予评价，这种方式被称为“主动评分”。另外一种是“被动评分”，是根据用户的行为模式由系统代替用户完成评价，不需要用户直接打分或输入评价数据。电子商务网站在被动评分的数据获取上有其优势，用户购买的商品记录是相当有用的数据。</li><li>最近邻搜索(Nearest neighbor search, NNS)：以用户为基础（User-based）的协同过滤的出发点是与用户兴趣爱好相同的另一组用户，就是计算两个用户的相似度。例如：查找n个和A有相似兴趣用户，把他们对M的评分作为A对M的评分预测。一般会根据数据的不同选择不同的算法，当前较多使用的相似度算法有Pearson Correlation Coefficient、Cosine-based Similarity、Adjusted Cosine Similarity。</li><li>产生推荐结果：有了最近邻**，就可以对目标用户的兴趣进行预测，产生推荐结果。依据推荐目的的不同进行不同形式的推荐，较常见的推荐结果有Top-N 推荐和关系推荐。Top-N 推荐是针对个体用户产生，对每个人产生不一样的结果，例如：透过对A用户的最近邻用户进行统计，选择出现频率高且在A用户的评分项目中不存在的，作为推荐结果。关系推荐是对最近邻用户的记录进行关系规则(association rules)挖掘。</li></ol><p><strong>以项目为基础（Item-based）的协同过滤</strong><br>以用户为基础的协同推荐算法随着用户数量的增多，计算的时间就会变长，所以在2001年Sarwar提出了基于项目的协同过滤推荐算法(Item-based Collaborative Filtering Algorithms)。以项目为基础的协同过滤方法有一个基本的假设“能够引起用户兴趣的项目，必定与其之前评分高的项目相似”，透过计算项目之间的相似性来代替用户之间的相似性。<br>方法步骤：</p><ol><li>收集用户信息：同以用户为基础（User-based）的协同过滤。</li><li>针对项目的最近邻搜索：先计算已评价项目和待预测项目的相似度，并以相似度作为权重，加权各已评价项目的分数，得到待预测项目的预测值。例如：要对项目 A 和项目 B 进行相似性计算，要先找出同时对 A 和 B 打过分的组合，对这些组合进行相似度计算，常用的算法同以用户为基础（User-based）的协同过滤。</li><li>产生推荐结果：以项目为基础的协同过滤不用考虑用户间的差别，所以精度比较差。但是却不需要用户的历史数据，或是进行用户识别。对于项目来讲，它们之间的相似性要稳定很多，因此可以离线完成工作量最大的相似性计算步骤，从而降低了在线计算量，提高推荐效率，尤其是在用户多于项目的情形下尤为显着。</li></ol><p><strong>以模型为基础（Model- based）的协同过滤</strong><br>以用户为基础（User-based）的协同过滤和以项目为基础（Item-based）的协同过滤系统称为以记忆为基础（Memory based）的协同过滤技术，他们共有的缺点是数据稀疏，难以处理大数据量影响即时结果，因此发展出以模型为基础的协同过滤技术。 以模型为基础的协同过滤(Model-based Collaborative Filtering)是先用历史数据得到一个模型，再用此模型进行预测。以模型为基础的协同过滤广泛使用的技术包括Latent Semantic Indexing、Bayesian Networks…等，根据对一个样本的分析得到模型。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 推荐系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ml-script</title>
      <link href="/2020/05/12/ml-script/"/>
      <url>/2020/05/12/ml-script/</url>
      
        <content type="html"><![CDATA[<h1 id="机器学习小抄"><a href="#机器学习小抄" class="headerlink" title="机器学习小抄"></a>机器学习小抄</h1><h2 id="机器学习概观"><a href="#机器学习概观" class="headerlink" title="机器学习概观"></a>机器学习概观</h2><ol><li><p>频率学派：统计学习<br>核心是「数据」，它既从数据中来，利用不同的模型去拟合数据背后的规律；也到数据中去，用拟合出的规律去推断和预测未知的结果. 统计学习中最基础的模型是<code>线性回归</code>，几乎所有其他模型都是从不同角度对线性回归模型做出的扩展与修正</p></li><li><p>贝叶斯学派：符号学习，概率图模型<br>基于关系的图模型更多地代表了「因果推理」的发展方向. 贝叶斯主义也需要计算待学习对象的概率分布，但它利用的不是海量的具体数据，而是变量之间的相关关系、每个变量的先验分布和大量复杂的积分技巧</p></li><li><p>机器学习三要素<br>机器学习的三要素就是：<code>表示，评估和优化</code><br>表示：指的是将样本空间映射到一个合适的特征空间，一般地，我们更青睐于这样的表示是低维度的，是更加稀疏交互的，同时也希望是相互独立的<br>评估：指的是模型在数据上表现的量化形式，我们选取合适的函数来表示什么样子的模型是好的，性能度量就是评估<br>优化：就是对评估函数进行求解，找出最合适的解，来确定最终的模型</p></li></ol><h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2><h3 id="1-特征选择"><a href="#1-特征选择" class="headerlink" title="1. 特征选择"></a>1. 特征选择</h3><h3 id="2-特征表达"><a href="#2-特征表达" class="headerlink" title="2. 特征表达"></a>2. 特征表达</h3><h3 id="3-特征预处理"><a href="#3-特征预处理" class="headerlink" title="3. 特征预处理"></a>3. 特征预处理</h3><p>标准化 归一化 </p><h3 id="4-降维-PCA-LDA-SVD"><a href="#4-降维-PCA-LDA-SVD" class="headerlink" title="4. 降维 PCA LDA SVD"></a>4. 降维 PCA LDA SVD</h3><h2 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h2><h3 id="1-分类"><a href="#1-分类" class="headerlink" title="1. 分类"></a>1. 分类</h3><p>TP FP TN FN ROC AUC</p><h3 id="2-回归"><a href="#2-回归" class="headerlink" title="2.回归"></a>2.回归</h3><p>均方误差，平均误差，R 方</p><h3 id="3-检验"><a href="#3-检验" class="headerlink" title="3. 检验"></a>3. 检验</h3><p>训练集 -（测试集） - 验证集<br>k 折交叉验证，留一法，自助法</p><h3 id="4-超参数调优"><a href="#4-超参数调优" class="headerlink" title="4. 超参数调优"></a>4. 超参数调优</h3><p>网格搜索，随机搜索，贝叶斯优化先验</p><h3 id="5-过拟合与欠拟合"><a href="#5-过拟合与欠拟合" class="headerlink" title="5. 过拟合与欠拟合"></a>5. 过拟合与欠拟合</h3><p>正则化，集成学习，dropout，特征选择</p><h2 id="假设空间"><a href="#假设空间" class="headerlink" title="假设空间"></a>假设空间</h2><h3 id="1-损失函数"><a href="#1-损失函数" class="headerlink" title="1. 损失函数"></a>1. 损失函数</h3><p>0-1 损失，平方损失，log 对数损失/交叉熵损失（逻辑回归），hinge 损失（SVM），指数损失（Adaboost）<br>多分类问题中，上述通过最大似然估计得到的损失函数与通过交叉熵得到的损失函数相同</p><script type="math/tex; mode=display">l_{C E}=-\sum_{i=1}^{C} t_{i} \log \left(y_{i}\right)</script><h3 id="2-目标函数"><a href="#2-目标函数" class="headerlink" title="2. 目标函数"></a>2. 目标函数</h3><p>目标函数即最小化损失函数+正则</p><ul><li>经验风险（训练集上的）</li><li>期望风险（验证集上的）</li><li>结构风险（加入一个正则化）</li><li>正则化项 -&gt; 模型的复杂度</li></ul><h3 id="距离度量"><a href="#距离度量" class="headerlink" title="距离度量"></a>距离度量</h3><p>[详情点击]<a href="https://blog.csdn.net/qq_28168421/article/details/81097655" target="_blank" rel="noopener">https://blog.csdn.net/qq_28168421/article/details/81097655</a><br>欧氏距离 马氏距离 等等</p><h2 id="目标函数的-优化-or-求解"><a href="#目标函数的-优化-or-求解" class="headerlink" title="目标函数的 优化 or 求解"></a>目标函数的 优化 or 求解</h2><ol><li><p>解析法<br>最小二乘法</p></li><li><p>迭代优化：一阶泰勒展开<br>梯度下降、随机梯度下降、批量梯度下降、共轭梯度法</p></li><li><p>迭代优化：二阶泰勒展开<br>牛顿法、拟牛顿法<br>（二阶法的收敛速度一般要远快于一阶法，但是在高维情况下，Hessian 矩阵求逆的计算复杂度很大，而且当目标函数非凸时，二阶法有可能会收到鞍点（Saddle Point)）</p></li><li><p>随机梯度下降<br>动量 Momentum（保持惯性），AdaGrad（环境感知），Adam（保持惯性+环境感知）……</p></li><li><p>限制中优化<br>KKT 条件（最大最小化互换），拉格朗日乘子法（讲约束条件融合到目标函数中）</p></li></ol><h2 id="逻辑回归小记"><a href="#逻辑回归小记" class="headerlink" title="逻辑回归小记"></a>逻辑回归小记</h2><ol><li><p>sigmoid 函数<br>将线性回归转化为逻辑回归，将回归问题转化为二分类问题</p></li><li><p>softmax 是对于 sigmoid 的多分类推广<br>将输出值变换成值为正且和为1的概率分布</p><script type="math/tex; mode=display">\hat{y} 1=\frac{\exp \left(o_{1}\right)}{\sum_{i=1}^{3} \exp \left(o_{i}\right)}</script></li><li><p>均方误差 不如 交叉熵损失函数<br>不会使得偏导接近 0 而难以学习</p></li><li><p>更新参数 𝜃<br>步长 = 梯度 x 学习率</p></li></ol><h2 id="PCA-降维"><a href="#PCA-降维" class="headerlink" title="PCA 降维"></a>PCA 降维</h2><ol><li><p>步骤<br>样本中心化 -&gt; 计算协方差矩阵 -&gt; 对协方差矩阵进行特征值分解 -&gt; 取最大的 m 个特征值对应的特征向量</p></li><li><p>原理<br>利用特征值与特征向量, 找到一组标准正交基 —-&gt; 从而可以将任何向量用标准正交基表示 —-&gt; 从而可以将任何协方差矩阵用 $UWU^T$ 形式表示, U 为正交基矩阵, W 就是本矩阵的特征值.</p></li><li><p>解释<br>PCA 的目的是找到一个方向 $v$, 使得在这个方向上数据 X 的投影的方差最大, 方差越大越能保留尽可能多的有效信息. 由于均值已经为 0, 数据 𝑋 在方向 𝑣 的方差就是 $Xv^2$. 最后推算的结论就是, 需要最大特征值对应的特征向量.<br>最终得到 X 的主成分矩阵为 $T=WX$, 其中 $W$ 为$XTX$ 的特征值矩阵.</p></li></ol><h2 id="SVM-算法小记"><a href="#SVM-算法小记" class="headerlink" title="SVM 算法小记"></a>SVM 算法小记</h2><ol><li><p>超平面，函数间隔，几何间隔<br>w, b 任意缩放，几何间隔不变</p></li><li><p>目标函数<br>找到最小的几何间隔，即为支持向量点，最大化这个支持向量到超平面的几何间隔</p></li><li><p>拉格朗日法：融合约束条件和目标函数<br>先调整 α 最大化目标函数；再调整 w,b 最小化目标函数</p></li><li><p>KKT 条件：满足最大最小化 -&gt; 得到对偶形式<br>可以先调整 w,b 对 w,b 求导，最后只剩 α</p></li><li><p>SMO 算法<br>选择优化两个参数，固定其他 N-2 个参数，简化成为二元函数 -&gt; 再转化为一元函数</p></li><li><p>线性不可分的情况<br>加松弛变量，利用核函数（代替内积，建立一个映射，在低维计算，大大减少计算量）</p></li><li><p>损失函数<br>铰链损失 Hinge Loss, 只关注函数间隔 &lt;=1 的情况, &gt;1 的情况的 loss 忽略为 0</p></li></ol><h2 id="决策树小记"><a href="#决策树小记" class="headerlink" title="决策树小记"></a>决策树小记</h2><ol><li><p>分类<br>ID3， C4.5， CART</p></li><li><p>C4.5 对 ID3 的改进</p></li></ol><ul><li>连续的特征离散化</li><li>引入信息增益比</li><li>针对有缺失值提出策略</li><li>针对过拟合的改进：预剪枝（生成决策树的时候判定）后剪枝（交叉验证判定）</li></ul><ol><li>CART 对 C4.5 的优化</li></ol><ul><li>使用基尼系数，无需对数运算</li><li>使用二叉树</li><li>除了分类，可以进行回归（用叶子的均值来预测结果）</li></ul><h2 id="集成学习（Ensemble）"><a href="#集成学习（Ensemble）" class="headerlink" title="集成学习（Ensemble）"></a>集成学习（Ensemble）</h2><ol><li>总体两类：</li></ol><ul><li>串行生成/序列集成，利用基础学习器之间的依赖关系。通过对之前训练中错误标记的样本赋值较高的权重</li><li>并行生成，利用基础学习器之间的独立性，通过平均可以显著降低错误</li><li>三大方法：Bagging，Boosting，Stacking</li></ul><ol><li>Bagging（bootstrap aggregating）自助采样法</li></ol><ul><li>从原始样本集中抽取训练集，k个训练集共得到k个模型，可以并行生成</li><li>分类问题：k个模型采用投票的方式得到分类结果 -&gt; 决策树演化为「随机森林」</li><li>回归问题：模型的均值作为最后的结果</li></ul><ol><li>Boosting 算法<br>将弱分类器组装成一个强分类器</li></ol><ul><li><p>AdaBoost（Adaptive boosting）算法：每次训练后，对训练失败的训练例赋以较大的<code>权重</code>，从而得到多个预测函数. 通过拟合<code>残差</code>的方式逐步减小残差，将每一步生成的模型<code>叠加</code>，只能顺序生成</p></li><li><p>GBDT（Gradient Boost Decision Tree)，每一次的计算是为了减少上一次的残差，GBDT在残差减少（负梯度）的方向上<code>建立一个新的模型</code></p></li></ul><ol><li><p>Stacking 算法<br>把之前训练的各个模型的「输出」，作为为「输入」来<code>训练</code>一个模型，以得到一个最终的输出. 通常使用logistic回归作为组合策略</p></li><li><p>决策树与这些算法框架进行结合所得到的新的算法：<br>1）Bagging + 决策树 = 随机森林<br>2）AdaBoost + 决策树 = 提升树<br>3）Gradient Boosting + 决策树 = GBDT</p></li></ol><h2 id="机器学习概念细节-（最好做一个速查索引）"><a href="#机器学习概念细节-（最好做一个速查索引）" class="headerlink" title="机器学习概念细节 （最好做一个速查索引）"></a>机器学习概念细节 （最好做一个速查索引）</h2><h3 id="范数-L1-L2"><a href="#范数-L1-L2" class="headerlink" title="范数 L1 L2"></a>范数 L1 L2</h3><ol><li>范数的概念<br>范数是具有“长度”概念的函数. 在向量空间内，为所有的向量的赋予非零的增长度或者大小. 不同的范数，所求的向量的长度或者大小是不同的. </li></ol><p>举个例子，2维空间中，向量(3,4)的长度是5，那么5就是这个向量的一个范数的值，更确切的说，是欧式范数或者L2范数的值. </p><ol><li>各种范数<br>L0范数是指向量中非0的元素的个数. (L0范数很难优化求解)</li></ol><p>L1范数是指向量中各个元素绝对值之和</p><p>L2范数是指向量各元素的平方和然后求平方根 -&gt; 岭回归 Ridge Regression == 权值衰减 weight decay</p><p>L1范数可以进行特征选择，即让特征的系数变为0.</p><p>L2范数可以防止过拟合，提升模型的泛化能力，有助于处理 condition number不好下的矩阵(数据变化很小矩阵求解后结果变化很大)</p><ol><li>各自的区别<br>L1范数和L0范数可以实现稀疏，L1因具有比L0更好的优化求解特性而被广泛应用</li></ol><p>L1 可以实现稀疏性，而 L2 不行. L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0. Lasso在特征选择时候非常有用，而Ridge就只是一种规则化而已</p><ol><li>实现稀疏的好处<br>利于特征选择 + 更好的可解释性</li></ol>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>竞赛-房租预测</title>
      <link href="/2020/02/26/housing-project/"/>
      <url>/2020/02/26/housing-project/</url>
      
        <content type="html"><![CDATA[<h1 id="房租预测"><a href="#房租预测" class="headerlink" title="房租预测"></a>房租预测</h1><h2 id="一-赛题分析"><a href="#一-赛题分析" class="headerlink" title="(一) 赛题分析"></a>(一) 赛题分析</h2><p>比赛要求参赛选手根据给定的数据集，建立模型，预测房屋租金。<br>数据集中的数据类别包括租赁房源、小区、二手房、配套、新房、土地、人口、客户、真实租金等。<br>这是典型的<code>回归预测</code>。 </p><h3 id="EDA"><a href="#EDA" class="headerlink" title="EDA"></a>EDA</h3><p>探索性数据分析, Exploratory Data Analysis，简称EDA</p><p>传统：先假定数据服从某种分布，然后运用这种模型进行预测，以概率论为基础，做各种的参数检验。<br>EDA：“抛开”概率理论，从数据出发，强调数据可视化</p><p>参考资料：<br><a href="https://www.jianshu.com/p/9325c9f88ee6" target="_blank" rel="noopener">一文带你探索性数据分析(EDA)</a></p><h2 id="1-预测指标"><a href="#1-预测指标" class="headerlink" title="1. 预测指标"></a>1. 预测指标</h2><h4 id="回归结果评价标准采用R-Square"><a href="#回归结果评价标准采用R-Square" class="headerlink" title="回归结果评价标准采用R-Square"></a>回归结果评价标准采用R-Square</h4><p><strong>R2（R-Square）的公式为</strong>：<br>残差平方和：(偏差)</p><script type="math/tex; mode=display">SS_{res}=\sum\left(y_{i}-\hat{y}_{i}\right)^{2}</script><p>总平均值: (方差)</p><script type="math/tex; mode=display">SS_{tot}=\sum\left(y_{i}-\overline{y}_{i}\right)^{2}</script><p>其中$\overline{y}$表示$y$的平均值<br>得到$R^2$表达式为：</p><script type="math/tex; mode=display">R^{2}=1-\frac{SS_{res}}{SS_{tot}}=1-\frac{\sum\left(y_{i}-\hat{y}_{i}\right)^{2}}{\sum\left(y_{i}-\overline{y}\right)^{2}}</script><p>$R^2$用于度量<code>因变量的变异中,可由自变量解释部分所占的比例</code>，取值范围是 0~1，$R^2$越接近1,表明回归平方和占总平方和的比例越大,回归线与各观测点越接近，用x的变化来解释y值变化的部分就越多,回归的拟合程度就越好。所以$R^2$也称为<code>拟合优度</code>（Goodness of Fit）的统计量。</p><p>$y_{i}$表示真实值，$\hat{y}_{i}$表示预测值，$\overline{y}_{i}$表示样本均值。得分越高拟合效果越好。</p><h4 id="简要分析"><a href="#简要分析" class="headerlink" title="简要分析"></a>简要分析</h4><p>该份训练集包含 41440行×52列数据<br>目标变量是 真实房租价格- <strong>tradeMoney</strong> </p><p>大多数数据都是int或float型；有部分字段是object型，即文本型中文或英文的，如rentType字段，这些字段在之后需要做处理  </p><h2 id="4-分类特征和连续型特征"><a href="#4-分类特征和连续型特征" class="headerlink" title="4. 分类特征和连续型特征"></a>4. 分类特征和连续型特征</h2><pre><code class="lang-py"># 根据特征含义和特征一览，大致可以判断出数值型和类别型特征如下categorical_feas = [&#39;rentType&#39;, &#39;houseType&#39;, &#39;houseFloor&#39;, &#39;region&#39;, &#39;plate&#39;, &#39;houseToward&#39;, &#39;houseDecoration&#39;,    &#39;communityName&#39;,&#39;city&#39;,&#39;region&#39;,&#39;plate&#39;,&#39;buildYear&#39;]numerical_feas=[&#39;ID&#39;,&#39;area&#39;,&#39;totalFloor&#39;,&#39;saleSecHouseNum&#39;,&#39;subwayStationNum&#39;,    &#39;busStationNum&#39;,&#39;interSchoolNum&#39;,&#39;schoolNum&#39;,&#39;privateSchoolNum&#39;,&#39;hospitalNum&#39;,    &#39;drugStoreNum&#39;,&#39;gymNum&#39;,&#39;bankNum&#39;,&#39;shopNum&#39;,&#39;parkNum&#39;,&#39;mallNum&#39;,&#39;superMarketNum&#39;,    &#39;totalTradeMoney&#39;,&#39;totalTradeArea&#39;,&#39;tradeMeanPrice&#39;,&#39;tradeSecNum&#39;,&#39;totalNewTradeMoney&#39;,    &#39;totalNewTradeArea&#39;,&#39;tradeNewMeanPrice&#39;,&#39;tradeNewNum&#39;,&#39;remainNewNum&#39;,&#39;supplyNewNum&#39;,    &#39;supplyLandNum&#39;,&#39;supplyLandArea&#39;,&#39;tradeLandNum&#39;,&#39;tradeLandArea&#39;,&#39;landTotalPrice&#39;,    &#39;landMeanPrice&#39;,&#39;totalWorkers&#39;,&#39;newWorkers&#39;,&#39;residentPopulation&#39;,&#39;pv&#39;,&#39;uv&#39;,&#39;lookNum&#39;]</code></pre><h2 id="5-缺失值分析"><a href="#5-缺失值分析" class="headerlink" title="5. 缺失值分析"></a>5. 缺失值分析</h2><pre><code class="lang-py"># 缺失值分析def missing_values(df):    alldata_na = pd.DataFrame(df.isnull().sum(), columns={&#39;missingNum&#39;})    alldata_na[&#39;existNum&#39;] = len(df) - alldata_na[&#39;missingNum&#39;]    alldata_na[&#39;sum&#39;] = len(df)    alldata_na[&#39;missingRatio&#39;] = alldata_na[&#39;missingNum&#39;]/len(df)*100    alldata_na[&#39;dtype&#39;] = df.dtypes    #ascending：默认True升序排列；False降序排列    alldata_na = alldata_na[alldata_na[&#39;missingNum&#39;]&gt;0].reset_index().sort_values(by=[&#39;missingNum&#39;,&#39;index&#39;],ascending=[False,True])    alldata_na.set_index(&#39;index&#39;,inplace=True)    return alldata_namissing_values(data_train)</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/26/PtSeMBzNDTI8l3f.png" alt=""></p><h3 id="简要分析-1"><a href="#简要分析-1" class="headerlink" title="简要分析"></a>简要分析</h3><p>这里采用编写函数的方式来直接获取结果（这种方式会在之后反复用到，建议大家尽早养成函数式编写的习惯）；<br>其实在总体情况一览中，info()函数也能看出来。  </p><p>结果是，仅有pv、uv存在缺失值，后面再探究会发现缺失的都是属于同一个plate，可能是官方直接删除了该plate的pv、uv</p><h2 id="6-单调特征列分析"><a href="#6-单调特征列分析" class="headerlink" title="6. 单调特征列分析"></a>6. 单调特征列分析</h2><pre><code class="lang-py">#是否有单调特征列(单调的特征列很大可能是时间)def incresing(vals):    cnt = 0    len_ = len(vals)    for i in range(len_-1):        if vals[i+1] &gt; vals[i]:            cnt += 1    return cntfea_cols = [col for col in data_train.columns]for col in fea_cols:    cnt = incresing(data_train[col].values)    if cnt / data_train.shape[0] &gt;= 0.55:        print(&#39;单调特征：&#39;,col)        print(&#39;单调特征值个数：&#39;, cnt)        print(&#39;单调特征值比例：&#39;, cnt / data_train.shape[0])</code></pre><p>单调特征： tradeTime<br>单调特征值个数： 24085<br>单调特征值比例： 0.5812017374517374</p><h3 id="简要分析-2"><a href="#简要分析-2" class="headerlink" title="简要分析"></a>简要分析</h3><p>先编写判断单调的函数 <em>incresing</em>， 然后再应用到每列上；<br>单调特征是 tradeTime，为时间列。  </p><p>多说句额外的，时间列在特征工程的时候，不同的情况下能有很多的变种形式，比如按年月日分箱，或者按不同的维度在时间上聚合分组，等等  </p><h2 id="7-特征nunique分布"><a href="#7-特征nunique分布" class="headerlink" title="7. 特征nunique分布"></a>7. 特征nunique分布</h2><pre><code class="lang-py"># 特征nunique分布# unique()是以 数组形式（numpy.ndarray）返回列的所有唯一值（特征的所有唯一值）nunique 返回唯一值的个数for feature in categorical_feas:   # 类别型特征    print(feature + &quot;的特征分布如下：&quot;)    print(data_train[feature].value_counts())    if feature != &#39;communityName&#39;: # communityName值太多，暂且不看图表        plt.hist(data_all[feature], bins=3)        plt.show()</code></pre><h3 id="简要分析-3"><a href="#简要分析-3" class="headerlink" title="简要分析"></a>简要分析</h3><p>用自带函数value_counts() 来得到每个分类变量的 种类 分布；<br>并且简单画出柱状图。  </p><p>rentType：4种，且绝大多数是无用的未知方式；<br>houseType：104种，绝大多数在3室及以下；<br>houseFloor：3种，分布较为均匀；<br>region：       15种；<br>plate：        66种；<br>houseToward：    10种；<br>houseDecoration： 4种，一大半是其他；<br>buildYear：     80种；<br>communityName：   4236种，且分布较为稀疏；  </p><p>此步骤是为之后数据处理和特征工程做准备，先理解每个字段的含义以及分布，之后需要根据实际含义对分类变量做不同的处理。</p><h2 id="8-统计特征值频次大于100的特征"><a href="#8-统计特征值频次大于100的特征" class="headerlink" title="8. 统计特征值频次大于100的特征"></a>8. 统计特征值频次大于100的特征</h2><pre><code class="lang-py"># 统计特征值出现频次大于100的特征for feature in categorical_feas:    df_value_counts = pd.DataFrame(data_train[feature].value_counts())    df_value_counts = df_value_counts.reset_index()    df_value_counts.columns = [feature, &#39;counts&#39;] # 列的名字    print(df_value_counts[df_value_counts[&#39;counts&#39;] &gt;= 100])</code></pre><pre><code>  rentType  counts0     未知方式   307591       整租    54722       合租    5204   houseType  counts0     1室1厅1卫    98051     2室1厅1卫    85122     2室2厅1卫    67833     3室1厅1卫    39924     3室2厅2卫    27375     4室1厅1卫    19576     3室2厅1卫    19207     1室0厅1卫    12868     1室2厅1卫     9339     2室2厅2卫     88110    4室2厅2卫     43511    2室0厅1卫     419......</code></pre><h3 id="简要分析-4"><a href="#简要分析-4" class="headerlink" title="简要分析"></a>简要分析</h3><p>此步骤和特征nunique分布结合步骤结合起来看，有一些小于100的是可以直接统一归类为其他的</p><h2 id="9-Label分布"><a href="#9-Label分布" class="headerlink" title="9. Label分布"></a>9. Label分布</h2><p>分段进行 tradeMoney 的绘图 (tradeMoney 就是最后要预测的值)</p><pre><code class="lang-py"># Label 分布fig,axes = plt.subplots(2,3,figsize=(20,5))fig.set_size_inches(20,12)sns.distplot(data_train[&#39;tradeMoney&#39;],ax=axes[0][0])sns.distplot(data_train[(data_train[&#39;tradeMoney&#39;]&lt;=20000)][&#39;tradeMoney&#39;],ax=axes[0][1])sns.distplot(data_train[(data_train[&#39;tradeMoney&#39;]&gt;20000)&amp;(data_train[&#39;tradeMoney&#39;]&lt;=50000)][&#39;tradeMoney&#39;],ax=axes[0][2])sns.distplot(data_train[(data_train[&#39;tradeMoney&#39;]&gt;50000)&amp;(data_train[&#39;tradeMoney&#39;]&lt;=100000)][&#39;tradeMoney&#39;],ax=axes[1][0])sns.distplot(data_train[(data_train[&#39;tradeMoney&#39;]&gt;100000)][&#39;tradeMoney&#39;],ax=axes[1][1])</code></pre><p>money&lt;=10000 38964<br>10000&lt;money&lt;=20000 1985<br>20000&lt;money&lt;=50000 433<br>50000&lt;money&lt;=100000 39<br>100000&lt;money 19</p><h3 id="简要分析-5"><a href="#简要分析-5" class="headerlink" title="简要分析"></a>简要分析</h3><p>将目标变量tradeMoney分组，并查看每组间的分布；<br>可以看出绝大多数都是集中在10000元以内的，并且从图中可以看到该分布是右偏的。  </p><p>这里只是一种实现方式，完全可以将tradeMoney和其他字段一起结合起来查看，比如楼层高低，地区板块。  </p><h2 id="二-数据清洗"><a href="#二-数据清洗" class="headerlink" title="(二) 数据清洗"></a>(二) 数据清洗</h2><h3 id="缺失值分析及处理"><a href="#缺失值分析及处理" class="headerlink" title="缺失值分析及处理"></a>缺失值分析及处理</h3><ul><li>缺失值出现的原因分析</li><li>采取合适的方式对缺失值进行填充</li></ul><h3 id="异常值分析及处理"><a href="#异常值分析及处理" class="headerlink" title="异常值分析及处理"></a>异常值分析及处理</h3><ul><li><p>根据测试集数据的分布处理训练集的数据分布</p></li><li><p>使用合适的方法找出异常值</p></li><li>对异常值进行处理</li></ul><h3 id="深度清洗"><a href="#深度清洗" class="headerlink" title="深度清洗"></a>深度清洗</h3><ul><li>分析每一个communityName、city、region、plate的数据分布并对其进行数据清洗</li></ul><h2 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a>缺失值处理</h2><p>虽然这步骤是缺失值处理，但还会涉及到一些最最基础的数据处理。  </p><ol><li><p><strong>主要思路分析</strong><br>缺失值的处理手段大体可以分为：<code>删除、填充、映射到高维(当做类别处理)</code>.<br>根据任务一，直接找到的缺失值情况是pu和pv；但是，根据特征nunique分布的分析，可以发现rentType存在”—“的情况，这也算是一种缺失值.<br>此外，诸如rentType的”未知方式”；houseToward的”暂无数据”等，本质上也算是一种缺失值，但是对于这些缺失方式，我们可以把它当做是<code>特殊的一类</code>处理，而不需要去主动修改或填充值. </p><p> (1) 将rentType的”—“转换成<code>未知方式</code>类别；<br> (2) pv/pu的缺失值用<code>均值</code>填充；<br> (3) buildYear存在”暂无信息”，将其用<code>众数</code>填充。  </p></li></ol><ol><li><strong>转换object类型数据</strong><br>这里直接采用<code>LabelEncoder</code>的方式编码，详细的编码方式见相关资料.  </li></ol><ol><li><strong>时间字段的处理</strong><br>buildYear由于存在”暂无信息”,所以需要主动将其转换int类型；<br>tradeTime，将其分割成月和日.</li></ol><ol><li><strong>删除无关字段</strong><br>ID是唯一码，建模无用，所以直接删除；<br>city只有一个SH值，也直接删除；<br>tradeTime已经分割成月和日，删除原来字段</li></ol><h2 id="异常值处理"><a href="#异常值处理" class="headerlink" title="异常值处理"></a>异常值处理</h2><p>这里主要针对area和tradeMoney两个维度处理。<br>针对tradeMoney，这里采用的是IsolationForest模型自动处理；<br>针对area和totalFloor是主观+数据可视化的方式得到的结果。</p><p>参考资料：<br><a href="https://zhuanlan.zhihu.com/p/25040651" target="_blank" rel="noopener">iForest （Isolation Forest）孤立森林 异常检测 入门篇</a></p><pre><code class="lang-py"># clean datadef IF_drop(train):    IForest = IsolationForest(contamination=0.01)    IForest.fit(train[&quot;tradeMoney&quot;].values.reshape(-1,1))    y_pred = IForest.predict(train[&quot;tradeMoney&quot;].values.reshape(-1,1))    drop_index = train.loc[y_pred==-1].index    print(drop_index)    train.drop(drop_index,inplace=True)    return traindata_train = IF_drop(data_train)def dropData(train):    # 丢弃部分异常值    train = train[train.area &lt;= 200]    train = train[(train.tradeMoney &lt;=16000) &amp; (train.tradeMoney &gt;=700)]    train.drop(train[(train[&#39;totalFloor&#39;] == 0)].index, inplace=True)    return train  #数据集异常值处理data_train = dropData(data_train)</code></pre><pre><code class="lang-py"># 处理异常值后再次查看面积和租金分布图plt.figure(figsize=(15,5))sns.boxplot(data_train.area)plt.show()plt.figure(figsize=(15,5))sns.boxplot(data_train.tradeMoney),plt.show()</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/26/FSBwQgIRaWX1OLv.png" alt="tradeMoney"></p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/26/l5v69PJUBwndINO.png" alt="area"></p><h2 id="深度清洗-1"><a href="#深度清洗-1" class="headerlink" title="深度清洗"></a>深度清洗</h2><p>针对每一个region的数据，对area和tradeMoney两个维度进行深度清洗, 逐个逐个 drop.<br>采用主观+数据可视化的方式.</p><pre><code class="lang-py">def cleanData(data):    data.drop(data[(data[&#39;region&#39;]==&#39;RG00001&#39;) &amp; (data[&#39;tradeMoney&#39;]&lt;1000)&amp;(data[&#39;area&#39;]&gt;50)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00001&#39;) &amp; (data[&#39;tradeMoney&#39;]&gt;25000)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00001&#39;) &amp; (data[&#39;area&#39;]&gt;250)&amp;(data[&#39;tradeMoney&#39;]&lt;20000)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00001&#39;) &amp; (data[&#39;area&#39;]&gt;400)&amp;(data[&#39;tradeMoney&#39;]&gt;50000)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00001&#39;) &amp; (data[&#39;area&#39;]&gt;100)&amp;(data[&#39;tradeMoney&#39;]&lt;2000)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00002&#39;) &amp; (data[&#39;area&#39;]&lt;100)&amp;(data[&#39;tradeMoney&#39;]&gt;60000)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00003&#39;) &amp; (data[&#39;area&#39;]&lt;300)&amp;(data[&#39;tradeMoney&#39;]&gt;30000)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00003&#39;) &amp; (data[&#39;tradeMoney&#39;]&lt;500)&amp;(data[&#39;area&#39;]&lt;50)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00003&#39;) &amp; (data[&#39;tradeMoney&#39;]&lt;1500)&amp;(data[&#39;area&#39;]&gt;100)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00003&#39;) &amp; (data[&#39;tradeMoney&#39;]&lt;2000)&amp;(data[&#39;area&#39;]&gt;300)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00003&#39;) &amp; (data[&#39;tradeMoney&#39;]&gt;5000)&amp;(data[&#39;area&#39;]&lt;20)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00003&#39;) &amp; (data[&#39;area&#39;]&gt;600)&amp;(data[&#39;tradeMoney&#39;]&gt;40000)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00004&#39;) &amp; (data[&#39;tradeMoney&#39;]&lt;1000)&amp;(data[&#39;area&#39;]&gt;80)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00006&#39;) &amp; (data[&#39;tradeMoney&#39;]&lt;200)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00005&#39;) &amp; (data[&#39;tradeMoney&#39;]&lt;2000)&amp;(data[&#39;area&#39;]&gt;180)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00005&#39;) &amp; (data[&#39;tradeMoney&#39;]&gt;50000)&amp;(data[&#39;area&#39;]&lt;200)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00006&#39;) &amp; (data[&#39;area&#39;]&gt;200)&amp;(data[&#39;tradeMoney&#39;]&lt;2000)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00007&#39;) &amp; (data[&#39;area&#39;]&gt;100)&amp;(data[&#39;tradeMoney&#39;]&lt;2500)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00010&#39;) &amp; (data[&#39;area&#39;]&gt;200)&amp;(data[&#39;tradeMoney&#39;]&gt;25000)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00010&#39;) &amp; (data[&#39;area&#39;]&gt;400)&amp;(data[&#39;tradeMoney&#39;]&lt;15000)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00010&#39;) &amp; (data[&#39;tradeMoney&#39;]&lt;3000)&amp;(data[&#39;area&#39;]&gt;200)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00010&#39;) &amp; (data[&#39;tradeMoney&#39;]&gt;7000)&amp;(data[&#39;area&#39;]&lt;75)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00010&#39;) &amp; (data[&#39;tradeMoney&#39;]&gt;12500)&amp;(data[&#39;area&#39;]&lt;100)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00004&#39;) &amp; (data[&#39;area&#39;]&gt;400)&amp;(data[&#39;tradeMoney&#39;]&gt;20000)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00008&#39;) &amp; (data[&#39;tradeMoney&#39;]&lt;2000)&amp;(data[&#39;area&#39;]&gt;80)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00009&#39;) &amp; (data[&#39;tradeMoney&#39;]&gt;40000)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00009&#39;) &amp; (data[&#39;area&#39;]&gt;300)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00009&#39;) &amp; (data[&#39;area&#39;]&gt;100)&amp;(data[&#39;tradeMoney&#39;]&lt;2000)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00011&#39;) &amp; (data[&#39;tradeMoney&#39;]&lt;10000)&amp;(data[&#39;area&#39;]&gt;390)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00012&#39;) &amp; (data[&#39;area&#39;]&gt;120)&amp;(data[&#39;tradeMoney&#39;]&lt;5000)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00013&#39;) &amp; (data[&#39;area&#39;]&lt;100)&amp;(data[&#39;tradeMoney&#39;]&gt;40000)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00013&#39;) &amp; (data[&#39;area&#39;]&gt;400)&amp;(data[&#39;tradeMoney&#39;]&gt;50000)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00013&#39;) &amp; (data[&#39;area&#39;]&gt;80)&amp;(data[&#39;tradeMoney&#39;]&lt;2000)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00014&#39;) &amp; (data[&#39;area&#39;]&gt;300)&amp;(data[&#39;tradeMoney&#39;]&gt;40000)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00014&#39;) &amp; (data[&#39;tradeMoney&#39;]&lt;1300)&amp;(data[&#39;area&#39;]&gt;80)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00014&#39;) &amp; (data[&#39;tradeMoney&#39;]&lt;8000)&amp;(data[&#39;area&#39;]&gt;200)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00014&#39;) &amp; (data[&#39;tradeMoney&#39;]&lt;1000)&amp;(data[&#39;area&#39;]&gt;20)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00014&#39;) &amp; (data[&#39;tradeMoney&#39;]&gt;25000)&amp;(data[&#39;area&#39;]&gt;200)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00014&#39;) &amp; (data[&#39;tradeMoney&#39;]&lt;20000)&amp;(data[&#39;area&#39;]&gt;250)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00005&#39;) &amp; (data[&#39;tradeMoney&#39;]&gt;30000)&amp;(data[&#39;area&#39;]&lt;100)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00005&#39;) &amp; (data[&#39;tradeMoney&#39;]&lt;50000)&amp;(data[&#39;area&#39;]&gt;600)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00005&#39;) &amp; (data[&#39;tradeMoney&#39;]&gt;50000)&amp;(data[&#39;area&#39;]&gt;350)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00006&#39;) &amp; (data[&#39;tradeMoney&#39;]&gt;4000)&amp;(data[&#39;area&#39;]&lt;100)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00006&#39;) &amp; (data[&#39;tradeMoney&#39;]&lt;600)&amp;(data[&#39;area&#39;]&gt;100)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00006&#39;) &amp; (data[&#39;area&#39;]&gt;165)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00012&#39;) &amp; (data[&#39;tradeMoney&#39;]&lt;800)&amp;(data[&#39;area&#39;]&lt;30)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00007&#39;) &amp; (data[&#39;tradeMoney&#39;]&lt;1100)&amp;(data[&#39;area&#39;]&gt;50)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00004&#39;) &amp; (data[&#39;tradeMoney&#39;]&gt;8000)&amp;(data[&#39;area&#39;]&lt;80)].index,inplace=True)    data.loc[(data[&#39;region&#39;]==&#39;RG00002&#39;)&amp;(data[&#39;area&#39;]&gt;50)&amp;(data[&#39;rentType&#39;]==&#39;合租&#39;),&#39;rentType&#39;]=&#39;整租&#39;    data.loc[(data[&#39;region&#39;]==&#39;RG00014&#39;)&amp;(data[&#39;rentType&#39;]==&#39;合租&#39;)&amp;(data[&#39;area&#39;]&gt;60),&#39;rentType&#39;]=&#39;整租&#39;    data.drop(data[(data[&#39;region&#39;]==&#39;RG00008&#39;)&amp;(data[&#39;tradeMoney&#39;]&gt;15000)&amp;(data[&#39;area&#39;]&lt;110)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00008&#39;)&amp;(data[&#39;tradeMoney&#39;]&gt;20000)&amp;(data[&#39;area&#39;]&gt;110)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00008&#39;)&amp;(data[&#39;tradeMoney&#39;]&lt;1500)&amp;(data[&#39;area&#39;]&lt;50)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00008&#39;)&amp;(data[&#39;rentType&#39;]==&#39;合租&#39;)&amp;(data[&#39;area&#39;]&gt;50)].index,inplace=True)    data.drop(data[(data[&#39;region&#39;]==&#39;RG00015&#39;) ].index,inplace=True)    data.reset_index(drop=True, inplace=True)    return datadata_train = cleanData(data_train)</code></pre><h2 id="三-特征工程"><a href="#三-特征工程" class="headerlink" title="(三) 特征工程"></a>(三) 特征工程</h2><pre><code class="lang-python">#核心代码举例# 统计特征    #计算均值    gp = train.groupby(by)[fea].mean()    #计算中位数    gp = train.groupby(by)[fea].median()    #计算方差    gp = train.groupby(by)[fea].std()    #计算最大值    gp = train.groupby(by)[fea].max()    #计算最小值    gp = train.groupby(by)[fea].min()    #计算出现次数    gp = train.groupby(by)[fea].size()# groupby生成统计特征：mean,std    # 按照communityName分组计算面积的均值和方差    temp = data.groupby(&#39;communityName&#39;)[&#39;area&#39;].agg({&#39;com_area_mean&#39;: &#39;mean&#39;, &#39;com_area_std&#39;: &#39;std&#39;})# 特征拆分    # 将houseType转为&#39;Room&#39;，&#39;Hall&#39;，&#39;Bath&#39;    def Room(x):        Room = int(x.split(&#39;室&#39;)[0])        return Room    def Hall(x):        Hall = int(x.split(&quot;室&quot;)[1].split(&quot;厅&quot;)[0])        return Hall    def Bath(x):        Bath = int(x.split(&quot;室&quot;)[1].split(&quot;厅&quot;)[1].split(&quot;卫&quot;)[0])        return Bath    data[&#39;Room&#39;] = data[&#39;houseType&#39;].apply(lambda x: Room(x))    data[&#39;Hall&#39;] = data[&#39;houseType&#39;].apply(lambda x: Hall(x))    data[&#39;Bath&#39;] = data[&#39;houseType&#39;].apply(lambda x: Bath(x))#特征合并    # 合并部分配套设施特征    data[&#39;trainsportNum&#39;] = 5 * data[&#39;subwayStationNum&#39;] / data[&#39;subwayStationNum&#39;].mean() + data[&#39;busStationNum&#39;] / \                                                                                             data[                                                                                                 &#39;busStationNum&#39;].mean()# 交叉生成特征:特征之间交叉+ - * / data[&#39;Room_Bath&#39;] = (data[&#39;Bath&#39;]+1) / (data[&#39;Room&#39;]+1)# 聚类特征from sklearn.mixture import GaussianMixture  使用GaussianMixture做聚类特征gmm = GaussianMixture(n_components=4, covariance_type=&#39;full&#39;, random_state=0)gmm.fit_predict(data)# 特征编码from sklearn.preprocessing import LabelEncoderdata[&#39;communityName&#39;] = LabelEncoder().fit_transform(data[&#39;communityName&#39;])from sklearn import preprocessing.OneHotEncoderdata[&#39;communityName&#39;] = OneHotEncoder().fit_transform(data[&#39;communityName&#39;])# 过大量级值取log平滑（针对线性模型有效）data[feature]=np.log1p(data[feature])</code></pre><pre><code class="lang-python">import pandas as pdimport warningswarnings.filterwarnings(&#39;ignore&#39;)from sklearn.preprocessing import LabelEncodertrain = pd.read_csv(&#39;./train_data.csv&#39;)test = pd.read_csv(&#39;./test_a.csv&#39;)target_train = train.pop(&#39;tradeMoney&#39;)target_test = test.pop(&#39;tradeMoney&#39;)</code></pre><h3 id="特征合并"><a href="#特征合并" class="headerlink" title="特征合并"></a>特征合并</h3><pre><code class="lang-python">def newfeature(data):    # 将houseType转为&#39;Room&#39;，&#39;Hall&#39;，&#39;Bath&#39;    def Room(x):        Room = int(x.split(&#39;室&#39;)[0])        return Room    def Hall(x):        Hall = int(x.split(&quot;室&quot;)[1].split(&quot;厅&quot;)[0])        return Hall    def Bath(x):        Bath = int(x.split(&quot;室&quot;)[1].split(&quot;厅&quot;)[1].split(&quot;卫&quot;)[0])        return Bath    data[&#39;Room&#39;] = data[&#39;houseType&#39;].apply(lambda x: Room(x))    data[&#39;Hall&#39;] = data[&#39;houseType&#39;].apply(lambda x: Hall(x))    data[&#39;Bath&#39;] = data[&#39;houseType&#39;].apply(lambda x: Bath(x))    data[&#39;Room_Bath&#39;] = (data[&#39;Bath&#39;]+1) / (data[&#39;Room&#39;]+1)    # 填充租房类型  loc 取行数据    data.loc[(data[&#39;rentType&#39;] == &#39;未知方式&#39;) &amp; (data[&#39;Room&#39;] &lt;= 1), &#39;rentType&#39;] = &#39;整租&#39;    # print(data.loc[(data[&#39;rentType&#39;]==&#39;未知方式&#39;)&amp;(data[&#39;Room_Bath&#39;]&gt;1),&#39;rentType&#39;])    data.loc[(data[&#39;rentType&#39;] == &#39;未知方式&#39;) &amp; (data[&#39;Room_Bath&#39;] &gt; 1), &#39;rentType&#39;] = &#39;合租&#39;    data.loc[(data[&#39;rentType&#39;] == &#39;未知方式&#39;) &amp; (data[&#39;Room&#39;] &gt; 1) &amp; (data[&#39;area&#39;] &lt; 50), &#39;rentType&#39;] = &#39;合租&#39;    data.loc[(data[&#39;rentType&#39;] == &#39;未知方式&#39;) &amp; (data[&#39;area&#39;] / data[&#39;Room&#39;] &lt; 20), &#39;rentType&#39;] = &#39;合租&#39;    # data.loc[(data[&#39;rentType&#39;]==&#39;未知方式&#39;)&amp;(data[&#39;area&#39;]&gt;60),&#39;rentType&#39;]=&#39;合租&#39;    data.loc[(data[&#39;rentType&#39;] == &#39;未知方式&#39;) &amp; (data[&#39;area&#39;] &lt;= 50) &amp; (data[&#39;Room&#39;] == 2), &#39;rentType&#39;] = &#39;合租&#39;    data.loc[(data[&#39;rentType&#39;] == &#39;未知方式&#39;) &amp; (data[&#39;area&#39;] &gt; 60) &amp; (data[&#39;Room&#39;] == 2), &#39;rentType&#39;] = &#39;整租&#39;    data.loc[(data[&#39;rentType&#39;] == &#39;未知方式&#39;) &amp; (data[&#39;area&#39;] &lt;= 60) &amp; (data[&#39;Room&#39;] == 3), &#39;rentType&#39;] = &#39;合租&#39;    data.loc[(data[&#39;rentType&#39;] == &#39;未知方式&#39;) &amp; (data[&#39;area&#39;] &gt; 60) &amp; (data[&#39;Room&#39;] == 3), &#39;rentType&#39;] = &#39;整租&#39;    data.loc[(data[&#39;rentType&#39;] == &#39;未知方式&#39;) &amp; (data[&#39;area&#39;] &gt;= 100) &amp; (data[&#39;Room&#39;] &gt; 3), &#39;rentType&#39;] = &#39;整租&#39;    # data.drop(&#39;Room_Bath&#39;, axis=1, inplace=True)    # 提升0.0001    def month(x):        month = int(x.split(&#39;/&#39;)[1])        return month    # def day(x):    #     day = int(x.split(&#39;/&#39;)[2])    #     return day    # 结果变差    # 分割交易时间    # data[&#39;year&#39;]=data[&#39;tradeTime&#39;].apply(lambda x:year(x))    data[&#39;month&#39;] = data[&#39;tradeTime&#39;].apply(lambda x: month(x))    # data[&#39;day&#39;] = data[&#39;tradeTime&#39;].apply(lambda x: day(x))# 结果变差    #     data[&#39;pv/uv&#39;] = data[&#39;pv&#39;] / data[&#39;uv&#39;]    #     data[&#39;房间总数&#39;] = data[&#39;室&#39;] + data[&#39;厅&#39;] + data[&#39;卫&#39;]    # 合并部分配套设施特征    data[&#39;trainsportNum&#39;] = 5 * data[&#39;subwayStationNum&#39;] / data[&#39;subwayStationNum&#39;].mean() + data[&#39;busStationNum&#39;] / \                                                                                             data[                                                                                                 &#39;busStationNum&#39;].mean()    data[&#39;all_SchoolNum&#39;] = 2 * data[&#39;interSchoolNum&#39;] / data[&#39;interSchoolNum&#39;].mean() + data[&#39;schoolNum&#39;] / data[        &#39;schoolNum&#39;].mean() \                            + data[&#39;privateSchoolNum&#39;] / data[&#39;privateSchoolNum&#39;].mean()    data[&#39;all_hospitalNum&#39;] = 2 * data[&#39;hospitalNum&#39;] / data[&#39;hospitalNum&#39;].mean() + \                              data[&#39;drugStoreNum&#39;] / data[&#39;drugStoreNum&#39;].mean()    data[&#39;all_mall&#39;] = data[&#39;mallNum&#39;] / data[&#39;mallNum&#39;].mean() + \                       data[&#39;superMarketNum&#39;] / data[&#39;superMarketNum&#39;].mean()    data[&#39;otherNum&#39;] = data[&#39;gymNum&#39;] / data[&#39;gymNum&#39;].mean() + data[&#39;bankNum&#39;] / data[&#39;bankNum&#39;].mean() + \                       data[&#39;shopNum&#39;] / data[&#39;shopNum&#39;].mean() + 2 * data[&#39;parkNum&#39;] / data[&#39;parkNum&#39;].mean()    data.drop([&#39;subwayStationNum&#39;, &#39;busStationNum&#39;,               &#39;interSchoolNum&#39;, &#39;schoolNum&#39;, &#39;privateSchoolNum&#39;,               &#39;hospitalNum&#39;, &#39;drugStoreNum&#39;, &#39;mallNum&#39;, &#39;superMarketNum&#39;, &#39;gymNum&#39;, &#39;bankNum&#39;, &#39;shopNum&#39;, &#39;parkNum&#39;],              axis=1, inplace=True)    # 提升0.0005#     data[&#39;houseType_1sumcsu&#39;]=data[&#39;Bath&#39;].map(lambda x:str(x))+data[&#39;month&#39;].map(lambda x:str(x))#     data[&#39;houseType_2sumcsu&#39;]=data[&#39;Bath&#39;].map(lambda x:str(x))+data[&#39;communityName&#39;]#     data[&#39;houseType_3sumcsu&#39;]=data[&#39;Bath&#39;].map(lambda x:str(x))+data[&#39;plate&#39;]    data.drop(&#39;houseType&#39;, axis=1, inplace=True)    data.drop(&#39;tradeTime&#39;, axis=1, inplace=True)    data[&quot;area&quot;] = data[&quot;area&quot;].astype(int)    # categorical_feats = [&#39;rentType&#39;, &#39;houseFloor&#39;, &#39;houseToward&#39;, &#39;houseDecoration&#39;, &#39;communityName&#39;,&#39;region&#39;, &#39;plate&#39;]    categorical_feats = [&#39;rentType&#39;, &#39;houseFloor&#39;, &#39;houseToward&#39;, &#39;houseDecoration&#39;,  &#39;region&#39;, &#39;plate&#39;,&#39;cluster&#39;]    return data, categorical_feats</code></pre><h3 id="计算统计特征"><a href="#计算统计特征" class="headerlink" title="计算统计特征"></a>计算统计特征</h3><pre><code class="lang-python">#计算统计特征def featureCount(train,test):    train[&#39;data_type&#39;] = 0    test[&#39;data_type&#39;] = 1    data = pd.concat([train, test], axis=0, join=&#39;outer&#39;)    def feature_count(data, features=[]):        new_feature = &#39;count&#39;        for i in features:            new_feature += &#39;_&#39; + i        temp = data.groupby(features).size().reset_index().rename(columns={0: new_feature})        data = data.merge(temp, &#39;left&#39;, on=features)        return data    data = feature_count(data, [&#39;communityName&#39;])    data = feature_count(data, [&#39;buildYear&#39;])    data = feature_count(data, [&#39;totalFloor&#39;])    data = feature_count(data, [&#39;communityName&#39;, &#39;totalFloor&#39;])    data = feature_count(data, [&#39;communityName&#39;, &#39;newWorkers&#39;])    data = feature_count(data, [&#39;communityName&#39;, &#39;totalTradeMoney&#39;])    new_train = data[data[&#39;data_type&#39;] == 0]    new_test = data[data[&#39;data_type&#39;] == 1]    new_train.drop(&#39;data_type&#39;, axis=1, inplace=True)    new_test.drop([&#39;data_type&#39;], axis=1, inplace=True)    return new_train, new_testtrain, test = featureCount(train, test)</code></pre><h3 id="groupby方法生成统计特征"><a href="#groupby方法生成统计特征" class="headerlink" title="groupby方法生成统计特征"></a>groupby方法生成统计特征</h3><pre><code class="lang-python">#groupby生成统计特征：mean,std等def gourpby(train,test):    train[&#39;data_type&#39;] = 0    test[&#39;data_type&#39;] = 1    data = pd.concat([train, test], axis=0, join=&#39;outer&#39;)    columns = [&#39;rentType&#39;, &#39;houseFloor&#39;, &#39;houseToward&#39;, &#39;houseDecoration&#39;, &#39;communityName&#39;, &#39;region&#39;, &#39;plate&#39;]    for feature in columns:        data[feature] = LabelEncoder().fit_transform(data[feature])    temp = data.groupby(&#39;communityName&#39;)[&#39;area&#39;].agg({&#39;com_area_mean&#39;: &#39;mean&#39;, &#39;com_area_std&#39;: &#39;std&#39;})    temp.fillna(0, inplace=True)    data = data.merge(temp, on=&#39;communityName&#39;, how=&#39;left&#39;)    data[&#39;price_per_area&#39;] = data.tradeMeanPrice / data.area * 100    temp = data.groupby(&#39;communityName&#39;)[&#39;price_per_area&#39;].agg(        {&#39;comm_price_mean&#39;: &#39;mean&#39;, &#39;comm_price_std&#39;: &#39;std&#39;})    temp.fillna(0, inplace=True)    data = data.merge(temp, on=&#39;communityName&#39;, how=&#39;left&#39;)    temp = data.groupby(&#39;plate&#39;)[&#39;price_per_area&#39;].agg(        {&#39;plate_price_mean&#39;: &#39;mean&#39;, &#39;plate_price_std&#39;: &#39;std&#39;})    temp.fillna(0, inplace=True)    data = data.merge(temp, on=&#39;plate&#39;, how=&#39;left&#39;)    data.drop(&#39;price_per_area&#39;, axis=1, inplace=True)    temp = data.groupby(&#39;plate&#39;)[&#39;area&#39;].agg({&#39;plate_area_mean&#39;: &#39;mean&#39;, &#39;plate_area_std&#39;: &#39;std&#39;})    temp.fillna(0, inplace=True)    data = data.merge(temp, on=&#39;plate&#39;, how=&#39;left&#39;)    temp = data.groupby([&#39;plate&#39;])[&#39;buildYear&#39;].agg({&#39;plate_year_mean&#39;: &#39;mean&#39;, &#39;plate_year_std&#39;: &#39;std&#39;})    data = data.merge(temp, on=&#39;plate&#39;, how=&#39;left&#39;)    data.plate_year_mean = data.plate_year_mean.astype(&#39;int&#39;)    data[&#39;comm_plate_year_diff&#39;] = data.buildYear - data.plate_year_mean    data.drop(&#39;plate_year_mean&#39;, axis=1, inplace=True)    temp = data.groupby(&#39;plate&#39;)[&#39;trainsportNum&#39;].agg(&#39;sum&#39;).reset_index(name=&#39;plate_trainsportNum&#39;)    data = data.merge(temp, on=&#39;plate&#39;, how=&#39;left&#39;)    temp = data.groupby([&#39;communityName&#39;, &#39;plate&#39;])[&#39;trainsportNum&#39;].agg(&#39;sum&#39;).reset_index(name=&#39;com_trainsportNum&#39;)    data = data.merge(temp, on=[&#39;communityName&#39;, &#39;plate&#39;], how=&#39;left&#39;)    data[&#39;trainsportNum_ratio&#39;] = list(map(lambda x, y: round(x / y, 3) if y != 0 else -1,                                           data[&#39;com_trainsportNum&#39;], data[&#39;plate_trainsportNum&#39;]))    data = data.drop([&#39;com_trainsportNum&#39;, &#39;plate_trainsportNum&#39;], axis=1)    temp = data.groupby(&#39;plate&#39;)[&#39;all_SchoolNum&#39;].agg(&#39;sum&#39;).reset_index(name=&#39;plate_all_SchoolNum&#39;)    data = data.merge(temp, on=&#39;plate&#39;, how=&#39;left&#39;)    temp = data.groupby([&#39;communityName&#39;, &#39;plate&#39;])[&#39;all_SchoolNum&#39;].agg(&#39;sum&#39;).reset_index(name=&#39;com_all_SchoolNum&#39;)    data = data.merge(temp, on=[&#39;communityName&#39;, &#39;plate&#39;], how=&#39;left&#39;)    data = data.drop([&#39;com_all_SchoolNum&#39;, &#39;plate_all_SchoolNum&#39;], axis=1)    temp = data.groupby([&#39;communityName&#39;, &#39;plate&#39;])[&#39;all_mall&#39;].agg(&#39;sum&#39;).reset_index(name=&#39;com_all_mall&#39;)    data = data.merge(temp, on=[&#39;communityName&#39;, &#39;plate&#39;], how=&#39;left&#39;)    temp = data.groupby(&#39;plate&#39;)[&#39;otherNum&#39;].agg(&#39;sum&#39;).reset_index(name=&#39;plate_otherNum&#39;)    data = data.merge(temp, on=&#39;plate&#39;, how=&#39;left&#39;)    temp = data.groupby([&#39;communityName&#39;, &#39;plate&#39;])[&#39;otherNum&#39;].agg(&#39;sum&#39;).reset_index(name=&#39;com_otherNum&#39;)    data = data.merge(temp, on=[&#39;communityName&#39;, &#39;plate&#39;], how=&#39;left&#39;)    data[&#39;other_ratio&#39;] = list(map(lambda x, y: round(x / y, 3) if y != 0 else -1,                                   data[&#39;com_otherNum&#39;], data[&#39;plate_otherNum&#39;]))    data = data.drop([&#39;com_otherNum&#39;, &#39;plate_otherNum&#39;], axis=1)    temp = data.groupby([&#39;month&#39;, &#39;communityName&#39;]).size().reset_index(name=&#39;communityName_saleNum&#39;)    data = data.merge(temp, on=[&#39;month&#39;, &#39;communityName&#39;], how=&#39;left&#39;)    temp = data.groupby([&#39;month&#39;, &#39;plate&#39;]).size().reset_index(name=&#39;plate_saleNum&#39;)    data = data.merge(temp, on=[&#39;month&#39;, &#39;plate&#39;], how=&#39;left&#39;)    data[&#39;sale_ratio&#39;] = round((data.communityName_saleNum + 1) / (data.plate_saleNum + 1), 3)    data[&#39;sale_newworker_differ&#39;] = 3 * data.plate_saleNum - data.newWorkers    data.drop([&#39;communityName_saleNum&#39;, &#39;plate_saleNum&#39;], axis=1, inplace=True)    new_train = data[data[&#39;data_type&#39;] == 0]    new_test = data[data[&#39;data_type&#39;] == 1]    new_train.drop(&#39;data_type&#39;, axis=1, inplace=True)    new_test.drop([&#39;data_type&#39;], axis=1, inplace=True)    return new_train, new_testtrain, test = gourpby(train, test)</code></pre><h3 id="聚类方法"><a href="#聚类方法" class="headerlink" title="聚类方法"></a>聚类方法</h3><pre><code class="lang-python">#聚类def cluster(train,test):    from sklearn.mixture import GaussianMixture    train[&#39;data_type&#39;] = 0    test[&#39;data_type&#39;] = 1    data = pd.concat([train, test], axis=0, join=&#39;outer&#39;)    col = [&#39;totalFloor&#39;,           &#39;houseDecoration&#39;, &#39;communityName&#39;, &#39;region&#39;, &#39;plate&#39;, &#39;buildYear&#39;,           &#39;tradeMeanPrice&#39;, &#39;tradeSecNum&#39;, &#39;totalNewTradeMoney&#39;,           &#39;totalNewTradeArea&#39;, &#39;tradeNewMeanPrice&#39;, &#39;tradeNewNum&#39;, &#39;remainNewNum&#39;,           &#39;landTotalPrice&#39;, &#39;landMeanPrice&#39;, &#39;totalWorkers&#39;,           &#39;newWorkers&#39;, &#39;residentPopulation&#39;, &#39;lookNum&#39;,           &#39;trainsportNum&#39;,           &#39;all_SchoolNum&#39;, &#39;all_hospitalNum&#39;, &#39;all_mall&#39;, &#39;otherNum&#39;]    # EM    gmm = GaussianMixture(n_components=3, covariance_type=&#39;full&#39;, random_state=0)    data[&#39;cluster&#39;]= pd.DataFrame(gmm.fit_predict(data[col]))    col1 = [&#39;totalFloor&#39;,&#39;houseDecoration&#39;, &#39;communityName&#39;, &#39;region&#39;, &#39;plate&#39;, &#39;buildYear&#39;]    col2 = [&#39;tradeMeanPrice&#39;, &#39;tradeSecNum&#39;, &#39;totalNewTradeMoney&#39;,            &#39;totalNewTradeArea&#39;, &#39;tradeNewMeanPrice&#39;, &#39;tradeNewNum&#39;, &#39;remainNewNum&#39;,            &#39;landTotalPrice&#39;, &#39;landMeanPrice&#39;, &#39;totalWorkers&#39;,            &#39;newWorkers&#39;, &#39;residentPopulation&#39;, &#39;lookNum&#39;,            &#39;trainsportNum&#39;,            &#39;all_SchoolNum&#39;, &#39;all_hospitalNum&#39;, &#39;all_mall&#39;, &#39;otherNum&#39;]    for feature1 in col1:        for feature2 in col2:            temp = data.groupby([&#39;cluster&#39;,feature1])[feature2].agg(&#39;mean&#39;).reset_index(name=feature2+&#39;_&#39;+feature1+&#39;_cluster_mean&#39;)            temp.fillna(0, inplace=True)            data = data.merge(temp, on=[&#39;cluster&#39;, feature1], how=&#39;left&#39;)    new_train = data[data[&#39;data_type&#39;] == 0]    new_test = data[data[&#39;data_type&#39;] == 1]    new_train.drop(&#39;data_type&#39;, axis=1, inplace=True)    new_test.drop([&#39;data_type&#39;], axis=1, inplace=True)    return new_train, new_testtrain, test = cluster(train, test)</code></pre><h3 id="log平滑"><a href="#log平滑" class="headerlink" title="log平滑"></a>log平滑</h3><pre><code class="lang-python"># 过大量级值取log平滑（针对线性模型有效）big_num_cols = [&#39;totalTradeMoney&#39;,&#39;totalTradeArea&#39;,&#39;tradeMeanPrice&#39;,&#39;totalNewTradeMoney&#39;, &#39;totalNewTradeArea&#39;,                &#39;tradeNewMeanPrice&#39;,&#39;remainNewNum&#39;, &#39;supplyNewNum&#39;, &#39;supplyLandArea&#39;,                &#39;tradeLandArea&#39;,&#39;landTotalPrice&#39;,&#39;landMeanPrice&#39;,&#39;totalWorkers&#39;,&#39;newWorkers&#39;,                &#39;residentPopulation&#39;,&#39;pv&#39;,&#39;uv&#39;]for col in big_num_cols:        train[col] = train[col].map(lambda x: np.log1p(x))        test[col] = test[col].map(lambda x: np.log1p(x))</code></pre><pre><code class="lang-python">#对比特征工程前后线性模型结果情况test=test.fillna(0)# Lasso回归from sklearn.linear_model import Lassolasso=Lasso(alpha=0.1)lasso.fit(train,target_train)#预测测试集和训练集结果y_pred_train=lasso.predict(train)y_pred_test=lasso.predict(test)#对比结果from sklearn.metrics import r2_scorescore_train=r2_score(y_pred_train,target_train)print(&quot;训练集结果：&quot;,score_train)score_test=r2_score(y_pred_test, target_test)print(&quot;测试集结果：&quot;,score_test)</code></pre><h2 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h2><pre><code class="lang-python">import pandas as pdimport warningswarnings.filterwarnings(&#39;ignore&#39;)from sklearn.preprocessing import LabelEncoder#读取数据train = pd.read_csv(&#39;&#39;)test = pd.read_csv(&#39;&#39;)target_train = train.pop(&#39;tradeMoney&#39;)target_test = test.pop(&#39;tradeMoney&#39;)</code></pre><h3 id="相关系数法"><a href="#相关系数法" class="headerlink" title="相关系数法"></a>相关系数法</h3><pre><code class="lang-python">#相关系数法特征选择from sklearn.feature_selection import SelectKBestprint(train.shape)sk=SelectKBest(k=150)new_train=sk.fit_transform(train,target_train)print(new_train.shape)# 获取对应列索引select_columns=sk.get_support(indices = True)# print(select_columns)# 获取对应列名# print(test.columns[select_columns])select_columns_name=test.columns[select_columns]new_test=test[select_columns_name]print(new_test.shape)# Lasso回归from sklearn.linear_model import Lassolasso=Lasso(alpha=0.1)lasso.fit(new_train,target_train)#预测测试集和训练集结果y_pred_train=lasso.predict(new_train)y_pred_test=lasso.predict(new_test)#对比结果from sklearn.metrics import r2_scorescore_train=r2_score(y_pred_train,target_train)print(&quot;训练集结果：&quot;,score_train)score_test=r2_score(y_pred_test, target_test)print(&quot;测试集结果：&quot;,score_test)</code></pre><h3 id="Wrapper"><a href="#Wrapper" class="headerlink" title="Wrapper"></a>Wrapper</h3><pre><code class="lang-python"># Wrapperfrom sklearn.feature_selection import RFEfrom sklearn.linear_model import LinearRegressionlr = LinearRegression()rfe = RFE(lr, n_features_to_select=160)rfe.fit(train,target_train)RFE(estimator=LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,                               normalize=False),    n_features_to_select=40, step=1, verbose=0)select_columns = [f for f, s in zip(train.columns, rfe.support_) if s]print(select_columns)new_train = train[select_columns]new_test = test[select_columns]# Lasso回归from sklearn.linear_model import Lassolasso=Lasso(alpha=0.1)lasso.fit(new_train,target_train)#预测测试集和训练集结果y_pred_train=lasso.predict(new_train)y_pred_test=lasso.predict(new_test)#对比结果from sklearn.metrics import r2_scorescore_train=r2_score(y_pred_train,target_train)print(&quot;训练集结果：&quot;,score_train)score_test=r2_score(y_pred_test, target_test)print(&quot;测试集结果：&quot;,score_test)</code></pre><h3 id="Embedded"><a href="#Embedded" class="headerlink" title="Embedded"></a>Embedded</h3><h4 id="基于惩罚项的特征选择法"><a href="#基于惩罚项的特征选择法" class="headerlink" title="基于惩罚项的特征选择法"></a>基于惩罚项的特征选择法</h4><h4 id="Lasso-L1-和-Ridge-L2"><a href="#Lasso-L1-和-Ridge-L2" class="headerlink" title="Lasso(L1) 和 Ridge(L2)"></a>Lasso(L1) 和 Ridge(L2)</h4><pre><code class="lang-python"># Embedded# 基于惩罚项的特征选择法# Lasso(l1)和Ridge(l2)from sklearn.linear_model import Ridgeridge = Ridge(alpha=5)ridge.fit(train,target_train)Ridge(alpha=5, copy_X=True, fit_intercept=True, max_iter=None, normalize=False,      random_state=None, solver=&#39;auto&#39;, tol=0.001)# 特征系数排序coefSort = ridge.coef_.argsort()print(coefSort)# 特征系数featureCoefSore=ridge.coef_[coefSort]print(featureCoefSore)select_columns = [f for f, s in zip(train.columns, featureCoefSore) if abs(s)&gt; 0.0000005 ] # 选择绝对值大于0.0000005的特征new_train = train[select_columns]new_test = test[select_columns]# Lasso回归from sklearn.linear_model import Lassolasso=Lasso(alpha=0.1)lasso.fit(new_train,target_train)#预测测试集和训练集结果y_pred_train=lasso.predict(new_train)y_pred_test=lasso.predict(new_test)#对比结果from sklearn.metrics import r2_scorescore_train=r2_score(y_pred_train,target_train)print(&quot;训练集结果：&quot;,score_train)score_test=r2_score(y_pred_test, target_test)print(&quot;测试集结果：&quot;,score_test)</code></pre><h4 id="基于树模型的特征选择法"><a href="#基于树模型的特征选择法" class="headerlink" title="基于树模型的特征选择法"></a>基于树模型的特征选择法</h4><h4 id="随机森林-平均不纯度减少（mean-decrease-impurity）"><a href="#随机森林-平均不纯度减少（mean-decrease-impurity）" class="headerlink" title="随机森林 平均不纯度减少（mean decrease impurity）"></a>随机森林 平均不纯度减少（mean decrease impurity）</h4><pre><code class="lang-python"># Embedded# 基于树模型的特征选择法# 随机森林 平均不纯度减少（mean decrease impurityfrom sklearn.ensemble import RandomForestRegressorrf = RandomForestRegressor()# 训练随机森林模型，并通过feature_importances_属性获取每个特征的重要性分数。rf = RandomForestRegressor()rf.fit(train,target_train)print(&quot;Features sorted by their score:&quot;)print(sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), train.columns),             reverse=True))select_columns = [f for f, s in zip(train.columns, rf.feature_importances_) if abs(s)&gt; 0.00005 ] # 选择绝对值大于0.00005的特征new_train = train[select_columns]new_test = test[select_columns]# Lasso回归from sklearn.linear_model import Lassolasso=Lasso(alpha=0.1)lasso.fit(new_train,target_train)#预测测试集和训练集结果y_pred_train=lasso.predict(new_train)y_pred_test=lasso.predict(new_test)#对比结果from sklearn.metrics import r2_scorescore_train=r2_score(y_pred_train,target_train)print(&quot;训练集结果：&quot;,score_train)score_test=r2_score(y_pred_test, target_test)print(&quot;测试集结果：&quot;,score_test)</code></pre><h2 id="四-模型选择"><a href="#四-模型选择" class="headerlink" title="(四) 模型选择"></a>(四) 模型选择</h2><h3 id="以-lightGBM-为例"><a href="#以-lightGBM-为例" class="headerlink" title="以 lightGBM 为例"></a>以 lightGBM 为例</h3><pre><code class="lang-py">from __future__ import print_functionimport lightgbm as lgbimport sklearnimport numpyimport hyperoptfrom hyperopt import hp, fmin, tpe, STATUS_OK, Trialsimport coloramaimport numpy as npN_HYPEROPT_PROBES = 500HYPEROPT_ALGO = tpe.suggest  #  tpe.suggest OR hyperopt.rand.suggest# ----------------------------------------------------------colorama.init()# ---------------------------------------------------------------------def get_lgb_params(space):    lgb_params = dict()    lgb_params[&#39;boosting_type&#39;] = space[&#39;boosting_type&#39;] if &#39;boosting_type&#39; in space else &#39;gbdt&#39;    lgb_params[&#39;objective&#39;] = &#39;regression&#39;    lgb_params[&#39;metric&#39;] = &#39;rmse&#39;    lgb_params[&#39;learning_rate&#39;] = space[&#39;learning_rate&#39;]    lgb_params[&#39;num_leaves&#39;] = int(space[&#39;num_leaves&#39;])    lgb_params[&#39;min_data_in_leaf&#39;] = int(space[&#39;min_data_in_leaf&#39;])    lgb_params[&#39;min_sum_hessian_in_leaf&#39;] = space[&#39;min_sum_hessian_in_leaf&#39;]    lgb_params[&#39;max_depth&#39;] = -1    lgb_params[&#39;lambda_l1&#39;] = space[&#39;lambda_l1&#39;] if &#39;lambda_l1&#39; in space else 0.0    lgb_params[&#39;lambda_l2&#39;] = space[&#39;lambda_l2&#39;] if &#39;lambda_l2&#39; in space else 0.0    lgb_params[&#39;max_bin&#39;] = int(space[&#39;max_bin&#39;]) if &#39;max_bin&#39; in space else 256    lgb_params[&#39;feature_fraction&#39;] = space[&#39;feature_fraction&#39;]    lgb_params[&#39;bagging_fraction&#39;] = space[&#39;bagging_fraction&#39;]    lgb_params[&#39;bagging_freq&#39;] = int(space[&#39;bagging_freq&#39;]) if &#39;bagging_freq&#39; in space else 1    lgb_params[&#39;nthread&#39;] = 4    return lgb_params# ---------------------------------------------------------------------obj_call_count = 0cur_best_score = 0 # 0 or np.inflog_writer = open( &#39;../log/lgb-hyperopt-log.txt&#39;, &#39;w&#39; )def objective(space):    global obj_call_count, cur_best_score    obj_call_count += 1    print(&#39;\nLightGBM objective call #{} cur_best_score={:7.5f}&#39;.format(obj_call_count,cur_best_score) )    lgb_params = get_lgb_params(space)    sorted_params = sorted(space.items(), key=lambda z: z[0])    params_str = str.join(&#39; &#39;, [&#39;{}={}&#39;.format(k, v) for k, v in sorted_params])    print(&#39;Params: {}&#39;.format(params_str) )    kf = KFold(n_splits=3, shuffle=True, random_state=0)    out_of_fold = np.zeros(len(X_train))    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):        D_train = lgb.Dataset(X_train.iloc[train_idx], label=Y_train[train_idx])        D_val = lgb.Dataset(X_train.iloc[val_idx], label=Y_train[val_idx])        # Train        num_round = 10000        clf = lgb.train(lgb_params,                           D_train,                           num_boost_round=num_round,                           # metrics=&#39;mlogloss&#39;,                           valid_sets=D_val,                           # valid_names=&#39;val&#39;,                           # fobj=None,                           # feval=None,                           # init_model=None,                           # feature_name=&#39;auto&#39;,                           # categorical_feature=&#39;auto&#39;,                           early_stopping_rounds=200,                           # evals_result=None,                           verbose_eval=False,                           # learning_rates=None,                           # keep_training_booster=False,                           # callbacks=None                           )        # predict        nb_trees = clf.best_iteration        val_loss = clf.best_score[&#39;valid_0&#39;]        print(&#39;nb_trees={} val_loss={}&#39;.format(nb_trees, val_loss))        out_of_fold[val_idx] = clf.predict(X_train.iloc[val_idx], num_iteration=nb_trees)        score = r2_score(out_of_fold, Y_train)    print(&#39;val_r2_score={}&#39;.format(score))    log_writer.write(&#39;score={} Params:{} nb_trees={}\n&#39;.format(score, params_str, nb_trees ))    log_writer.flush()    if score&gt;cur_best_score:        cur_best_score = score        print(colorama.Fore.GREEN + &#39;NEW BEST SCORE={}&#39;.format(cur_best_score) + colorama.Fore.RESET)    return {&#39;loss&#39;: -score, &#39;status&#39;: STATUS_OK}# --------------------------------------------------------------------------------space ={        &#39;num_leaves&#39;: hp.quniform (&#39;num_leaves&#39;, 10, 100, 1),        &#39;min_data_in_leaf&#39;:  hp.quniform (&#39;min_data_in_leaf&#39;, 10, 100, 1),        &#39;feature_fraction&#39;: hp.uniform(&#39;feature_fraction&#39;, 0.75, 1.0),        &#39;bagging_fraction&#39;: hp.uniform(&#39;bagging_fraction&#39;, 0.75, 1.0),        &#39;learning_rate&#39;: hp.uniform(&#39;learning_rate&#39;, 0, 0.01),#         &#39;learning_rate&#39;: hp.loguniform(&#39;learning_rate&#39;, -5.0, -2.3),        &#39;min_sum_hessian_in_leaf&#39;: hp.loguniform(&#39;min_sum_hessian_in_leaf&#39;, 0, 2.3),        &#39;max_bin&#39;: hp.quniform (&#39;max_bin&#39;, 88, 200, 1),        &#39;bagging_freq&#39;: hp.quniform (&#39;bagging_freq&#39;, 1, 15, 1),        &#39;lambda_l1&#39;: hp.uniform(&#39;lambda_l1&#39;, 0, 10 ),        &#39;lambda_l2&#39;: hp.uniform(&#39;lambda_l2&#39;, 0, 10 ),       }trials = Trials()best = hyperopt.fmin(fn=objective,                     space=space,                     algo=HYPEROPT_ALGO,                     max_evals=N_HYPEROPT_PROBES,                     trials=trials,                     verbose=1)print(&#39;-&#39;*50)print(&#39;The best params:&#39;)print( best )print(&#39;\n\n&#39;)</code></pre><h2 id="五-模型融合"><a href="#五-模型融合" class="headerlink" title="(五) 模型融合"></a>(五) 模型融合</h2><p>将特征放进模型中预测，并将预测结果作为新的特征加入原有特征中再经过模型预测结果（可以反复预测多次将结果加入最后的特征中）</p><pre><code class="lang-python">from sklearn.model_selection import KFold    folds = KFold(n_splits=5, shuffle=True, random_state=2333)    &quot;===================================第一轮========================================================&quot;    y_pre_list = []    r2_list = []    train_feat = pd.Series()    for fold_, (trn_idx, val_idx) in enumerate(folds.split(feature.values, label)):        print(&quot;fold {}&quot;.format(fold_))        trn_data = lgb.Dataset(feature.iloc[trn_idx], label[trn_idx], categorical_feature=categorical_feats)        val_data = lgb.Dataset(feature.iloc[val_idx], label[val_idx], categorical_feature=categorical_feats)        num_round = 10000        clf = lgb.train(params, trn_data, num_round,valid_sets=[trn_data, val_data], verbose_eval=500,                    early_stopping_rounds=200)        y_pre = clf.predict(feature.iloc[val_idx], num_iteration=clf.best_iteration)        r2 = r2_score(y_pre,label[val_idx])        r2_list.append(r2)        train_feat = train_feat.append(pd.Series(y_pre,index=val_idx))        y_pre_test = clf.predict(test,num_iteration=clf.best_iteration)        y_pre_list.append(y_pre_test)    print(&#39;r2 score{:}&#39;.format(r2))    print(&#39;r2:{:}&#39;.format(np.mean(r2_list)))    y_pred_final=  (y_pre_list[0]+y_pre_list[1]+y_pre_list[2]+y_pre_list[3]+y_pre_list[4])/5    feature[&#39;pre&#39;] = train_feat    test[&#39;pre&#39;] = y_pred_final    &quot;===================================第二轮========================================================&quot;    y_pre_list = []    r2_list = []    train_feat = pd.Series()    for fold_, (trn_idx, val_idx) in enumerate(folds.split(feature.values, label)):        print(&quot;fold {}&quot;.format(fold_))        trn_data = lgb.Dataset(feature.iloc[trn_idx], label[trn_idx], categorical_feature=categorical_feats)        val_data = lgb.Dataset(feature.iloc[val_idx], label[val_idx], categorical_feature=categorical_feats)        num_round = 10000        clf = lgb.train(params, trn_data, num_round, feval=get_r2_metric,valid_sets=[trn_data, val_data], verbose_eval=500,                    early_stopping_rounds=200)        y_pre = clf.predict(feature.iloc[val_idx], num_iteration=clf.best_iteration)        r2 = r2_score(y_pre,label[val_idx])        r2_list.append(r2)        train_feat = train_feat.append(pd.Series(y_pre,index=val_idx))        y_pre_test = clf.predict(test,num_iteration=clf.best_iteration)        y_pre_list.append(y_pre_test)    print(&#39;r2 score{:}&#39;.format(r2))    print(&#39;r2:{:}&#39;.format(np.mean(r2_list)))    y_pred_final=  (y_pre_list[0]+y_pre_list[1]+y_pre_list[2]+y_pre_list[3]+y_pre_list[4])/5    feature[&#39;pre_2&#39;] = train_feat    test[&#39;pre_2&#39;] = y_pred_final    &quot;=======================第三轮========================================================&quot;    y_pre_list = []    r2_list = []    train_feat = pd.Series()    for fold_, (trn_idx, val_idx) in enumerate(folds.split(feature.values, label)):        print(&quot;fold {}&quot;.format(fold_))        trn_data = lgb.Dataset(feature.iloc[trn_idx], label[trn_idx], categorical_feature=categorical_feats)        val_data = lgb.Dataset(feature.iloc[val_idx], label[val_idx], categorical_feature=categorical_feats)        num_round = 10000        clf = lgb.train(params, trn_data, num_round, feval=get_r2_metric,valid_sets=[trn_data, val_data], verbose_eval=500,                    early_stopping_rounds=200)        y_pre = clf.predict(feature.iloc[val_idx], num_iteration=clf.best_iteration)        r2 = r2_score(y_pre,label[val_idx])        r2_list.append(r2)        train_feat = train_feat.append(pd.Series(y_pre,index=val_idx))        y_pre_test = clf.predict(test,num_iteration=clf.best_iteration)        y_pre_list.append(y_pre_test)    print(&#39;r2 score{:}&#39;.format(r2))    print(&#39;r2:{:}&#39;.format(np.mean(r2_list)))    y_pred_final=  (y_pre_list[0]+y_pre_list[1]+y_pre_list[2]+y_pre_list[3]+y_pre_list[4])/5    return y_pred_final</code></pre><p>pre1-pren分别是n组模型预测出来的结果，将其进行加权融合</p><pre><code class="lang-python">pre = (pre1 + pre2 + pre3 +...+pren )/npd.DataFrame(pre).to_csv(&quot;pre.csv&quot;,header=None,index=None)</code></pre><h3 id="blending"><a href="#blending" class="headerlink" title="blending"></a>blending</h3><pre><code class="lang-python">def blend(train,test,target):    &#39;&#39;&#39;5折&#39;&#39;&#39;    # n_flods = 5    # skf = list(StratifiedKFold(y, n_folds=n_flods))    &#39;&#39;&#39;切分训练数据集为d1,d2两部分&#39;&#39;&#39;    X_d1, X_d2, y_d1, y_d2 = train_test_split(train, target, test_size=0.5, random_state=914)    train_ = np.zeros((X_d2.shape[0],len(clfs*3)))    test_ = np.zeros((test.shape[0],len(clfs*3)))    for j,clf in enumerate(clfs):        &#39;&#39;&#39;依次训练各个单模型&#39;&#39;&#39;        # print(j, clf)        &#39;&#39;&#39;使用第1个部分作为预测，第2部分来训练模型，获得其预测的输出作为第2部分的新特征。&#39;&#39;&#39;        # X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test]        X_d1fillna=X_d1.fillna(0)        X_d2fillna = X_d2.fillna(0)        X_predictfillna= test.fillna(0)        clf.fit(X_d1fillna,y_d1)        y_submission = clf.predict(X_d2fillna)        y_test_submission = clf.predict(X_predictfillna)        train_[:,j*3] = y_submission*y_submission        &#39;&#39;&#39;对于测试集，直接用这k个模型的预测值作为新的特征。&#39;&#39;&#39;        test_[:, j*3] = y_test_submission*y_test_submission        train_[:, j+1] =(y_submission - y_submission.min()) /(y_submission.max() - y_submission.min())        &#39;&#39;&#39;对于测试集，直接用这k个模型的预测值作为新的特征。&#39;&#39;&#39;        y_test_submission = (y_test_submission - y_test_submission.min()) / \                            (y_test_submission.max() - y_test_submission.min())        test_[:, j+1] = y_test_submission        train_[:, j+2] = np.log(y_submission)        &#39;&#39;&#39;对于测试集，直接用这k个模型的预测值作为新的特征。&#39;&#39;&#39;        y_test_submission =np.log(y_test_submission)        test_[:, j+2] = y_test_submission        # print(&quot;val auc Score: %f&quot; % r2_score(y_predict, dataset_d2[:, j]))        print(&#39;已完成第&#39;,j)    train_.to_csv(&#39;./input/train_blending.csv&#39;, index=False)    test_.to_csv(&#39;./input/test_blending.csv&#39;, index=False)</code></pre><h3 id="Stacking"><a href="#Stacking" class="headerlink" title="Stacking"></a>Stacking</h3><pre><code class="lang-python">#!pip install mlxtendimport warningswarnings.filterwarnings(&#39;ignore&#39;)import itertoolsimport numpy as npimport seaborn as snsimport matplotlib.pyplot as pltimport matplotlib.gridspec as gridspecfrom sklearn import datasetsfrom sklearn.linear_model import LogisticRegressionfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.naive_bayes import GaussianNB from sklearn.ensemble import RandomForestClassifierfrom mlxtend.classifier import StackingClassifierfrom sklearn.model_selection import cross_val_score, train_test_splitfrom mlxtend.plotting import plot_learning_curvesfrom mlxtend.plotting import plot_decision_regions</code></pre><pre><code class="lang-python"># 以python自带的鸢尾花数据集为例iris = datasets.load_iris()X, y = iris.data[:, 1:3], iris.targetclf1 = KNeighborsClassifier(n_neighbors=1)clf2 = RandomForestClassifier(random_state=1)clf3 = GaussianNB()lr = LogisticRegression()sclf = StackingClassifier(classifiers=[clf1, clf2, clf3],                           meta_classifier=lr)label = [&#39;KNN&#39;, &#39;Random Forest&#39;, &#39;Naive Bayes&#39;, &#39;Stacking Classifier&#39;]clf_list = [clf1, clf2, clf3, sclf]fig = plt.figure(figsize=(10,8))gs = gridspec.GridSpec(2, 2)grid = itertools.product([0,1],repeat=2)clf_cv_mean = []clf_cv_std = []for clf, label, grd in zip(clf_list, label, grid):    scores = cross_val_score(clf, X, y, cv=3, scoring=&#39;accuracy&#39;)    print(&quot;Accuracy: %.2f (+/- %.2f) [%s]&quot; %(scores.mean(), scores.std(), label))    clf_cv_mean.append(scores.mean())    clf_cv_std.append(scores.std())    clf.fit(X, y)    ax = plt.subplot(gs[grd[0], grd[1]])    fig = plot_decision_regions(X=X, y=y, clf=clf)    plt.title(label)plt.show()</code></pre><pre><code>Accuracy: 0.91 (+/- 0.01) [KNN]Accuracy: 0.93 (+/- 0.05) [Random Forest]Accuracy: 0.92 (+/- 0.03) [Naive Bayes]Accuracy: 0.95 (+/- 0.03) [Stacking Classifier]</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/26/aGs18DEHCYZFnoP.png" alt=""></p><h2 id="六-总结"><a href="#六-总结" class="headerlink" title="(六) 总结"></a>(六) 总结</h2><p><a href="https://blog.csdn.net/qq_39756719/article/details/95634744" target="_blank" rel="noopener">答辩 ppt</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> project </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动手深度学习(十一):现代 CNN 模型</title>
      <link href="/2020/02/20/dl-notes11-modern-cnn/"/>
      <url>/2020/02/20/dl-notes11-modern-cnn/</url>
      
        <content type="html"><![CDATA[<h1 id="深度卷积神经网络"><a href="#深度卷积神经网络" class="headerlink" title="深度卷积神经网络"></a>深度卷积神经网络</h1><p>LeNet:  在大的真实数据集上的表现并不尽如⼈意。<br>1.神经网络计算复杂。<br>2.还没有⼤量深⼊研究参数初始化和⾮凸优化算法等诸多领域。  </p><p>机器学习的特征提取:手工定义的特征提取函数<br>神经网络的特征提取：通过学习得到数据的多级表征，并逐级表⽰越来越抽象的概念或模式。  </p><p>神经网络发展的限制:数据、硬件</p><h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><p>首次证明了学习到的特征可以超越⼿⼯设计的特征，从而⼀举打破计算机视觉研究的前状。<br><strong>特征：</strong></p><ol><li>8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层。</li><li>将sigmoid激活函数改成了更加简单的ReLU激活函数。<br> ReLu函数的好处：缓解梯度消失，导数计算更快，收敛快，负数的时候梯度为 0，有正则化，稀疏化的作用</li><li>用Dropout来控制全连接层的模型复杂度。<br> Dropout 解决过拟合，不会过度依赖某个神经元</li><li>引入数据增强，如翻转、裁剪和颜色变化，从而进一步扩大数据集来缓解过拟合。</li></ol><p>更多的通道数，具有更多的特征</p><p>计算示例（第一层）：(224-11)/4 + 1 = 54 这一层输出的宽和高就是 54，通道数是 96</p><p>下一层 (54-3)/2 +1 = 26</p><p>下一层 (26+4-5)/1 + 1 =26<br>…</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/d9UtZ2CpjrIbiyV.png" alt="计算公式"></p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/upload/image/q5kv4gpx88.png?imageView2/0/w/640/h/640" alt="LeNet 和 AlexNet 对比"></p><pre><code class="lang-python">#考虑到本代码中的模型过大，CPU 训练较慢，需要 GPU#我们还将代码上传了一份到 https://www.kaggle.com/boyuai/boyu-d2l-modernconvolutionalnetwork#如希望提前使用gpu运行请至kaggle。import timeimport torchfrom torch import nn, optimimport torchvisionimport numpy as npimport syssys.path.append(&quot;/home/kesci/input/&quot;) import d2lzh1981 as d2limport osimport torch.nn.functional as Fdevice = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)class AlexNet(nn.Module):    def __init__(self):        super(AlexNet, self).__init__()        self.conv = nn.Sequential(            nn.Conv2d(1, 96, 11, 4), # in_channels, out_channels, kernel_size, stride, padding            nn.ReLU(),            nn.MaxPool2d(3, 2), # kernel_size, stride            # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数            nn.Conv2d(96, 256, 5, 1, 2),            nn.ReLU(),            nn.MaxPool2d(3, 2),            # 连续3个卷积层，且使用更小的卷积窗口。除了最后的卷积层外，进一步增大了输出通道数。            # 前两个卷积层后不使用池化层来减小输入的高和宽            nn.Conv2d(256, 384, 3, 1, 1),            nn.ReLU(),            nn.Conv2d(384, 384, 3, 1, 1),            nn.ReLU(),            nn.Conv2d(384, 256, 3, 1, 1),            nn.ReLU(),            nn.MaxPool2d(3, 2)        )         # 这里全连接层的输出个数比LeNet中的大数倍。使用丢弃层来缓解过拟合        self.fc = nn.Sequential(            nn.Linear(256*5*5, 4096),            nn.ReLU(),            nn.Dropout(0.5),            #由于使用CPU镜像，精简网络，若为GPU镜像可添加该层            #nn.Linear(4096, 4096),            #nn.ReLU(),            #nn.Dropout(0.5),            # 输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000            nn.Linear(4096, 10),        )    def forward(self, img):  # 把 self.conv 和 self.fc 连接起来        feature = self.conv(img)  # 输出 4 维：batch_size * channels * h * w        output = self.fc(feature.view(img.shape[0], -1))  # 变成 2 维：batch_size * hiddens        return output</code></pre><pre><code class="lang-python">net = AlexNet()print(net)</code></pre><pre><code>AlexNet(  (conv): Sequential(    (0): Conv2d(1, 96, kernel_size=(11, 11), stride=(4, 4))    (1): ReLU()    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)    (3): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))    (4): ReLU()    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)    (6): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (7): ReLU()    (8): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (9): ReLU()    (10): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (11): ReLU()    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)  )  (fc): Sequential(    (0): Linear(in_features=6400, out_features=4096, bias=True)    (1): ReLU()    (2): Dropout(p=0.5, inplace=False)    (3): Linear(in_features=4096, out_features=10, bias=True)  ))</code></pre><h3 id="载入数据集"><a href="#载入数据集" class="headerlink" title="载入数据集"></a>载入数据集</h3><pre><code class="lang-python"># 本函数已保存在d2lzh_pytorch包中方便以后使用def load_data_fashion_mnist(batch_size, resize=None, root=&#39;/home/kesci/input/FashionMNIST2065&#39;):    &quot;&quot;&quot;Download the fashion mnist dataset and then load into memory.&quot;&quot;&quot;    trans = []    if resize:        trans.append(torchvision.transforms.Resize(size=resize))    trans.append(torchvision.transforms.ToTensor())  # 变成 tensor 才能训练    transform = torchvision.transforms.Compose(trans)    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=True, download=True, transform=transform)    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=False, download=True, transform=transform)    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=2)    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=2)    return train_iter, test_iter#batchsize=128batch_size = 16# 如出现“out of memory”的报错信息，可减小batch_size或resizetrain_iter, test_iter = load_data_fashion_mnist(batch_size,224)for X, Y in train_iter:    print(&#39;X =&#39;, X.shape,        &#39;\nY =&#39;, Y.type(torch.int32)) # Y 就是标签    break</code></pre><pre><code>X = torch.Size([16, 1, 224, 224]) Y = tensor([2, 4, 5, 2, 3, 2, 7, 7, 7, 3, 5, 7, 8, 2, 6, 3], dtype=torch.int32)</code></pre><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><pre><code class="lang-python">lr, num_epochs = 0.001, 3optimizer = torch.optim.Adam(net.parameters(), lr=lr)d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</code></pre><pre><code>training on  cpuepoch 1, loss 0.4997, train acc 0.812, test acc 0.865, time 3995.5 sec</code></pre><p>cpu 特别慢，明显不如 cuda(gpu)</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/U1pLjfKmEd8IAnu.png" alt="AlexNet 结果"></p><h2 id="使用重复元素的网络（VGG）"><a href="#使用重复元素的网络（VGG）" class="headerlink" title="使用重复元素的网络（VGG）"></a>使用重复元素的网络（VGG）</h2><p>AlexNet：比较死板，很难扩展<br>VGG：通过重复使⽤简单的基础块来构建深度模型。（不同的 VGG 结构是一样的，但参数可能不一样）<br>Block:数个相同的填充为1、窗口形状为$3\times 3$的卷积层,接上一个步幅为2、窗口形状为$2\times 2$的最大池化层（图中写错）<br>卷积层保持输入的高和宽不变，而池化层则对其减半。</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/upload/image/q5l6vut7h1.png?imageView2/0/w/640/h/640" alt=""></p><h3 id="VGG11的简单实现"><a href="#VGG11的简单实现" class="headerlink" title="VGG11的简单实现"></a>VGG11的简单实现</h3><pre><code class="lang-python">def vgg_block(num_convs, in_channels, out_channels): #卷积层个数，输入通道数，输出通道数    blk = []    for i in range(num_convs):        if i == 0:            blk.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))        else:            blk.append(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)) # 之后通道数都不变        blk.append(nn.ReLU())    blk.append(nn.MaxPool2d(kernel_size=2, stride=2)) # 这里会使宽高减半    return nn.Sequential(*blk)  # 对 block 内进行组合</code></pre><pre><code class="lang-python">conv_arch = ((1, 1, 64), (1, 64, 128), (2, 128, 256), (2, 256, 512), (2, 512, 512)) # 每一层的 channel h w# 经过5个vgg_block, 宽高会减半5次, 变成 224/32 = 7，最后变成 7 * 7 * 512fc_features = 512 * 7 * 7 # c * w * hfc_hidden_units = 4096 # 任意</code></pre><pre><code class="lang-python">def vgg(conv_arch, fc_features, fc_hidden_units=4096): # 把多个 block 组合起来    net = nn.Sequential()    # 卷积层部分    for i, (num_convs, in_channels, out_channels) in enumerate(conv_arch):        # 每经过一个vgg_block都会使宽高减半        net.add_module(&quot;vgg_block_&quot; + str(i+1), vgg_block(num_convs, in_channels, out_channels))    # 全连接层部分    net.add_module(&quot;fc&quot;, nn.Sequential(d2l.FlattenLayer(),                                 nn.Linear(fc_features, fc_hidden_units),                                 nn.ReLU(),                                 nn.Dropout(0.5),                                 nn.Linear(fc_hidden_units, fc_hidden_units),                                 nn.ReLU(),                                 nn.Dropout(0.5),                                 nn.Linear(fc_hidden_units, 10)  # 两个隐藏层，一个输出层                                ))    return net</code></pre><pre><code class="lang-python">net = vgg(conv_arch, fc_features, fc_hidden_units)X = torch.rand(1, 1, 224, 224)# named_children获取一级子模块及其名字(named_modules会返回所有子模块,包括子模块的子模块)for name, blk in net.named_children():     X = blk(X)    print(name, &#39;output shape: &#39;, X.shape)</code></pre><pre><code>vgg_block_1 output shape:  torch.Size([1, 64, 112, 112])vgg_block_2 output shape:  torch.Size([1, 128, 56, 56])vgg_block_3 output shape:  torch.Size([1, 256, 28, 28])vgg_block_4 output shape:  torch.Size([1, 512, 14, 14])vgg_block_5 output shape:  torch.Size([1, 512, 7, 7])fc output shape:  torch.Size([1, 10])</code></pre><pre><code class="lang-python">ratio = 8  # Fashion数据集不用太多参数，采用小一点的模型，也防止过拟合small_conv_arch = [(1, 1, 64//ratio), (1, 64//ratio, 128//ratio), (2, 128//ratio, 256//ratio),                    (2, 256//ratio, 512//ratio), (2, 512//ratio, 512//ratio)]net = vgg(small_conv_arch, fc_features // ratio, fc_hidden_units // ratio)print(net)</code></pre><pre><code>Sequential(  (vgg_block_1): Sequential(    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (1): ReLU()    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)  )  (vgg_block_2): Sequential(    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (1): ReLU()    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)  )  (vgg_block_3): Sequential(    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (1): ReLU()    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (3): ReLU()    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)  )  (vgg_block_4): Sequential(    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (1): ReLU()    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (3): ReLU()    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)  )  (vgg_block_5): Sequential(    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (1): ReLU()    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (3): ReLU()    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)  )  (fc): Sequential(    (0): FlattenLayer()    (1): Linear(in_features=3136, out_features=512, bias=True)    (2): ReLU()    (3): Dropout(p=0.5, inplace=False)    (4): Linear(in_features=512, out_features=512, bias=True)    (5): ReLU()    (6): Dropout(p=0.5, inplace=False)    (7): Linear(in_features=512, out_features=10, bias=True)  ))</code></pre><pre><code class="lang-python">batchsize=16#batch_size = 64# 如出现“out of memory”的报错信息，可减小batch_size或resize# train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)lr, num_epochs = 0.001, 5optimizer = torch.optim.Adam(net.parameters(), lr=lr)d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</code></pre><h2 id="⽹络中的⽹络（NiN）"><a href="#⽹络中的⽹络（NiN）" class="headerlink" title="⽹络中的⽹络（NiN）"></a>⽹络中的⽹络（NiN）</h2><p>LeNet、AlexNet和VGG：先以由卷积层构成的模块充分抽取空间特征，再以由<code>全连接层</code>构成的模块来输出分类结果。 </p><p>NiN：</p><ul><li><code>串联</code>多个由卷积层和“全连接”层构成的小⽹络来构建⼀个深层⽹络。（小模块组装起来）</li><li>⽤了<code>输出通道数</code>等于<code>标签类别数</code>的NiN块，通过调节最后一个模块的通道数，来使最后的输出等于类别数</li><li>然后使⽤<code>全局平均池化层</code> GlobalAvgPool 对每个通道中所有元素求平均并直接⽤于分类，可以减少模型参数，缓解过拟合，但也会造成模型训练时间增加</li></ul><h2 id="卷积层与全连接层-对比分析"><a href="#卷积层与全连接层-对比分析" class="headerlink" title="卷积层与全连接层:对比分析"></a>卷积层与全连接层:对比分析</h2><p>多换一个数据集的话（主要是size的变化），NiN block仅需要改变第一个Convolution的大小，而后续的1x1的Convolution保持不变</p><p>卷积和全连接的一个重要的差别，卷积其实<code>不关心特征图的H和W</code>.<br>如果你从10分类shape224<em>224</em>3的彩色数据集，切换到一个2分类shape160<em>160</em>1的灰度图数据集.<br>只需要替换第一个卷积的input_channel 和最后一个卷积或者全连接层的output_channel.<br>而如果你是从彩色数据集换成了彩色数据集，第一个卷积甚至都不需要发生变化.</p><h2 id="理解卷积工作原理"><a href="#理解卷积工作原理" class="headerlink" title="理解卷积工作原理"></a>理解卷积工作原理</h2><ol><li><p>卷积神经网络通过使用<code>滑动窗口</code>在输入的不同位置处重复计算，减小参数数量。连接卷积层块和全连接层块时，需要做一次展平操作。</p></li><li><p>在通过卷积层或池化层后，输出的高和宽可能减小，为了尽可能<code>保留输入的特征</code>，我们可以在减小高宽的同时增加<code>通道数</code>。</p></li><li><p>卷积的工作流程是，以一定的卷积核大小，一定的步长，在特征图的不同位置，用<code>同一个卷积核</code>来进行互相关运算。</p></li><li><p>这就好像是，一个卷积核是用来提取某一种<code>局部特征</code>的，它在图像中的不同位置来寻找是否有符合它所关心的特征的局部区域。</p></li><li><p>这种工作机制导致了图像的<code>尺寸（宽和高）并不影响</code>卷积运算，只有通道数的变化才会影响。可以和权值共享的概念联系一下理解。</p></li><li><p>1x1 conv除了改变channel数，还有增加非线性的作用，一般都会跟着一个<code>激活层</code>，其他感觉比较玄学设计，CV里可能跟ReLU可以做到单侧抑制增加稀疏有关吧。</p></li></ol><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/upload/image/q5l6u1p5vy.png?imageView2/0/w/960/h/960" alt=""></p><ul><li><p><code>1×1卷积核作用</code></p><p>  1.放缩通道数：通过控制卷积核的数量达到通道数的放缩，放大 or 缩小<br>  2.增加非线性。1×1卷积核的卷积过程<code>相当于全连接层的计算过程</code>，并且还加入了非线性激活函数，从而可以增加网络的非线性。<br>  3.计算参数少   </p></li></ul><pre><code class="lang-python">def nin_block(in_channels, out_channels, kernel_size, stride, padding):  # 可复用的 block    blk = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),                        nn.ReLU(),                        nn.Conv2d(out_channels, out_channels, kernel_size=1), # 后面两个 channel 不变                        nn.ReLU(),                        nn.Conv2d(out_channels, out_channels, kernel_size=1), # kernel_size=1 不改变形状                         nn.ReLU())    return blk</code></pre><pre><code class="lang-python"># 已保存在d2lzh_pytorchclass GlobalAvgPool2d(nn.Module):    # 全局平均池化层可通过将池化窗口形状设置成输入的高和宽实现    def __init__(self):        super(GlobalAvgPool2d, self).__init__()    def forward(self, x):        return F.avg_pool2d(x, kernel_size=x.size()[2:]) # 对每个通道的整个 feature_map 取平均，等于 x.size()即可net = nn.Sequential(    nin_block(1, 96, kernel_size=11, stride=4, padding=0),    nn.MaxPool2d(kernel_size=3, stride=2),    nin_block(96, 256, kernel_size=5, stride=1, padding=2),    nn.MaxPool2d(kernel_size=3, stride=2),    nin_block(256, 384, kernel_size=3, stride=1, padding=1),    nn.MaxPool2d(kernel_size=3, stride=2),     nn.Dropout(0.5),    # 标签类别数是10    nin_block(384, 10, kernel_size=3, stride=1, padding=1),  # 输出为 10，=类别数    GlobalAvgPool2d(),  # 形状是（批量大小，10，1，1）    # 将四维的输出转成二维的输出，其形状为(批量大小, 10)    d2l.FlattenLayer())</code></pre><pre><code class="lang-python">X = torch.rand(1, 1, 224, 224)for name, blk in net.named_children():     X = blk(X)    print(name, &#39;output shape: &#39;, X.shape)</code></pre><pre><code>0 output shape:  torch.Size([1, 96, 54, 54])1 output shape:  torch.Size([1, 96, 26, 26])2 output shape:  torch.Size([1, 256, 26, 26])3 output shape:  torch.Size([1, 256, 12, 12])4 output shape:  torch.Size([1, 384, 12, 12])5 output shape:  torch.Size([1, 384, 5, 5])6 output shape:  torch.Size([1, 384, 5, 5])7 output shape:  torch.Size([1, 10, 5, 5])8 output shape:  torch.Size([1, 10, 1, 1])9 output shape:  torch.Size([1, 10])</code></pre><pre><code class="lang-python">batch_size = 128# 如出现“out of memory”的报错信息，可减小batch_size或resize#train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)lr, num_epochs = 0.002, 5optimizer = torch.optim.Adam(net.parameters(), lr=lr)d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</code></pre><p>NiN重复使⽤由卷积层和代替全连接层的1×1卷积层构成的NiN块来构建深层⽹络。<br>NiN去除了容易造成过拟合的全连接输出层，而是将其替换成输出通道数等于标签类别数 的NiN块和全局平均池化层。<br>NiN的以上设计思想影响了后⾯⼀系列卷积神经⽹络的设计。  </p><h2 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h2><ol><li>由Inception基础块组成。  </li><li>Inception块相当于⼀个有4条线路的⼦⽹络（并行线路）。它通过不同窗口形状的卷积层和最⼤池化层来并⾏抽取信息，并使⽤<strong>「1×1卷积层」</strong> <strong>「减少通道数从而降低模型复杂度」</strong>。   </li><li>可以⾃定义的超参数是每个层的输出通道数，我们以此来控制模型复杂度。 </li><li>都用了 padding 保证高和宽不变</li></ol><p>概括 4 个模型：</p><ul><li>AlexNet 难以灵活地改变模型结构。</li><li>VGG 通过重复使⽤简单的基础块来构建深度模型, 由VGG block组成</li><li>NiN 使⽤全局平均池化层对每个通道中所有元素求平均并直接⽤于分类, 由NiN block组成</li><li>GooLeNet 通过不同窗口形状的卷积层和最⼤池化层来并⾏抽取信息, 由Inception组成</li></ul><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/upload/image/q5l6uortw.png?imageView2/0/w/640/h/640" alt="Inception"></p><pre><code class="lang-python">class Inception(nn.Module):    # c1 - c4为每条线路里的层的输出通道数    def __init__(self, in_c, c1, c2, c3, c4):        super(Inception, self).__init__()        # 线路1，单1 x 1卷积层        self.p1_1 = nn.Conv2d(in_c, c1, kernel_size=1)        # 线路2，1 x 1卷积层后接3 x 3卷积层        self.p2_1 = nn.Conv2d(in_c, c2[0], kernel_size=1)        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)        # 线路3，1 x 1卷积层后接5 x 5卷积层        self.p3_1 = nn.Conv2d(in_c, c3[0], kernel_size=1)        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)        # 线路4，3 x 3最大池化层后接1 x 1卷积层        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)        self.p4_2 = nn.Conv2d(in_c, c4, kernel_size=1)    def forward(self, x):        p1 = F.relu(self.p1_1(x))        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))        p4 = F.relu(self.p4_2(self.p4_1(x)))        return torch.cat((p1, p2, p3, p4), dim=1)  # 在通道维上连结输出（各个线路通道数直接相加）</code></pre><h3 id="GoogLeNet模型"><a href="#GoogLeNet模型" class="headerlink" title="GoogLeNet模型"></a>GoogLeNet模型</h3><p>完整模型结构 （中间的并联是若干个 Inception）</p><p>最后一层，它可以确保经过 Global AvgPool 后输出的形状为 [batch_size, num_of_channels, 1, 1]。<br>当输入图像的尺寸发生变化时，可以发挥作用</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/upload/image/q5l6x0fyyn.png?imageView2/0/w/640/h/640" alt="完整GoogLeNet模型"></p><pre><code class="lang-python">b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),  # 抽取特征，减小宽高                   nn.ReLU(),                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),  # 1x1 增加非线性的作用                   nn.Conv2d(64, 192, kernel_size=3, padding=1), # padding=1 在 kernel_size=3 时可保持宽高不变                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))b3 = nn.Sequential(Inception(192, 64, (96, 128), (16, 32), 32),  # 对应 inception 内的各个线路                   Inception(256, 128, (128, 192), (32, 96), 64),                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))b4 = nn.Sequential(Inception(480, 192, (96, 208), (16, 48), 64),  # 480 就是上一层各个线路的叠加                   Inception(512, 160, (112, 224), (24, 64), 64),                   Inception(512, 128, (128, 256), (24, 64), 64),                   Inception(512, 112, (144, 288), (32, 64), 64),                   Inception(528, 256, (160, 320), (32, 128), 128),                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))b5 = nn.Sequential(Inception(832, 256, (160, 320), (32, 128), 128),                   Inception(832, 384, (192, 384), (48, 128), 128),                   d2l.GlobalAvgPool2d())net = nn.Sequential(b1, b2, b3, b4, b5,                     d2l.FlattenLayer(), nn.Linear(1024, 10))net = nn.Sequential(b1, b2, b3, b4, b5, d2l.FlattenLayer(), nn.Linear(1024, 10))X = torch.rand(1, 1, 96, 96)for blk in net.children():     X = blk(X)    print(&#39;output shape: &#39;, X.shape)#batchsize=128batch_size = 16# 如出现“out of memory”的报错信息，可减小batch_size或resize#train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)lr, num_epochs = 0.001, 5optimizer = torch.optim.Adam(net.parameters(), lr=lr)d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</code></pre><p>每一层训练，feature_map 图像的宽高都会减半，特征数都会翻倍，表示提取的特征越来越高级，越来越抽象</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/8xDuCcnMRre1LbU.png" alt="各层的实际参数-训练结果"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> CV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动手深度学习(十):LeNet 模型 -- CNN 升级版</title>
      <link href="/2020/02/20/dl-notes10-LeNet/"/>
      <url>/2020/02/20/dl-notes10-LeNet/</url>
      
        <content type="html"><![CDATA[<h2 id="LeNet-模型（CNN-升级版）"><a href="#LeNet-模型（CNN-升级版）" class="headerlink" title="LeNet 模型（CNN 升级版）"></a>LeNet 模型（CNN 升级版）</h2><ol><li>lenet 模型介绍</li><li>lenet 网络搭建</li><li>运用lenet进行图像识别-fashion-mnist数据集</li></ol><h1 id="Convolutional-Neural-Networks"><a href="#Convolutional-Neural-Networks" class="headerlink" title="Convolutional Neural Networks"></a>Convolutional Neural Networks</h1><p>使用全连接层的局限性：</p><ul><li>图像在同一列邻近的像素在这个向量中可能相距较远。它们构成的模式可能难以被模型识别。</li><li>对于大尺寸的输入图像，使用全连接层容易导致模型过大。</li></ul><p>使用卷积层的优势：</p><ul><li>卷积层保留输入形状。</li><li>卷积层通过滑动窗口将同一卷积核与不同位置的输入重复计算，从而避免参数尺寸过大。</li></ul><h2 id="LeNet-模型"><a href="#LeNet-模型" class="headerlink" title="LeNet 模型"></a>LeNet 模型</h2><p>LeNet分为卷积层块和全连接层块两个部分。下面我们分别介绍这两个模块。</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://s2.ax1x.com/2020/02/19/3VrrQ0.png" alt="LeNet 模型图"></p><p>卷积层块里的基本单位是卷积层后接平均池化层：卷积层用来识别图像里的空间模式，如线条和物体局部，之后的平均池化层则用来降低卷积层对位置的敏感性。</p><p>卷积层块由两个这样的基本单位重复堆叠构成。在卷积层块中，每个卷积层都使用$5 \times 5$的窗口，并在输出上使用sigmoid激活函数。第一个卷积层输出通道数为6，第二个卷积层输出通道数则增加到16。</p><p>全连接层块含3个全连接层。它们的输出个数分别是120、84和10，其中10为输出的类别个数。</p><p>下面我们通过Sequential类来实现LeNet模型。</p><pre><code class="lang-python">#importimport syssys.path.append(&quot;/home/kesci/input&quot;)import d2lzh1981 as d2limport torchimport torch.nn as nnimport torch.optim as optimimport time</code></pre><pre><code class="lang-python">#netclass Flatten(torch.nn.Module):  #展平操作    def forward(self, x):        return x.view(x.shape[0], -1)class Reshape(torch.nn.Module): #将图像大小重定型    def forward(self, x):        return x.view(-1,1,28,28)      #(B x C x H x W)net = torch.nn.Sequential(     #Lelet                                                      Reshape(),    nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2), #卷积 b*1*28*28  =&gt;b*6*28*28 （b:batch_size)    nn.Sigmoid(),                                                           nn.AvgPool2d(kernel_size=2, stride=2),    # 平均池化                #b*6*28*28  =&gt;b*6*14*14    nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),           #b*6*14*14  =&gt;b*16*10*10    nn.Sigmoid(),    nn.AvgPool2d(kernel_size=2, stride=2),                              #b*16*10*10  =&gt; b*16*5*5    Flatten(),          # 纯粹的展平操作                                #b*16*5*5   =&gt; b*400    nn.Linear(in_features=16*5*5, out_features=120),    nn.Sigmoid(),    nn.Linear(120, 84),    nn.Sigmoid(),    nn.Linear(84, 10))</code></pre><p>接下来我们构造一个高和宽均为28的单通道数据样本，并逐层进行前向计算来查看每个层的输出形状。</p><pre><code class="lang-python">#printX = torch.randn(size=(1,1,28,28), dtype = torch.float32)   # batch_size = 1, channel = 1, h = w =28for layer in net:    X = layer(X)    print(layer.__class__.__name__,&#39;output shape: \t&#39;,X.shape)</code></pre><pre><code>Reshape output shape:      torch.Size([1, 1, 28, 28])Conv2d output shape:      torch.Size([1, 6, 28, 28])Sigmoid output shape:      torch.Size([1, 6, 28, 28])AvgPool2d output shape:      torch.Size([1, 6, 14, 14])Conv2d output shape:      torch.Size([1, 16, 10, 10])Sigmoid output shape:      torch.Size([1, 16, 10, 10])AvgPool2d output shape:      torch.Size([1, 16, 5, 5])Flatten output shape:      torch.Size([1, 400])Linear output shape:      torch.Size([1, 120])Sigmoid output shape:      torch.Size([1, 120])Linear output shape:      torch.Size([1, 84])Sigmoid output shape:      torch.Size([1, 84])Linear output shape:      torch.Size([1, 10])</code></pre><p>可以看到，在卷积层块中输入的高和宽在逐层减小。卷积层由于使用高和宽均为5的卷积核，从而将高和宽分别减小4，而池化层则将高和宽减半，但通道数则从1增加到16。全连接层则逐层减少输出个数，直到变成图像的类别数10。</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://s2.ax1x.com/2020/02/19/3VrHeO.png" alt="逐层变化情况"></p><h2 id="获取数据和训练模型"><a href="#获取数据和训练模型" class="headerlink" title="获取数据和训练模型"></a>获取数据和训练模型</h2><p>下面我们来实现LeNet模型。我们仍然使用Fashion-MNIST作为训练数据集。</p><pre><code class="lang-python"># 数据batch_size = 256train_iter, test_iter = d2l.load_data_fashion_mnist(  # 训练集、测试集    batch_size=batch_size, root=&#39;/home/kesci/input/FashionMNIST2065&#39;)print(len(train_iter))  # 235 个 batch，每个 batch 256张图</code></pre><pre><code>235</code></pre><p>为了使读者更加形象的看到数据，添加额外的部分来展示数据的图像</p><pre><code class="lang-python">#数据展示import matplotlib.pyplot as pltdef show_fashion_mnist(images, labels):    d2l.use_svg_display()    # 这里的_表示我们忽略（不使用）的变量    _, figs = plt.subplots(1, len(images), figsize=(12, 12))    for f, img, lbl in zip(figs, images, labels):        f.imshow(img.view((28, 28)).numpy())        f.set_title(lbl)        f.axes.get_xaxis().set_visible(False)        f.axes.get_yaxis().set_visible(False)    plt.show()for Xdata,ylabel in train_iter:    breakX, y = [], []for i in range(10):    print(Xdata[i].shape,ylabel[i].numpy())    X.append(Xdata[i]) # 将第i个feature加到X中    y.append(ylabel[i].numpy()) # 将第i个label加到y中show_fashion_mnist(X, y)</code></pre><pre><code>torch.Size([1, 28, 28]) 3torch.Size([1, 28, 28]) 8torch.Size([1, 28, 28]) 1torch.Size([1, 28, 28]) 4torch.Size([1, 28, 28]) 0torch.Size([1, 28, 28]) 0torch.Size([1, 28, 28]) 4torch.Size([1, 28, 28]) 9torch.Size([1, 28, 28]) 4torch.Size([1, 28, 28]) 7</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/4FE5CE6E20494BFE898E9D8EAAF30C7B/q5ndypu2sq.svg"></p><p>因为卷积神经网络计算比多层感知机要复杂，建议使用GPU来加速计算。我们查看看是否可以用GPU，如果成功则使用<code>cuda:0</code>，否则仍然使用<code>cpu</code>。</p><pre><code class="lang-python"># This function has been saved in the d2l package for future use#use GPUdef try_gpu():    &quot;&quot;&quot;If GPU is available, return torch.device as cuda:0; else return torch.device as cpu.&quot;&quot;&quot;    if torch.cuda.is_available():        device = torch.device(&#39;cuda:0&#39;)    else:        device = torch.device(&#39;cpu&#39;)    return devicedevice = try_gpu()device</code></pre><pre><code>device(type=&#39;cpu&#39;)</code></pre><p>我们实现<code>evaluate_accuracy</code>函数，该函数用于计算模型<code>net</code>在数据集<code>data_iter</code>上的准确率。</p><pre><code class="lang-python">#计算准确率&#39;&#39;&#39;(1). net.train()  启用 BatchNormalization 和 Dropout，将BatchNormalization和Dropout置为True(2). net.eval()不启用 BatchNormalization 和 Dropout，将BatchNormalization和Dropout置为False&#39;&#39;&#39;def evaluate_accuracy(data_iter, net,device=torch.device(&#39;cpu&#39;)):    &quot;&quot;&quot;Evaluate accuracy of a model on the given data set.&quot;&quot;&quot;    acc_sum,n = torch.tensor([0],dtype=torch.float32,device=device),0    for X,y in data_iter:        # If device is the GPU, copy the data to the GPU.        X,y = X.to(device),y.to(device)        net.eval()        with torch.no_grad():   # 不需要梯度，也不会反向传播            y = y.long()            acc_sum += torch.sum((torch.argmax(net(X), dim=1) == y))              # dim=1 dim 0 -&gt; 1 -&gt; ... 维度从大到小[[0.2 ,0.4 ,0.5 ,0.6 ,0.8] ,[ 0.1,0.2 ,0.4 ,0.3 ,0.1]] =&gt; [ 4 , 2 ]            n += y.shape[0]    return acc_sum.item()/n</code></pre><p>我们定义函数<code>train_ch5</code>，用于训练模型。</p><pre><code class="lang-python">#训练函数def train_ch5(net, train_iter, test_iter,criterion, num_epochs, batch_size, device,lr=None):    &quot;&quot;&quot;Train and evaluate a model with CPU or GPU.&quot;&quot;&quot;    print(&#39;training on&#39;, device)    net.to(device)    optimizer = optim.SGD(net.parameters(), lr=lr)    for epoch in range(num_epochs):        train_l_sum = torch.tensor([0.0],dtype=torch.float32,device=device)     # loss 总和        train_acc_sum = torch.tensor([0.0],dtype=torch.float32,device=device)   # 预测正确的总数        n, start = 0, time.time()        for X, y in train_iter:  # X 训练数据，y 训练标签            net.train()  # 与 net.eval 相反，启用 批标准化和 dropout            optimizer.zero_grad()  # 不同批的梯度不相关，要清零            X,y = X.to(device),y.to(device)             y_hat = net(X)            loss = criterion(y_hat, y)            loss.backward()            optimizer.step()            with torch.no_grad():                y = y.long()                train_l_sum += loss.float()                train_acc_sum += (torch.sum((torch.argmax(y_hat, dim=1) == y))).float()                n += y.shape[0]        test_acc = evaluate_accuracy(test_iter, net,device)        print(&#39;epoch %d, loss %.4f, train acc %.3f, test acc %.3f, &#39;              &#39;time %.1f sec&#39;              % (epoch + 1, train_l_sum/n, train_acc_sum/n, test_acc,                 time.time() - start))</code></pre><p>我们重新将模型参数初始化到对应的设备<code>device</code>(<code>cpu</code> or <code>cuda:0</code>)之上，并使用Xavier随机初始化。损失函数和训练算法则依然使用交叉熵损失函数和小批量随机梯度下降。</p><pre><code class="lang-python"># 训练lr, num_epochs = 0.9, 10def init_weights(m):    if type(m) == nn.Linear or type(m) == nn.Conv2d:        torch.nn.init.xavier_uniform_(m.weight)net.apply(init_weights)net = net.to(device)criterion = nn.CrossEntropyLoss()   #交叉熵描述了两个概率分布之间的距离，交叉熵越小说明两者之间越接近train_ch5(net, train_iter, test_iter, criterion,num_epochs, batch_size,device, lr)</code></pre><pre><code>training on cpuepoch 1, loss 0.0091, train acc 0.100, test acc 0.168, time 21.6 secepoch 2, loss 0.0065, train acc 0.355, test acc 0.599, time 21.5 secepoch 3, loss 0.0035, train acc 0.651, test acc 0.665, time 21.8 secepoch 4, loss 0.0028, train acc 0.717, test acc 0.723, time 21.7 secepoch 5, loss 0.0025, train acc 0.746, test acc 0.753, time 21.4 secepoch 6, loss 0.0023, train acc 0.767, test acc 0.754, time 21.5 secepoch 7, loss 0.0022, train acc 0.782, test acc 0.785, time 21.3 secepoch 8, loss 0.0021, train acc 0.798, test acc 0.791, time 21.8 secepoch 9, loss 0.0019, train acc 0.811, test acc 0.790, time 22.0 secepoch 10, loss 0.0019, train acc 0.821, test acc 0.804, time 22.1 sec</code></pre><pre><code class="lang-python"># testfor testdata,testlabe in test_iter:    testdata,testlabe = testdata.to(device),testlabe.to(device)    breakprint(testdata.shape,testlabe.shape)net.eval()y_pre = net(testdata)print(torch.argmax(y_pre,dim=1)[:10])print(testlabe[:10])</code></pre><pre><code>torch.Size([256, 1, 28, 28]) torch.Size([256])tensor([9, 2, 1, 1, 6, 1, 2, 6, 5, 7])tensor([9, 2, 1, 1, 6, 1, 4, 6, 5, 7])</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>卷积神经网络就是含卷积层的网络。<br>LeNet交替使用卷积层和最大池化层后接全连接层来进行图像分类。</p><h2 id="练习题"><a href="#练习题" class="headerlink" title="练习题"></a>练习题</h2><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://s2.ax1x.com/2020/02/19/3VkvBd.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> CV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动手深度学习(九):卷积神经网络 CNN 基础</title>
      <link href="/2020/02/20/dl-notes9-cnn-basic/"/>
      <url>/2020/02/20/dl-notes9-cnn-basic/</url>
      
        <content type="html"><![CDATA[<h1 id="卷积神经网络基础"><a href="#卷积神经网络基础" class="headerlink" title="卷积神经网络基础"></a>卷积神经网络基础</h1><p>卷积本质上体现了<code>两种信号的时移累加</code>。</p><p>本节我们介绍卷积神经网络的基础概念，主要是卷积层和池化层，并解释填充、步幅、输入通道和输出通道的含义。</p><h2 id="二维卷积层"><a href="#二维卷积层" class="headerlink" title="二维卷积层"></a>二维卷积层</h2><p>本节介绍的是最常见的二维卷积层，常用于处理图像数据。</p><h3 id="二维互相关运算"><a href="#二维互相关运算" class="headerlink" title="二维互相关运算"></a>二维互相关运算</h3><p>二维互相关（cross-correlation）运算的输入是一个二维输入数组和一个二维<code>核（kernel）数组</code>，输出也是一个二维数组，其中核数组通常称为<code>卷积核</code>或<code>过滤器</code>（filter）。卷积核的尺寸通常小于输入数组，卷积核在输入数组上滑动，在每个位置上，卷积核与该位置处的输入子数组按元素相乘并求和，得到输出数组中相应位置的元素。图1展示了一个互相关运算的例子，阴影部分分别是输入的第一个计算区域、核数组以及对应的输出。</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/xTyKEpR7j6gVaHb.png" alt="二维互相关运算"></p><p>下面我们用<code>corr2d</code>函数实现二维互相关运算，它接受输入数组<code>X</code>与核数组<code>K</code>，并输出数组<code>Y</code>。</p><pre><code class="lang-python">import torch import torch.nn as nndef corr2d(X, K):    H, W = X.shape    h, w = K.shape    Y = torch.zeros(H - h + 1, W - w + 1)    for i in range(Y.shape[0]):        for j in range(Y.shape[1]):            Y[i, j] = (X[i: i + h, j: j + w] * K).sum()    return Y</code></pre><p>构造上图中的输入数组<code>X</code>、核数组<code>K</code>来验证二维互相关运算的输出。</p><pre><code class="lang-python">X = torch.tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]])K = torch.tensor([[0, 1], [2, 3]])Y = corr2d(X, K)print(Y)</code></pre><pre><code>tensor([[19., 25.],        [37., 43.]])</code></pre><h3 id="二维卷积层-1"><a href="#二维卷积层-1" class="headerlink" title="二维卷积层"></a>二维卷积层</h3><p>二维卷积层将输入和卷积核做互相关运算，并加上一个标量偏置来得到输出。卷积层的<code>模型参数</code>包括「卷积核」<code>self.weight</code>和「标量偏置」<code>self.bias</code>。</p><ul><li><p><code>nn.Parameter()</code>是 tensor 的子类，会为参数自动附上梯度，是可学习的</p></li><li><p><code>nn.Module</code>表示的模型中会维护一个参数集合，定义nn.Parameter()会自动把参数注册到 model 的参数集合中</p></li></ul><pre><code class="lang-python">class Conv2D(nn.Module):    def __init__(self, kernel_size):  # kernel_size 卷积层的高和宽        super(Conv2D, self).__init__()        self.weight = nn.Parameter(torch.randn(kernel_size))        self.bias = nn.Parameter(torch.randn(1))    def forward(self, x):        return corr2d(x, self.weight) + self.bias  # 广播机制，每个位置加了同样一个偏置</code></pre><p>下面我们看一个例子，我们构造一张$6 \times 8$的图像，中间4列为黑（0），其余为白（1），希望检测到颜色边缘。我们的标签是一个$6 \times 7$的二维数组，第2列是1（从1到0的边缘），第6列是-1（从0到1的边缘）。</p><pre><code class="lang-python">X = torch.ones(6, 8)Y = torch.zeros(6, 7)X[:, 2: 6] = 0Y[:, 1] = 1Y[:, 5] = -1print(X)print(Y)</code></pre><pre><code>tensor([[1., 1., 0., 0., 0., 0., 1., 1.],        [1., 1., 0., 0., 0., 0., 1., 1.],        [1., 1., 0., 0., 0., 0., 1., 1.],        [1., 1., 0., 0., 0., 0., 1., 1.],        [1., 1., 0., 0., 0., 0., 1., 1.],        [1., 1., 0., 0., 0., 0., 1., 1.]])tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])</code></pre><p>我们希望学习一个$1 \times 2$卷积层，通过卷积层来检测颜色边缘。（因为计算边缘考虑的是同一行的相邻两个元素，1 行 2 列）</p><pre><code class="lang-python">conv2d = Conv2D(kernel_size=(1, 2))step = 30lr = 0.01for i in range(step):    Y_hat = conv2d(X)    l = ((Y_hat - Y) ** 2).sum()    l.backward()    # 梯度下降    conv2d.weight.data -= lr * conv2d.weight.grad    conv2d.bias.data -= lr * conv2d.bias.grad    # 梯度清零    conv2d.weight.grad.zero_()    conv2d.bias.grad.zero_()    if (i + 1) % 5 == 0:        print(&#39;Step %d, loss %.3f&#39; % (i + 1, l.item()))print(conv2d.weight.data)print(conv2d.bias.data)</code></pre><pre><code>Step 5, loss 4.569Step 10, loss 0.949Step 15, loss 0.228Step 20, loss 0.060Step 25, loss 0.016Step 30, loss 0.004tensor([[ 1.0161, -1.0177]])tensor([0.0009])</code></pre><h3 id="互相关运算与卷积运算"><a href="#互相关运算与卷积运算" class="headerlink" title="互相关运算与卷积运算"></a>互相关运算与卷积运算</h3><p>卷积层得名于卷积运算，但卷积层中用到的并非卷积运算而是互相关运算。我们将核数组上下翻转、左右翻转，再与输入数组做互相关运算，这一过程就是卷积运算。由于卷积层的核数组是可学习的，所以使用互相关运算与使用卷积运算并无本质区别。</p><h3 id="特征图与感受野"><a href="#特征图与感受野" class="headerlink" title="特征图与感受野"></a>特征图与感受野</h3><p>二维卷积层<strong>输出</strong>的二维数组可以看作是输入在空间维度（宽和高）上某一级的表征，也叫<strong>「特征图」（feature map）</strong>。影响元素$x$的前向计算的所有可能<strong>输入</strong>区域（可能大于输入的实际尺寸）叫做$x$的 <strong>「感受野」（receptive field）</strong>。</p><p>以图1为例，输入中阴影部分的四个元素是输出中阴影部分元素的感受野。我们将图中形状为$2 \times 2$的输出记为$Y$，将$Y$与另一个形状为$2 \times 2$的核数组做互相关运算，输出单个元素$z$。那么，$z$在$Y$上的感受野包括$Y$的全部四个元素，在输入上的感受野包括其中全部9个元素。可见，我们可以通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征。</p><p>堆叠加深，不同层的感受野不同，<code>越深越大</code></p><h2 id="填充和步幅"><a href="#填充和步幅" class="headerlink" title="填充和步幅"></a>填充和步幅</h2><p>我们介绍卷积层的两个超参数，即填充和步幅，它们可以对给定形状的输入和卷积核改变输出形状。</p><h3 id="填充"><a href="#填充" class="headerlink" title="填充"></a>填充</h3><p>填充（padding）是指在输入高和宽的两侧填充元素（通常是0元素），图2里我们在原输入高和宽的两侧分别添加了值为0的元素。</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/qzCBiYL8nONrIds.png" alt="填充"></p><p>图2 在输入的高和宽两侧分别填充了0元素的二维互相关计算</p><p>如果原输入的高和宽是$n_h$和$n_w$，卷积核的高和宽是$k_h$和$k_w$，在高的两侧一共填充$p_h$行，在宽的两侧一共填充$p_w$列，则输出形状为：</p><script type="math/tex; mode=display">(n_h+p_h-k_h+1)\times(n_w+p_w-k_w+1)</script><p>我们在卷积神经网络中使用奇数高宽的核，比如$3 \times 3$，$5 \times 5$的卷积核，对于高度（或宽度）为大小为$2 k + 1$的核，令步幅为1，在高（或宽）两侧选择大小为$k$的填充，便可保持输入与输出尺寸相同。</p><h3 id="步幅"><a href="#步幅" class="headerlink" title="步幅"></a>步幅</h3><p>在互相关运算中，卷积核在输入数组上滑动，每次滑动的行数与列数即是步幅（stride）。此前我们使用的步幅都是1，图3展示了在高上步幅为3、在宽上步幅为2的二维互相关运算。</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/r9cVRdxynqzvjbt.png" alt="步幅"></p><p>图3 高和宽上步幅分别为3和2的二维互相关运算</p><p>一般来说，当高上步幅为$s_h$，宽上步幅为$s_w$时，输出形状为：</p><script type="math/tex; mode=display">\lfloor(n_h+p_h-k_h+s_h)/s_h\rfloor \times \lfloor(n_w+p_w-k_w+s_w)/s_w\rfloor</script><p>如果$p_h=k_h-1$，$p_w=k_w-1$，那么输出形状将简化为$\lfloor(n_h+s_h-1)/s_h\rfloor \times \lfloor(n_w+s_w-1)/s_w\rfloor$。更进一步，如果输入的高和宽能分别被高和宽上的步幅整除，那么输出形状将是$(n_h / s_h) \times (n_w/s_w)$。</p><p>当$p_h = p_w = p$时，我们称填充为$p$；当$s_h = s_w = s$时，我们称步幅为$s$。</p><h2 id="多输入通道和多输出通道"><a href="#多输入通道和多输出通道" class="headerlink" title="多输入通道和多输出通道"></a>多输入通道和多输出通道</h2><p>之前的输入和输出都是二维数组，但真实数据的维度经常更高。例如，彩色图像在高和宽2个维度外还有RGB（红、绿、蓝）3个颜色通道。假设彩色图像的高和宽分别是$h$和$w$（像素），那么它可以表示为一个$3 \times h \times w$的多维数组，我们将大小为3的这一维称为通道（channel）维。</p><h3 id="多输入通道"><a href="#多输入通道" class="headerlink" title="多输入通道"></a>多输入通道</h3><p>卷积层的输入可以包含多个通道，图4展示了一个含2个输入通道的二维互相关计算的例子。（每层的输入和该层的核相乘，最后叠加成单维度输出，输出只有一个通道）</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/NCjg1iDhPY8ItOF.png" alt="2 输入通道 1 输出通道"></p><p>图4 含2个输入通道的互相关计算</p><p>假设输入数据的通道数为$c_i$，卷积核形状为$k_h\times k_w$，我们为每个输入通道各分配一个形状为$k_h\times k_w$的核数组，将$c_i$个互相关运算的二维输出按通道相加，得到一个二维数组作为输出。我们把$c_i$个核数组在通道维上连结，即得到一个形状为$c_i\times k_h\times k_w$的卷积核。</p><h3 id="多输出通道"><a href="#多输出通道" class="headerlink" title="多输出通道"></a>多输出通道</h3><p>卷积层的输出也可以包含多个通道，设卷积核输入通道数和输出通道数分别为$c_i$和$c_o$，高和宽分别为$k_h$和$k_w$。如果希望得到含多个通道的输出，我们可以为每个输出通道分别创建形状为$c_i\times k_h\times k_w$的核数组，将它们在输出通道维上连结（有加法计算），卷积核的形状即$c_o\times c_i\times k_h\times k_w$。<br>（如下图，3 个输入通道，2 个输出通道，中间有 6 个 $1 \times 1$ 的核数组。)</p><p>对于输出通道的卷积核，我们提供这样一种理解，一个$c_i \times k_h \times k_w$的核数组可以提取某种局部特征，但是输入可能具有相当丰富的特征，我们需要有多个这样的$c_i \times k_h \times k_w$的核数组，不同的核数组提取的是不同的特征。</p><h3 id="1x1卷积层"><a href="#1x1卷积层" class="headerlink" title="1x1卷积层"></a>1x1卷积层</h3><p>最后讨论形状为$1 \times 1$的卷积核，我们通常称这样的卷积运算为$1 \times 1$卷积，称包含这种卷积核的卷积层为$1 \times 1$卷积层。图5展示了使用输入通道数为3、输出通道数为2的$1\times 1$卷积核的互相关计算。</p><p>$1\times 1$卷积层的作用与<code>全连接层</code>等价，相当于<code>矩阵乘法</code>。</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/I8MX65oJvur3b7f.png" alt="2x3=6 个 1x1 卷积层"></p><p>图5 1x1卷积核的互相关计算。输入和输出具有<code>相同的高和宽</code></p><p>$1 \times 1$卷积核可在不改变高宽的情况下，<code>调整通道数</code>。$1 \times 1$卷积核不识别高和宽维度上相邻元素构成的模式，其主要计算发生在通道维上。假设我们将通道维当作特征维，将高和宽维度上的元素当成数据样本，那么$1\times 1$卷积层的作用与全连接层等价。</p><h2 id="卷积层与全连接层的对比"><a href="#卷积层与全连接层的对比" class="headerlink" title="卷积层与全连接层的对比"></a>卷积层与全连接层的对比</h2><p>二维卷积层经常用于处理图像，与此前的全连接层相比，它主要有两个优势：</p><ul><li><p>一是全连接层把图像<code>展平</code>成一个向量，在输入图像上相邻的元素可能因为展平操作不再相邻，网络难以捕捉局部信息。而卷积层的设计，天然地具有提取局部信息的能力。</p></li><li><p>二是卷积层的参数量更少。不考虑偏置的情况下，一个形状为$(c_i, c_o, h, w)$的卷积核的参数量是$c_i \times c_o \times h \times w$，<code>与输入图像的宽高无关</code>。假如一个卷积层的输入和输出形状分别是$(c_1, h_1, w_1)$和$(c_2, h_2, w_2)$，如果要用全连接层进行连接，参数数量就是$c_1 \times c_2 \times h_1 \times w_1 \times h_2 \times w_2$。使用卷积层可以以较少的参数数量来处理更大的图像。</p></li></ul><h2 id="卷积层的简洁实现"><a href="#卷积层的简洁实现" class="headerlink" title="卷积层的简洁实现"></a>卷积层的简洁实现</h2><p>我们使用Pytorch中的<code>nn.Conv2d</code>类来实现二维卷积层，主要关注以下几个构造函数参数：</p><ul><li><code>in_channels</code> (python:int) – Number of channels in the input imag  输入通道数</li><li><code>out_channels</code> (python:int) – Number of channels produced by the convolution 输出通道数</li><li><code>kernel_size</code> (python:int or tuple) – Size of the convolving kernel 卷积核大小，int 表示高和宽相等，元组表示不相等</li><li><code>stride</code> (python:int or tuple, optional) – Stride of the convolution. Default: 1</li><li><code>padding</code> (python:int or tuple, optional) – Zero-padding added to both sides of the input. Default: 0</li><li><code>bias</code> (bool, optional) – If True, adds a learnable bias to the output. Default: True</li></ul><p><code>forward</code>函数的参数为一个四维张量，形状为$(N批量大小, C_{in}通道数， H_{in}高度, W_{in}宽度)$，返回值也是一个四维张量，形状为$(N, C_{out}, H_{out}, W_{out})$</p><p>kernel_size（即卷积核大小）这是需要人为设定的参数，该参数是不需要学习的，当然大小不同，卷积结果也是不同的。经过大量实验表明，大多选用<code>1x1、3x3、5x5</code>等尺寸较小且长宽为奇数的卷积核。</p><p>代码讲解</p><pre><code class="lang-python">X = torch.rand(4, 2, 3, 5)print(X.shape)conv2d = nn.Conv2d(in_channels=2, out_channels=3, kernel_size=(3, 5), stride=1, padding=(1, 2))Y = conv2d(X)print(&#39;Y.shape: &#39;, Y.shape)print(&#39;weight.shape: &#39;, conv2d.weight.shape)  # padding 和 size 刚好是 2k+1 的关系，所以 in 和 out 宽高不变print(&#39;bias.shape: &#39;, conv2d.bias.shape) # Y.shape:  torch.Size([4, 2, 3, 5])# Y.shape:  torch.Size([4, 3, 3, 5])# weight.shape:  torch.Size([3, 2, 3, 5])# bias.shape:  torch.Size([3])</code></pre><h2 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h2><h3 id="二维池化层"><a href="#二维池化层" class="headerlink" title="二维池化层"></a>二维池化层</h3><p>池化层主要用于缓解卷积层对位置的过度敏感性。同卷积层一样，池化层每次对输入数据的一个固定形状窗口（又称池化窗口）中的元素计算输出，池化层直接计算池化窗口内元素的最大值或者平均值，该运算也分别叫做<code>最大池化</code>或<code>平均池化</code>。图6展示了池化窗口形状为$2\times 2$的最大池化。</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/CpF4XrYOE3Z2qKc.png" alt="2x2 的最大池化"></p><p>二维平均池化的工作原理与二维最大池化类似，但将最大运算符替换成平均运算符。池化窗口形状为$p \times q$的池化层称为$p \times q$池化层，其中的池化运算叫作$p \times q$池化。</p><p>池化层也可以在输入的高和宽两侧填充并调整窗口的移动步幅来改变输出形状。池化层填充和步幅与卷积层填充和步幅的工作机制一样。</p><p>在处理多通道输入数据时，池化层对每个输入通道分别池化，但不会像卷积层那样将各通道的结果按通道相加。这意味着池化层的<code>输出通道数与输入通道数相等</code>。</p><ul><li><code>卷积与池化的区别</code>：<br>  池化层没有需要学习的特征<br>  卷积层用来学习识别某一种特征<br>  池化层的主要作用是降维<br>  卷积层当设置步长 &gt;1 时也可以用来完成降维的工作</li></ul><h3 id="池化层的简洁实现"><a href="#池化层的简洁实现" class="headerlink" title="池化层的简洁实现"></a>池化层的简洁实现</h3><p>我们使用Pytorch中的<code>nn.MaxPool2d</code>实现最大池化层，关注以下构造函数参数：</p><ul><li><code>kernel_size</code> – the size of the window to take a max over</li><li><code>stride</code> – the stride of the window. Default value is kernel_size</li><li><code>padding</code> – implicit zero padding to be added on both sides</li></ul><p><code>forward</code>函数的参数为一个四维张量，形状为$(N, C, H_{in}, W_{in})$，返回值也是一个四维张量，形状为$(N, C, H_{out}, W_{out})$，其中$N$是批量大小，$C, H, W$分别表示通道数、高度、宽度。</p><p>代码讲解</p><pre><code class="lang-python">X = torch.arange(32, dtype=torch.float32).view(1, 2, 4, 4)pool2d = nn.MaxPool2d(kernel_size=3, padding=1, stride=(2, 1))Y = pool2d(X)print(X)print(Y)</code></pre><pre><code>tensor([[[[ 0.,  1.,  2.,  3.],          [ 4.,  5.,  6.,  7.],          [ 8.,  9., 10., 11.],          [12., 13., 14., 15.]],         [[16., 17., 18., 19.],          [20., 21., 22., 23.],          [24., 25., 26., 27.],          [28., 29., 30., 31.]]]])tensor([[[[ 5.,  6.,  7.,  7.],          [13., 14., 15., 15.]],         [[21., 22., 23., 23.],          [29., 30., 31., 31.]]]])</code></pre><p>平均池化层使用的是<code>nn.AvgPool2d</code>，使用方法与<code>nn.MaxPool2d</code>相同。</p><h2 id="练习题"><a href="#练习题" class="headerlink" title="练习题"></a>练习题</h2><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/ktF7bxGVugQzO5U.png" alt=""></p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/FYjpTINr4ViwZ39.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> CV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动手深度学习(八):机器翻译简介</title>
      <link href="/2020/02/20/dl-notes8-mach-trans/"/>
      <url>/2020/02/20/dl-notes8-mach-trans/</url>
      
        <content type="html"><![CDATA[<h1 id="机器翻译和数据集"><a href="#机器翻译和数据集" class="headerlink" title="机器翻译和数据集"></a>机器翻译和数据集</h1><p>机器翻译（MT）：将一段文本从一种语言自动翻译为另一种语言，用神经网络解决这个问题通常称为神经机器翻译（NMT）。<br>主要特征：输出是单词<code>序列</code>而不是单个单词。 输出序列的长度可能与源序列的长度<code>不同</code>。</p><h2 id="机器翻译任务代码总结如下"><a href="#机器翻译任务代码总结如下" class="headerlink" title="机器翻译任务代码总结如下"></a>机器翻译任务代码总结如下</h2><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><ol><li><code>读取数据</code>，处理数据中的编码问题，并将无效的字符串删除</li><li><code>分词</code>，分词的目的就是将字符串转换成单词组成的列表。目前有很多现成的分词工具可以直接使用，也可以直接按照空格进行分词(不推荐，因为分词不是很准确)</li><li><code>建立词典</code>，将单词组成的列表编程单词id组成的列表，这里会得到如下几样东西<ol><li>去重后词典，及其中单词对应的索引列表</li><li>还可以得到给定索引找到其对应的单词的列表，以及给定单词得到对应索引的字典</li><li>原始语料所有词对应的词典索引的列表</li></ol></li><li>对数据进行<code>padding操作</code>。基于rnn的Seq2Seq模型，可以处理任意长度，“变长数据的读入”基于rnn的机器翻译也并不需要固定长度。要<code>padding</code>，是因为像tf、pytorch这些框架要求一个batch的数据必须长度相等，不然会报错；要<code>截断</code>，设置最大的数据长度是因为decode的时候达到这个长度我们就停止；再一个原因就是为了加快计算，不然的话为了单个特别长的数据，batch中的其他数据都补成这么长，很慢。</li><li><code>制作数据生成器</code>，但是需要注意的是对于翻译任务的数据格式，机器翻译的输入是一段文本序列，输出也是一段文本序列。</li></ol><h3 id="Seq2Seq模型的构建"><a href="#Seq2Seq模型的构建" class="headerlink" title="Seq2Seq模型的构建"></a>Seq2Seq模型的构建</h3><ol><li>先编码后解码的框架: 先对输入序列使用循环神经网络对他进行编码，编码成一个<code>向量</code>之后，再将编码得到的向量作为一个新的<code>解码循环神经网络</code>的隐藏状态的输入，进行解码，一次输出一个序列的元素，再将模型训练输出的序列元素与真实标签计算损失进行学习。</li><li>词嵌入: 一般情况下输入到编码网络中的数据不是一个<code>onehot</code>向量而是经过了编码之后的向量，比如由<code>word2vec</code>技术，让编码后的向量由更加丰富的含义。</li><li>在进行编码和解码的过程中数据都是以<code>时间步</code>展开，也就是(Seq_len,)这种形式的数据进行处理的</li><li>对于编码与解码的循环神经网络，可以通过控制隐藏层的<code>层数</code>及每一层隐藏层神经元的<code>数量</code>来控制模型的复杂度</li><li>编码部分，RNN的用0初始化隐含状态，最后的输出主要是隐藏状态, 编码RNN输出的隐含状态认为是其对应的编码向量</li><li>解码器的整体形状与编码器是一样的，只不过解码器的模型的隐藏状态是<code>由编码器的输出的隐藏状态初始化</code>的。</li></ol><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><ol><li>解码器的输出是一个和词典维度相同的向量，其每个值对应与向量索引位置对应词的分数，一般是选择<code>分数最大</code>的那个词作为最终的输出。</li><li>在计算损失函数之前，要把<code>padding去掉</code>，因为padding的部分不参与计算</li></ol><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><ol><li>解码器在测试的时候需要将模型的输出作为下一个时间步的输入</li><li>Beam Search搜索算法。<ol><li>假设预测的时候词典的大小为3，内容为a,b,c. <code>beam size</code>为2，解码的时候过程如下</li><li>生成第一个词的时候，选择概率最大的两个词，假设为a,c.那么当前的两个序列就是a和c。</li><li>生成第二个词的时候，将当前序列a和c，分别与此表中的所有词进行组合，得到新的6个序列aa ab ac ca cb cc,计算每个序列的得分，并选择得分最高的2个序列，作为新的当前序列，假如为aa cb </li><li>后面不断重复这个过程，直到遇到结束符或者达到最大长度为止，最终输出得分最高的2个序列。</li></ol></li></ol><pre><code class="lang-python">import osos.listdir(&#39;/home/kesci/input/&#39;)</code></pre><pre><code>[&#39;fraeng6506&#39;, &#39;d2l9528&#39;, &#39;d2l6239&#39;]</code></pre><pre><code class="lang-python">import syssys.path.append(&#39;/home/kesci/input/d2l9528/&#39;)import collectionsimport d2limport zipfilefrom d2l.data.base import Vocabimport timeimport torchimport torch.nn as nnimport torch.nn.functional as Ffrom torch.utils import datafrom torch import optim</code></pre><h3 id="数据预处理-1"><a href="#数据预处理-1" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>将数据集清洗、转化为神经网络的输入minbatch</p><pre><code class="lang-python">with open(&#39;/home/kesci/input/fraeng6506/fra.txt&#39;, &#39;r&#39;) as f:      raw_text = f.read()print(raw_text[0:1000])</code></pre><pre><code>Go.    Va !    CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) &amp; #1158250 (Wittydev)Hi.    Salut !    CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) &amp; #509819 (Aiji)Hi.    Salut.    CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) &amp; #4320462 (gillux)Run!    Cours !    CC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) &amp; #906331 (sacredceltic)Run!    Courez !    CC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) &amp; #906332 (sacredceltic)Who?    Qui ?    CC-BY 2.0 (France) Attribution: tatoeba.org #2083030 (CK) &amp; #4366796 (gillux)Wow!    Ça alors !    CC-BY 2.0 (France) Attribution: tatoeba.org #52027 (Zifre) &amp; #374631 (zmoo)Fire!    Au feu !    CC-BY 2.0 (France) Attribution: tatoeba.org #1829639 (Spamster) &amp; #4627939 (sacredceltic)Help!    À l&#39;aide !    CC-BY 2.0 (France) Attribution: tatoeba.org #435084 (lukaszpp) &amp; #128430 (sysko)Jump.    Saute.    CC-BY 2.0 (France) Attribution: tatoeba.org #631038 (Shishir) &amp; #2416938 (Phoenix)Stop!    Ça suffit !    CC-BY 2.0 (France) Attribution: tato</code></pre><pre><code class="lang-python">def preprocess_raw(text):    text = text.replace(&#39;\u202f&#39;, &#39; &#39;).replace(&#39;\xa0&#39;, &#39; &#39;)    out = &#39;&#39;    for i, char in enumerate(text.lower()):        if char in (&#39;,&#39;, &#39;!&#39;, &#39;.&#39;) and i &gt; 0 and text[i-1] != &#39; &#39;:            out += &#39; &#39;        out += char    return outtext = preprocess_raw(raw_text)print(text[0:1000])</code></pre><pre><code>go .    va !    cc-by 2 .0 (france) attribution: tatoeba .org #2877272 (cm) &amp; #1158250 (wittydev)hi .    salut !    cc-by 2 .0 (france) attribution: tatoeba .org #538123 (cm) &amp; #509819 (aiji)hi .    salut .    cc-by 2 .0 (france) attribution: tatoeba .org #538123 (cm) &amp; #4320462 (gillux)run !    cours !    cc-by 2 .0 (france) attribution: tatoeba .org #906328 (papabear) &amp; #906331 (sacredceltic)run !    courez !    cc-by 2 .0 (france) attribution: tatoeba .org #906328 (papabear) &amp; #906332 (sacredceltic)who?    qui ?    cc-by 2 .0 (france) attribution: tatoeba .org #2083030 (ck) &amp; #4366796 (gillux)wow !    ça alors !    cc-by 2 .0 (france) attribution: tatoeba .org #52027 (zifre) &amp; #374631 (zmoo)fire !    au feu !    cc-by 2 .0 (france) attribution: tatoeba .org #1829639 (spamster) &amp; #4627939 (sacredceltic)help !    à l&#39;aide !    cc-by 2 .0 (france) attribution: tatoeba .org #435084 (lukaszpp) &amp; #128430 (sysko)jump .    saute .    cc-by 2 .0 (france) attribution: tatoeba .org #631038 (shishir) &amp; #2416938 (phoenix)stop !    ça suffit !    cc-b</code></pre><p>字符在计算机里是以编码的形式存在，我们通常所用的空格是 \x20 ，是在标准ASCII可见字符 0x20~0x7e 范围内。<br>而 \xa0 属于 latin1 （ISO/IEC_8859-1）中的扩展字符集字符，代表不间断空白符nbsp(non-breaking space)，超出gbk编码范围，是需要去除的特殊字符。再数据预处理的过程中，我们首先需要对数据进行清洗。</p><h3 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h3><p>字符串—-单词组成的列表</p><pre><code class="lang-python">num_examples = 50000source, target = [], []for i, line in enumerate(text.split(&#39;\n&#39;)):    if i &gt; num_examples:        break    parts = line.split(&#39;\t&#39;)    if len(parts) &gt;= 2:        source.append(parts[0].split(&#39; &#39;))        target.append(parts[1].split(&#39; &#39;))source[0:3], target[0:3]</code></pre><pre><code>([[&#39;go&#39;, &#39;.&#39;], [&#39;hi&#39;, &#39;.&#39;], [&#39;hi&#39;, &#39;.&#39;]], [[&#39;va&#39;, &#39;!&#39;], [&#39;salut&#39;, &#39;!&#39;], [&#39;salut&#39;, &#39;.&#39;]])</code></pre><pre><code class="lang-python">d2l.set_figsize()d2l.plt.hist([[len(l) for l in source], [len(l) for l in target]],label=[&#39;source&#39;, &#39;target&#39;])d2l.plt.legend(loc=&#39;upper right&#39;);</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/t19DZi4dB2XOPEn.png" alt="分词结果"></p><h3 id="建立词典"><a href="#建立词典" class="headerlink" title="建立词典"></a>建立词典</h3><p>单词组成的列表—-单词id组成的列表</p><pre><code class="lang-python">def build_vocab(tokens):    tokens = [token for line in tokens for token in line]    return d2l.data.base.Vocab(tokens, min_freq=3, use_special_tokens=True)src_vocab = build_vocab(source)  # 4 个特殊字符，pad 补充 bos句开始 eos句结束 unk未知字符len(src_vocab)</code></pre><pre><code>3789</code></pre><p><code>Vocab</code> 会进行词频统计, 从大到小排序<br><code>__len__</code> 返回长度<br><code>__getitem__</code> 返回单词对应的 id<br><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/3oBvEZdHYKXzwfg.png" alt="Vocab"></p><h3 id="载入数据集"><a href="#载入数据集" class="headerlink" title="载入数据集"></a>载入数据集</h3><p>进行 pad 保证句子长度是一样的, 一个 batch 使用的同一个长度</p><pre><code class="lang-python">def pad(line, max_len, padding_token):    if len(line) &gt; max_len:        return line[:max_len]    return line + [padding_token] * (max_len - len(line))pad(src_vocab[source[0]], 10, src_vocab.pad)</code></pre><pre><code>[38, 4, 0, 0, 0, 0, 0, 0, 0, 0]</code></pre><p>首尾标注 <code>bos</code> 和 <code>eos</code></p><pre><code class="lang-python">def build_array(lines, vocab, max_len, is_source):    lines = [vocab[line] for line in lines]    if not is_source:  # 如果是目标数据集，就要加一对首尾标注        lines = [[vocab.bos] + line + [vocab.eos] for line in lines]    array = torch.tensor([pad(line, max_len, vocab.pad) for line in lines])    valid_len = (array != vocab.pad).sum(1) #第一个维度    return array, valid_len</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/LW4YMRvUpCA9ul6.png" alt="TensorDataset"></p><pre><code class="lang-python">def load_data_nmt(batch_size, max_len): # This function is saved in d2l.    src_vocab, tgt_vocab = build_vocab(source), build_vocab(target)  # 先进行分词    src_array, src_valid_len = build_array(source, src_vocab, max_len, True)  # 构建源语言数组    tgt_array, tgt_valid_len = build_array(target, tgt_vocab, max_len, False) # 构建目标语言数组    train_data = data.TensorDataset(src_array, src_valid_len, tgt_array, tgt_valid_len)     # TensorDataset 利用 assert 判断 4 个参数是不是一一对应的    train_iter = data.DataLoader(train_data, batch_size, shuffle=True)    return src_vocab, tgt_vocab, train_iter</code></pre><p>X 是英语句子, Y 是法语句子</p><pre><code class="lang-python">src_vocab, tgt_vocab, train_iter = load_data_nmt(batch_size=2, max_len=8)for X, X_valid_len, Y, Y_valid_len, in train_iter:    print(&#39;X =&#39;, X.type(torch.int32), &#39;\nValid lengths for X =&#39;, X_valid_len,        &#39;\nY =&#39;, Y.type(torch.int32), &#39;\nValid lengths for Y =&#39;, Y_valid_len)    break</code></pre><pre><code>X = tensor([[   5,   24,    3,    4,    0,    0,    0,    0],        [  12, 1388,    7,    3,    4,    0,    0,    0]], dtype=torch.int32) Valid lengths for X = tensor([4, 5]) Y = tensor([[   1,   23,   46,    3,    3,    4,    2,    0],        [   1,   15,  137,   27, 4736,    4,    2,    0]], dtype=torch.int32) Valid lengths for Y = tensor([7, 7])</code></pre><h1 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a>Encoder-Decoder</h1><p> encoder：输入到隐藏状态<br> decoder：隐藏状态到输出</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/7aEyZcGDVS1zXPU.png" alt="Encoder-Decoder"></p><pre><code class="lang-python">class Encoder(nn.Module):    def __init__(self, **kwargs):        super(Encoder, self).__init__(**kwargs)    def forward(self, X, *args):        raise NotImplementedError</code></pre><pre><code class="lang-python">class Decoder(nn.Module):    def __init__(self, **kwargs):        super(Decoder, self).__init__(**kwargs)    def init_state(self, enc_outputs, *args):        raise NotImplementedError    def forward(self, X, state):        raise NotImplementedError</code></pre><pre><code class="lang-python">class EncoderDecoder(nn.Module):    def __init__(self, encoder, decoder, **kwargs):        super(EncoderDecoder, self).__init__(**kwargs)        self.encoder = encoder        self.decoder = decoder    def forward(self, enc_X, dec_X, *args):        enc_outputs = self.encoder(enc_X, *args)        dec_state = self.decoder.init_state(enc_outputs, *args)        return self.decoder(dec_X, dec_state)</code></pre><p>可以应用在对话系统、生成式任务中。</p><h1 id="Sequence-to-Sequence模型"><a href="#Sequence-to-Sequence模型" class="headerlink" title="Sequence to Sequence模型"></a>Sequence to Sequence模型</h1><h3 id="模型："><a href="#模型：" class="headerlink" title="模型："></a>模型：</h3><ul><li>训练（已知英语所对应的法语）</li><li>Encoder 是一个 RNN，可以是 LSTM 可以是 GRU</li><li>hidden state 就是 Encoder 得到的 ht，就是语义编码，也就是 Decoder 用来初始化的 $H_{-1}$</li><li>Decoder 就是一个生成语言模型，有了前几个生成后几个，类似于歌词生成<br><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/JL3VW7gznsGIStw.png" alt="生成"></li><li>预测（不知道对应的法语）</li></ul><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/YhcpTl6nQERLosu.png" alt="预测"></p><h3 id="具体结构："><a href="#具体结构：" class="headerlink" title="具体结构："></a>具体结构：</h3><ul><li>比如 apple 对应的 id，会把它翻译成<code>词向量</code>，代表 apple 的含义</li><li><code>dense()</code>多用于CNN网络搭建时搭建全连接层使用，在RNN网络搭建，特别是涉及到用RNN做分类时，dense()可以用作RNN<code>分类预测输出</code>, 每个输出也是一个分类问题</li><li><code>embedding层</code>的作用就是给每个单词，赋一个特定的词向量<br><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/5otbRSFCVqhZDUK.png" alt="具体结构"><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3></li></ul><pre><code class="lang-python">class Seq2SeqEncoder(d2l.Encoder):    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,                 dropout=0, **kwargs):        super(Seq2SeqEncoder, self).__init__(**kwargs)        self.num_hiddens=num_hiddens        self.num_layers=num_layers        self.embedding = nn.Embedding(vocab_size, embed_size) # embed_size 词向量维度，输入单词的形状        self.rnn = nn.LSTM(embed_size,num_hiddens, num_layers, dropout=dropout)    def begin_state(self, batch_size, device):  # 初始化        return [torch.zeros(size=(self.num_layers, batch_size, self.num_hiddens),  device=device),                torch.zeros(size=(self.num_layers, batch_size, self.num_hiddens),  device=device)]    def forward(self, X, *args):        X = self.embedding(X) # X shape: (batch_size, seq_len, embed_size)         # batch_size 有几句话，seq_len 一句话有几个单词，每个单词都变成了 embed_size 维的词向量        X = X.transpose(0, 1)  # RNN 第一个维度应该是时间（句子的顺序是第一个维度），所以把第 1、2 维度调换一下        # state = self.begin_state(X.shape[1], device=X.device)        out, state = self.rnn(X) # out 是每个单元的输出，state 是最后一个单元的输出        # out 的 shape 是 (seq_len, batch_size, num_hiddens).        # state 包含最后一个记忆细胞的状态，和隐层状态        # 在最后一步， state shape 是 (num_layers, batch_size, num_hiddens)        return out, state</code></pre><pre><code class="lang-python">encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8,num_hiddens=16, num_layers=2) #每个词用 8 维向量表示X = torch.zeros((4, 7),dtype=torch.long)  # 输入 4 句话，每句话 7 个单词output, state = encoder(X)output.shape, len(state), state[0].shape, state[1].shape  # len(state)=2 表示包含了记忆细胞和隐层状态两个部分</code></pre><pre><code>(torch.Size([7, 4, 16]), 2, torch.Size([2, 4, 16]), torch.Size([2, 4, 16]))</code></pre><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><pre><code class="lang-python">class Seq2SeqDecoder(d2l.Decoder):    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,                 dropout=0, **kwargs):        super(Seq2SeqDecoder, self).__init__(**kwargs)        self.embedding = nn.Embedding(vocab_size, embed_size)        self.rnn = nn.LSTM(embed_size,num_hiddens, num_layers, dropout=dropout)        self.dense = nn.Linear(num_hiddens,vocab_size) # dense 用于输出结果        # 通过全连接层把16个隐藏单元映射到100个单词，在100 维度的向量里，选出评分最高的，作为结果输出    def init_state(self, enc_outputs, *args):        return enc_outputs[1]  # 用 Encoder 的输出作初始化    def forward(self, X, state):        X = self.embedding(X).transpose(0, 1)        out, state = self.rnn(X, state)        # Make the batch to be the first dimension to simplify loss computation.        out = self.dense(out).transpose(0, 1)        return out, state</code></pre><pre><code class="lang-python">decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8,num_hiddens=16, num_layers=2)state = decoder.init_state(encoder(X))out, state = decoder(X, state)out.shape, len(state), state[0].shape, state[1].shape  # out 4*7*10 10 代表单词表大小，在 10 个得分里选最高的</code></pre><pre><code>(torch.Size([4, 7, 10]), 2, torch.Size([2, 4, 16]), torch.Size([2, 4, 16]))</code></pre><h3 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h3><p>计算有效长度, 忽略 padding</p><pre><code class="lang-python">def SequenceMask(X, X_len,value=0):      # X 一个 batch 的输入，X_len 输入原本的有效长度（由于有 pad）    maxlen = X.size(1)    mask = torch.arange(maxlen)[None, :].to(X_len.device) &lt; X_len[:, None] # len 以后的部分都用 value 填充    X[~mask]=value    return X</code></pre><pre><code class="lang-python">X = torch.tensor([[1,2,3], [4,5,6]])SequenceMask(X,torch.tensor([1,2]))  # 第一句有效长度为 1，第二句有效长度为 2</code></pre><pre><code>tensor([[1, 0, 0],        [4, 5, 0]])</code></pre><pre><code class="lang-python">X = torch.ones((2,3, 4))SequenceMask(X, torch.tensor([1,2]),value=-1)</code></pre><pre><code>tensor([[[ 1.,  1.,  1.,  1.],         [-1., -1., -1., -1.],         [-1., -1., -1., -1.]],        [[ 1.,  1.,  1.,  1.],         [ 1.,  1.,  1.,  1.],         [-1., -1., -1., -1.]]])</code></pre><p>定义交叉熵损失函数, 继承<code>CrossEntropyLoss</code></p><pre><code class="lang-python">class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):    # pred shape: (batch_size, seq_len, vocab_size)    # label shape: (batch_size, seq_len)    # valid_length shape: (batch_size, )    def forward(self, pred, label, valid_length):        # the sample weights shape should be (batch_size, seq_len)        weights = torch.ones_like(label)        weights = SequenceMask(weights, valid_length).float()  # padding 部分的损失变成 0，保留有效长度部分的存世        self.reduction=&#39;none&#39;        output=super(MaskedSoftmaxCELoss, self).forward(pred.transpose(1,2), label)        return (output*weights).mean(dim=1)</code></pre><pre><code class="lang-python">loss = MaskedSoftmaxCELoss()loss(torch.ones((3, 4, 10)), torch.ones((3,4),dtype=torch.long), torch.tensor([4,3,0])) # label 3个句子，4 个正确单词； 有效长度[4, 3, 0]</code></pre><pre><code>tensor([2.3026, 1.7269, 0.0000])</code></pre><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><pre><code class="lang-python">def train_ch7(model, data_iter, lr, num_epochs, device):  # Saved in d2l 如果device是GPU，下面的device都要设定    model.to(device)    optimizer = optim.Adam(model.parameters(), lr=lr)    loss = MaskedSoftmaxCELoss()    tic = time.time()    for epoch in range(1, num_epochs+1):        l_sum, num_tokens_sum = 0.0, 0.0  # loss 的总和，单词数量的总和        for batch in data_iter:            optimizer.zero_grad()            X, X_vlen, Y, Y_vlen = [x.to(device) for x in batch]            Y_input, Y_label, Y_vlen = Y[:,:-1], Y[:,1:], Y_vlen-1 # Y_input decoder的输入            Y_hat, _ = model(X, Y_input, X_vlen, Y_vlen)            l = loss(Y_hat, Y_label, Y_vlen).sum()            l.backward()            with torch.no_grad():                d2l.grad_clipping_nn(model, 5, device)  #梯度裁剪            num_tokens = Y_vlen.sum().item()            optimizer.step()            l_sum += l.sum().item()            num_tokens_sum += num_tokens        if epoch % 50 == 0:            print(&quot;epoch {0:4d},loss {1:.3f}, time {2:.1f} sec&quot;.format(                   epoch, (l_sum/num_tokens_sum), time.time()-tic))            tic = time.time()</code></pre><pre><code class="lang-python">embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.0batch_size, num_examples, max_len = 64, 1e3, 10lr, num_epochs, ctx = 0.005, 300, d2l.try_gpu()src_vocab, tgt_vocab, train_iter = d2l.load_data_nmt(    batch_size, max_len,num_examples)encoder = Seq2SeqEncoder(    len(src_vocab), embed_size, num_hiddens, num_layers, dropout)decoder = Seq2SeqDecoder(    len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)model = d2l.EncoderDecoder(encoder, decoder)train_ch7(model, train_iter, lr, num_epochs, ctx)</code></pre><pre><code>epoch   50,loss 0.093, time 38.2 secepoch  100,loss 0.046, time 37.9 secepoch  150,loss 0.032, time 36.8 secepoch  200,loss 0.027, time 37.5 secepoch  250,loss 0.026, time 37.8 secepoch  300,loss 0.025, time 37.3 sec</code></pre><h3 id="测试-1"><a href="#测试-1" class="headerlink" title="测试"></a>测试</h3><pre><code class="lang-python">def translate_ch7(model, src_sentence, src_vocab, tgt_vocab, max_len, device):    src_tokens = src_vocab[src_sentence.lower().split(&#39; &#39;)]    src_len = len(src_tokens)    if src_len &lt; max_len:        src_tokens += [src_vocab.pad] * (max_len - src_len)  # 进行 padding    enc_X = torch.tensor(src_tokens, device=device)    enc_valid_length = torch.tensor([src_len], device=device)    # use expand_dim to add the batch_size dimension.    enc_outputs = model.encoder(enc_X.unsqueeze(dim=0), enc_valid_length) # unsqueeze 增加一个维度（因为有多个 batch_size)    dec_state = model.decoder.init_state(enc_outputs, enc_valid_length) # encoder 的输出作为 dec 的输入初始化    dec_X = torch.tensor([tgt_vocab.bos], device=device).unsqueeze(dim=0) # 设为 bos 表示开始    predict_tokens = []    for _ in range(max_len):        Y, dec_state = model.decoder(dec_X, dec_state) # 当前 RNN 单元的 output Y，给下一个单元的 dec_state        dec_X = Y.argmax(dim=2)  # 得分最高的单词，作为下一个单元的输入        py = dec_X.squeeze(dim=0).int().item()        if py == tgt_vocab.eos:  # 结束则跳出循环            break        predict_tokens.append(py)    return &#39; &#39;.join(tgt_vocab.to_tokens(predict_tokens))</code></pre><pre><code class="lang-python">for sentence in [&#39;Go .&#39;, &#39;Wow !&#39;, &quot;I&#39;m OK .&quot;, &#39;I won !&#39;]:    print(sentence + &#39; =&gt; &#39; + translate_ch7(        model, sentence, src_vocab, tgt_vocab, max_len, ctx))</code></pre><pre><code>Go . =&gt; va !Wow ! =&gt; &lt;unk&gt; !I&#39;m OK . =&gt; ça va .I won ! =&gt; j&#39;ai gagné !</code></pre><h2 id="Beam-Search-集束搜索"><a href="#Beam-Search-集束搜索" class="headerlink" title="Beam Search 集束搜索"></a>Beam Search 集束搜索</h2><p>简单greedy search：（贪心）只考虑了局部最优解</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/emJlxcIbrCVTS1o.png" alt="简单贪心"></p><p>维特比算法：选择整体分数最高的句子（搜索空间太大）<br>集束搜索：每次不只选一个，选择 top n，增加备选项但又不全局搜索</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/L5aI1irfAKm2znq.png" alt="集束搜索"></p><h2 id="练习题"><a href="#练习题" class="headerlink" title="练习题"></a>练习题</h2><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/BAHTqRNlPsiMdyr.png" alt="机器翻译-练习题"></p><ul><li>训练时下一个单词是已知的, 预测时需要作为下一个单词生成的输入</li><li>每个 batch 之内, size 是给定的, 句子长度都是一样的</li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动手深度学习(七):循环神经网络 RNN 基础</title>
      <link href="/2020/02/19/dl-notes7-rnn-basic/"/>
      <url>/2020/02/19/dl-notes7-rnn-basic/</url>
      
        <content type="html"><![CDATA[<h1 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h1><p>本节介绍循环神经网络，下图展示了如何基于循环神经网络实现语言模型。我们的目的是基于当前的输入与过去的输入序列，预测序列的下一个字符。循环神经网络引入一个隐藏变量$H$，用$H_{t}$表示$H$在时间步$t$的值。$H_{t}$的计算基于$X_{t}$和$H_{t-1}$，可以认为$H_{t}$记录了到当前字符为止的序列信息，利用$H_{t}$对序列的下一个字符进行预测。</p><h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><p>W_xh: 状态-输入权重<br>W_hh: 状态-状态权重<br>W_hq: 状态-输出权重<br>b_h: 隐藏层的偏置<br>b_q: 输出层的偏置</p><h3 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h3><ul><li>循环神经网络 <code>RNN</code> 的参数就是上述的三个权重和两个偏置，并且在沿着时间训练（参数的更新），参数的数量没有发生变化，仅仅是上述的参数的值在更新。循环神经网络可以看作是沿着时间维度上的权值共享</li><li>卷积神经网络 <code>CNN</code> 中，一个卷积核通过在特征图上滑动进行卷积，是空间维度的权值共享。在卷积神经网络中通过控制特征图的数量来控制每一层模型的复杂度，而循环神经网络是通过控制W_xh和W_hh中h的维度来控制模型的复杂度。</li></ul><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://s2.ax1x.com/2020/02/19/3VcEfH.png" alt="RNN 模型"></p><h2 id="一个batch的数据的表示"><a href="#一个batch的数据的表示" class="headerlink" title="一个batch的数据的表示"></a>一个batch的数据的表示</h2><ul><li>如何将一个batch的数据转换成时间步数个（批量大小，词典大小）的矩阵？</li><li>每个字符都是一个词典大小的向量(<code>one-hot 表示法</code>)，每个样本是时间步数个序列，每个batch是批量大小个样本</li><li>第一个{批量大小，词典大小}的矩阵：取出一个批量样本中<code>每个序列的第一个字符</code>，并将每个字符展开成词典大小的向量，就形成了第一个时间步所表示的矩阵</li><li>第二个{批量大小，词典大小}的矩阵：取出一个批量样本中<code>每个序列的第二个字符</code>，并将每个字符展开成词典大小的向量，就形成了第二个时间步所表示的矩阵</li><li>最后就形成了时间步个{批量大小，词典大小}的矩阵. 1个 batch = <code>step * batch_size * vocab_size</code> ，这也就是每个batch最后的形式</li></ul><h2 id="隐藏状态的初始化"><a href="#隐藏状态的初始化" class="headerlink" title="隐藏状态的初始化"></a>隐藏状态的初始化</h2><ul><li>随机采样时：每次迭代都需要重新初始化隐藏状态（每个epoch有很多词迭代，每次迭代都需要进行初始化，因为对于随机采样的样本中只有一个批量内的数据是连续的）</li><li>相邻采样时：如果是相邻采样，则说明前后两个batch的数据是连续的，所以在训练每个batch的时候只需要更新一次（也就是说模型在一个epoch中的迭代不需要重新初始化隐藏状态）</li></ul><h2 id="循环神经网络的构造"><a href="#循环神经网络的构造" class="headerlink" title="循环神经网络的构造"></a>循环神经网络的构造</h2><p>我们先看循环神经网络的具体构造。假设$\boldsymbol{X}_t \in \mathbb{R}^{n \times d}$是时间步$t$的小批量输入（其批量大小为 𝑛 ，输入个数为 𝑑），$\boldsymbol{H}_t  \in \mathbb{R}^{n \times h}$是该时间步的隐藏变量，则：</p><script type="math/tex; mode=display">\boldsymbol{H}_t = \phi(\boldsymbol{X}_t \boldsymbol{W}_{xh} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{hh}  + \boldsymbol{b}_h).</script><p>其中，$\boldsymbol{W}_{xh} \in \mathbb{R}^{d \times h}$，$\boldsymbol{W}_{hh} \in \mathbb{R}^{h \times h}$，$\boldsymbol{b}_{h} \in \mathbb{R}^{1 \times h}$，$\phi$函数是<code>非线性激活函数</code>。由于引入了$\boldsymbol{H}_{t-1} \boldsymbol{W}_{hh}$，$H_{t}$能够捕捉截至当前时间步的序列的历史信息，就像是神经网络当前时间步的状态或记忆一样。由于$H_{t}$的计算基于$H_{t-1}$，上式的计算是循环的，使用循环计算的网络即<code>循环神经网络</code>（recurrent neural network）。</p><p>在时间步$t$，输出层的输出为：</p><script type="math/tex; mode=display">\boldsymbol{O}_t = \boldsymbol{H}_t \boldsymbol{W}_{hq} + \boldsymbol{b}_q.</script><p>其中$\boldsymbol{W}_{hq} \in \mathbb{R}^{h \times q}$，$\boldsymbol{b}_q \in \mathbb{R}^{1 \times q}$。<br>（h 是由上一层隐藏层的神经元个数决定的，q 是自己的输出个数）</p><h2 id="从零开始实现循环神经网络"><a href="#从零开始实现循环神经网络" class="headerlink" title="从零开始实现循环神经网络"></a>从零开始实现循环神经网络</h2><p>我们先尝试从零开始实现一个基于字符级循环神经网络的语言模型，这里我们使用周杰伦的歌词作为语料，首先我们读入数据：</p><pre><code class="lang-python">import torchimport torch.nn as nnimport timeimport mathimport syssys.path.append(&quot;/home/kesci/input&quot;)import d2l_jay9460 as d2l(corpus_indices, char_to_idx, idx_to_char, vocab_size) = d2l.load_data_jay_lyrics()device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)</code></pre><h3 id="one-hot向量"><a href="#one-hot向量" class="headerlink" title="one-hot向量"></a>one-hot向量</h3><p>我们需要将字符表示成向量，这里采用one-hot向量。假设词典大小是$N$，每次字符对应一个从$0$到$N-1$的唯一的索引，则该字符的向量是一个长度为$N$的向量，若字符的索引是$i$，则该向量的第$i$个位置为$1$，其他位置为$0$。下面分别展示了索引为0和2的one-hot向量，向量长度等于词典大小 vocab_size。</p><pre><code class="lang-python">def one_hot(x, n_class, dtype=torch.float32):    result = torch.zeros(x.shape[0], n_class, dtype=dtype, device=x.device)  # shape: (n, n_class)    result.scatter_(1, x.long().view(-1, 1), 1)  # result[i, x[i, 0]] = 1    return result# one-hot 每个人携带整个序列，但是只标记自己print(vocab_size)x = torch.tensor([0, 2])x_one_hot = one_hot(x, vocab_size)print(x_one_hot)print(x_one_hot.shape)print(x_one_hot.sum(axis=1))</code></pre><pre><code>1027tensor([[1., 0., 0.,  ..., 0., 0., 0.],        [0., 0., 1.,  ..., 0., 0., 0.]])torch.Size([2, 1027])tensor([1., 1.])</code></pre><p>我们每次采样的小批量的形状是（批量大小, 时间步数）。下面的函数将这样的小批量变换成数个形状为（批量大小, 词典大小）的矩阵，矩阵个数等于时间步数。也就是说，时间步$t$的输入为$\boldsymbol{X}_t \in \mathbb{R}^{n \times d}$，其中$n$为批量大小，$d$为词向量大小，即one-hot向量长度（词典大小）。</p><pre><code class="lang-python">def to_onehot(X, n_class):      # X 相当于小批量，批量大小即行数n，时间步数即列数d, n_class 相当于字典大小，即单词个数 vocab_size    # 每次传给 one_hot 参数，X[:, i]表示第 i 列所有行，每次两行，0/5, 1/6... n_class 即 vocab_size    # 对每一列进行 one_hot, 最后返回的一个三维 tensor    # one_hot(X[:, i] 为 [tensor([0, 5]), tensor([1, 6]), tensor([2, 7]), tensor([3, 8]), tensor([4, 9])]    return [one_hot(X[:, i], n_class) for i in range(X.shape[1])]X = torch.arange(10).view(2, 5) #此处批量大小为 2，时间步数为 5inputs = to_onehot(X, vocab_size)print(X)print(inputs)# 上面是一行一组x = torch.tensor([0, 2]), x只有一行，这里是两个一组，因为 X 有两行</code></pre><h3 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><pre><code class="lang-python">num_inputs, num_hiddens, num_outputs = vocab_size, 256, vocab_size# num_inputs: d# num_hiddens: h, 隐藏单元的个数256, 是超参数# num_outputs: qdef get_params():    def _one(shape): # 对给定的 shape 进行随机初始化        param = torch.zeros(shape, device=device, dtype=torch.float32)        nn.init.normal_(param, 0, 0.01)        return torch.nn.Parameter(param)    # 隐藏层参数    W_xh = _one((num_inputs, num_hiddens))    W_hh = _one((num_hiddens, num_hiddens))    b_h = torch.nn.Parameter(torch.zeros(num_hiddens, device=device))    # 输出层参数    W_hq = _one((num_hiddens, num_outputs))    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device))    return (W_xh, W_hh, b_h, W_hq, b_q)</code></pre><h3 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h3><p>函数<code>rnn</code>用循环的方式依次完成循环神经网络每个时间步的计算。</p><pre><code class="lang-python">def rnn(inputs, state, params):    # inputs和outputs皆为num_steps个形状为(batch_size, vocab_size)的矩阵    W_xh, W_hh, b_h, W_hq, b_q = params    H, = state   # 状态可能有多个，先定义成一个元组    outputs = []    for X in inputs:        H = torch.tanh(torch.matmul(X, W_xh) + torch.matmul(H, W_hh) + b_h)  # 根据公式        Y = torch.matmul(H, W_hq) + b_q        outputs.append(Y)    return outputs, (H,)   # 可能会使用相邻采样，当前的状态(H,)就可以作为下一个状态的初始值</code></pre><p>函数init_rnn_state初始化隐藏变量，这里的返回值是一个元组。</p><pre><code class="lang-python">def init_rnn_state(batch_size, num_hiddens, device):    return (torch.zeros((batch_size, num_hiddens), device=device), )</code></pre><p>做个简单的测试来观察输出结果的个数（时间步数），以及第一个时间步的输出层输出的形状和隐藏状态的形状。</p><pre><code class="lang-python"># print(X.shape)  # 输入批量，批量大小是 2，时间步数是 5# print(num_hiddens) # 隐藏单元个数 256# print(vocab_size)  # 字典大小 1027state = init_rnn_state(X.shape[0], num_hiddens, device)  # 初始化状态inputs = to_onehot(X.to(device), vocab_size)params = get_params()outputs, state_new = rnn(inputs, state, params)  # 把 one_hot 形式的 X 作为 inputs 输入进去# print(len(inputs), inputs[0].shape)   # 5 torch.Size([2, 1027])# print(len(outputs), outputs[0].shape) # 5 torch.Size([2, 1027])# print(len(state), state[0].shape)     # 1 torch.Size([2, 256])# print(len(state_new), state_new[0].shape) # 1 torch.Size([2, 256])  都是 2 行 256 列</code></pre><h3 id="裁剪梯度"><a href="#裁剪梯度" class="headerlink" title="裁剪梯度"></a>裁剪梯度</h3><p>循环神经网络中较容易出现梯度衰减或梯度爆炸，这会导致网络几乎无法训练。<code>裁剪梯度</code>（clip gradient）是一种应对<code>梯度爆炸</code>的方法。假设我们把所有模型参数的梯度拼接成一个向量 $\boldsymbol{g}$，并设裁剪的阈值是$\theta$。裁剪后的梯度</p><script type="math/tex; mode=display"> \min\left(\frac{\theta}{\|\boldsymbol{g}\|}, 1\right)\boldsymbol{g}</script><p>的$L_2$范数不超过$\theta$。</p><pre><code class="lang-python">def grad_clipping(params, theta, device):    norm = torch.tensor([0.0], device=device)    for param in params:        norm += (param.grad.data ** 2).sum()    norm = norm.sqrt().item()    if norm &gt; theta:  # 如果不够 1，就乘上一个数        for param in params:            param.grad.data *= (theta / norm)</code></pre><h3 id="定义预测函数"><a href="#定义预测函数" class="headerlink" title="定义预测函数"></a>定义预测函数</h3><p>以下函数基于前缀<code>prefix</code>（含有数个字符的字符串）来预测接下来的<code>num_chars</code>个字符。这个函数稍显复杂，其中我们将循环神经单元<code>rnn</code>设置成了函数参数，这样在后面小节介绍其他循环神经网络时能重复使用这个函数。</p><pre><code class="lang-python">def predict_rnn(prefix, num_chars, rnn, params, init_rnn_state,  # 给定 prefix，预测接下来的 num_chars                num_hiddens, vocab_size, device, idx_to_char, char_to_idx):    state = init_rnn_state(1, num_hiddens, device)    output = [char_to_idx[prefix[0]]]   # output记录prefix加上预测的num_chars个字符    for t in range(num_chars + len(prefix) - 1):        # 将上一时间步的输出作为当前时间步的输入        X = to_onehot(torch.tensor([[output[-1]]], device=device), vocab_size)        # 计算输出和更新隐藏状态        (Y, state) = rnn(X, state, params)        # 下一个时间步的输入是prefix里的字符(还在初始态）, 或者当前的最佳预测字符（计算出了 Y）        if t &lt; len(prefix) - 1:            output.append(char_to_idx[prefix[t + 1]])        else:            output.append(Y[0].argmax(dim=1).item())    return &#39;&#39;.join([idx_to_char[i] for i in output])</code></pre><p>我们先测试一下<code>predict_rnn</code>函数。我们将根据前缀“分开”创作长度为10个字符（不考虑前缀长度）的一段歌词。因为模型参数为随机值，所以预测结果也是随机的。</p><pre><code class="lang-python">predict_rnn(&#39;分开&#39;, 10, rnn, params, init_rnn_state, num_hiddens, vocab_size,            device, idx_to_char, char_to_idx)</code></pre><pre><code>&#39;分开潮养每霜敌刺母张暴欢&#39;</code></pre><h3 id="困惑度"><a href="#困惑度" class="headerlink" title="困惑度"></a>困惑度</h3><p>我们通常使用<code>困惑度</code>（perplexity）来评价语言模型的好坏。回忆一下“softmax回归”一节中交叉熵损失函数的定义。困惑度是对交叉熵损失函数做指数运算后得到的值。特别地，</p><ul><li><code>最佳</code>情况下，模型总是把标签类别的概率预测为1，此时困惑度为1；  （困惑度就是预测概率的倒数）</li><li><code>最坏</code>情况下，模型总是把标签类别的概率预测为0，此时困惑度为正无穷；</li><li><code>基线</code>情况下，模型总是预测所有类别的概率都相同，此时困惑度为类别个数。</li></ul><p>显然，任何一个有效模型的困惑度必须小于类别个数。在本例中，困惑度必须<code>小于</code>词典大小<code>vocab_size</code>。</p><h3 id="定义模型训练函数"><a href="#定义模型训练函数" class="headerlink" title="定义模型训练函数"></a>定义模型训练函数</h3><p>跟之前章节的模型训练函数相比，这里的模型训练函数有以下几点不同：</p><ol><li>使用困惑度评价模型。</li><li>在迭代模型参数前裁剪梯度。</li><li>对时序数据采用不同采样方法将导致隐藏状态初始化的不同。</li></ol><pre><code class="lang-python">def train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,                          vocab_size, device, corpus_indices, idx_to_char,                          char_to_idx, is_random_iter, num_epochs, num_steps,                          lr, clipping_theta, batch_size, pred_period,                          pred_len, prefixes):    if is_random_iter:        data_iter_fn = d2l.data_iter_random  # 随机采样    else:        data_iter_fn = d2l.data_iter_consecutive # 相邻采样    params = get_params()    loss = nn.CrossEntropyLoss()    for epoch in range(num_epochs):        if not is_random_iter:  # 如使用相邻采样，在epoch开始时初始化隐藏状态            state = init_rnn_state(batch_size, num_hiddens, device)        l_sum, n, start = 0.0, 0, time.time()        data_iter = data_iter_fn(corpus_indices, batch_size, num_steps, device)        for X, Y in data_iter:            if is_random_iter:  # 如使用随机采样，在每个小批量更新前初始化隐藏状态                state = init_rnn_state(batch_size, num_hiddens, device)            else:  # 否则需要使用detach函数从计算图分离隐藏状态                for s in state:                    s.detach_()  # detach_()将返回的向量从创建它的计算图中分离出来，后面就不再对它求梯度了            # inputs是num_steps个形状为(batch_size, vocab_size)的矩阵            inputs = to_onehot(X, vocab_size)            # outputs有num_steps个形状为(batch_size, vocab_size)的矩阵            (outputs, state) = rnn(inputs, state, params)  # 前向计算            # 拼接之后形状为(num_steps * batch_size, vocab_size)            outputs = torch.cat(outputs, dim=0)            # Y的形状是(batch_size, num_steps)，转置后再变成形状为            # (num_steps * batch_size,)的向量，这样跟输出的行一一对应            y = torch.flatten(Y.T)            # 使用交叉熵损失计算平均分类误差            l = loss(outputs, y.long())            # 梯度清0            if params[0].grad is not None:                for param in params:                    param.grad.data.zero_()            l.backward()            grad_clipping(params, clipping_theta, device)  # 裁剪梯度            d2l.sgd(params, lr, 1)  # 因为误差已经取过均值，梯度不用再做平均            l_sum += l.item() * y.shape[0]            n += y.shape[0]        if (epoch + 1) % pred_period == 0:            print(&#39;epoch %d, perplexity %f, time %.2f sec&#39; % (                epoch + 1, math.exp(l_sum / n), time.time() - start))            for prefix in prefixes:                print(&#39; -&#39;, predict_rnn(prefix, pred_len, rnn, params, init_rnn_state,                    num_hiddens, vocab_size, device, idx_to_char, char_to_idx))</code></pre><h3 id="训练模型并创作歌词"><a href="#训练模型并创作歌词" class="headerlink" title="训练模型并创作歌词"></a>训练模型并创作歌词</h3><p>现在我们可以训练模型了。首先，设置模型超参数。我们将根据前缀“分开”和“不分开”分别创作长度为50个字符（不考虑前缀长度）的一段歌词。我们每过50个迭代周期便根据当前训练的模型创作一段歌词。</p><pre><code class="lang-python">num_epochs, num_steps, batch_size, lr, clipping_theta = 250, 35, 32, 1e2, 1e-2pred_period, pred_len, prefixes = 50, 50, [&#39;分开&#39;, &#39;不分开&#39;]</code></pre><p>下面采用随机采样训练模型并创作歌词。</p><pre><code class="lang-python">train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,                      vocab_size, device, corpus_indices, idx_to_char,                      char_to_idx, True, num_epochs, num_steps, lr,                      clipping_theta, batch_size, pred_period, pred_len,                      prefixes)</code></pre><pre><code>epoch 50, perplexity 71.093786, time 0.76 sec - 分开 我不要再想你 我知要这 我的让我心能 不要我 别子我 别子我 别子我 别子我 别子我 别子我 别子 - 不分开 我不要这生你 我知想这 我的让我心能 不要我 别子我 别子我 别子我 别子我 别子我 别子我 别子epoch 100, perplexity 9.897621, time 0.71 sec - 分开 我想想好生活 不知 是你 却不 痛箭  穿什么 如果我遇起头 有话去对医药 说哼 我不 我不 我不 - 不分开永 我不想再想 我不 我不 我不要 爱情走的太快就像龙卷风 不能不让我疯狂的可爱女人 坏坏的让我疯狂epoch 150, perplexity 2.847206, time 0.77 sec - 分开 还默法 一步两步三步四步望著天 看星星 一颗两颗三颗四颗 连成线背著背默默许下心愿 看远方的星是否 - 不分开吗 我不能再想 我不 我不 我不要 爱情走的太快就像龙卷风 不能承受我已无处可躲 我不想再想 我不能epoch 200, perplexity 1.564534, time 0.72 sec - 分开 有想都 教拳脚武术的老板 练铁沙掌 耍杨家枪 在伤子中夫我妈愿知错搞错  爱 用着星的对笑下想通  - 不分开想 我不能再想 我不 我不 我不能 爱情走的太快就像龙卷风 不能承受我已无处可躲 我不要再想 我不要epoch 250, perplexity 1.311809, time 0.72 sec - 分开 那切都上一步四步望透的 为吸走 一颗两颗三颗四颗 连成线背著背 游荡在蓝安排的雨 随时准备来袭 我 - 不分开简简的胖女巫 用拉丁文念咒语啦啦呜 她养的黑猫笑起来像哭 啦啦啦呜 静满村枪 在人笑功 我一定带我妈</code></pre><p>接下来采用相邻采样训练模型并创作歌词。</p><pre><code class="lang-python">train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,                      vocab_size, device, corpus_indices, idx_to_char,                      char_to_idx, False, num_epochs, num_steps, lr,                      clipping_theta, batch_size, pred_period, pred_len,                      prefixes)</code></pre><pre><code>epoch 50, perplexity 60.294393, time 0.74 sec - 分开 我想要你想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我 - 不分开 我想要你 你有了 别不我的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我epoch 100, perplexity 7.141162, time 0.72 sec - 分开 我已要再爱 我不要再想 我不 我不 我不要再想 我不 我不 我不要 爱情我的见快就像龙卷风 离能开 - 不分开柳 你天黄一个棍 后知哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 epoch 150, perplexity 2.090277, time 0.73 sec - 分开 我已要这是你在著 不想我都做得到 但那个人已经不是我 没有你在 我却多难熬  没有你在我有多难熬多 - 不分开觉 你已经离 我想再好 这样心中 我一定带我 我的完空 不你是风 一一彩纵 在人心中 我一定带我妈走epoch 200, perplexity 1.305391, time 0.77 sec - 分开 我已要这样牵看你的手 它一定实现它一定像现 载著你 彷彿载著阳光 不管到你留都是晴天 蝴蝶自在飞力 - 不分开觉 你已经离开我 不知不觉 我跟了这节奏 后知后觉 又过了一个秋 后知后觉 我该好好生活 我该好好生epoch 250, perplexity 1.230800, time 0.79 sec - 分开 我不要 是你看的太快了悲慢 担心今手身会大早 其么我也睡不着  昨晚梦里你来找 我才  原来我只想 - 不分开觉 你在经离开我 不知不觉 你知了有节奏 后知后觉 后知了一个秋 后知后觉 我该好好生活 我该好好生</code></pre><h2 id="循环神经网络的-pytorch-简洁实现"><a href="#循环神经网络的-pytorch-简洁实现" class="headerlink" title="循环神经网络的 pytorch 简洁实现"></a>循环神经网络的 pytorch 简洁实现</h2><h3 id="定义模型-1"><a href="#定义模型-1" class="headerlink" title="定义模型"></a>定义模型</h3><p>我们使用Pytorch中的<code>nn.RNN</code>来构造循环神经网络。在本节中，我们主要关注<code>nn.RNN</code>的以下几个构造函数参数：</p><ul><li><code>input_size</code> - The number of expected features in the input x</li><li><code>hidden_size</code> – The number of features in the hidden state h</li><li><code>nonlinearity</code> – The non-linearity to use. Can be either ‘tanh’ or ‘relu’. Default: ‘tanh’</li><li><code>batch_first</code> – If True, then the input and output tensors are provided as (batch_size, num_steps, input_size). Default: False</li></ul><p>这里的<code>batch_first</code>决定了输入的形状，我们使用默认的参数<code>False</code>，对应的输入形状是 (num_steps, batch_size, input_size)。</p><p><code>forward</code>函数的参数为：</p><ul><li><code>input</code> of shape (num_steps, batch_size, input_size): tensor containing the features of the input sequence. （三维）</li><li><code>h_0</code> of shape (num_layers * num_directions, batch_size, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1. （<code>h_0</code>对应的是 RNN 中的初始 state）<br>（num_layers主要和深度 RNN 有关，num_directions主要和双向 RNN 有关，此处 <code>num_layers 乘以 num_directions</code> = 1）</li></ul><p><code>forward</code>函数的返回值是：</p><ul><li><code>output</code> of shape (num_steps, batch_size, num_directions * hidden_size): tensor containing the output features (h_t) from the last layer of the RNN, for each t.</li><li><code>h_n</code> of shape (num_layers * num_directions, batch_size, hidden_size): tensor containing the hidden state for t = num_steps. （最后一个时间步，隐藏状态的值）</li></ul><p>现在我们构造一个<code>nn.RNN</code>实例，并用一个简单的例子来看一下输出的形状。</p><pre><code class="lang-python">rnn_layer = nn.RNN(input_size=vocab_size, hidden_size=num_hiddens)num_steps, batch_size = 35, 2X = torch.rand(num_steps, batch_size, vocab_size)state = NoneY, state_new = rnn_layer(X, state)print(Y.shape, state_new.shape)</code></pre><pre><code>torch.Size([35, 2, 256]) torch.Size([1, 2, 256])</code></pre><p>我们定义一个完整的基于循环神经网络的语言模型。</p><pre><code class="lang-python">class RNNModel(nn.Module):    def __init__(self, rnn_layer, vocab_size):        super(RNNModel, self).__init__()        self.rnn = rnn_layer  # 一个 rnn 实例，也可以是一个 LSTM 实例        self.hidden_size = rnn_layer.hidden_size * (2 if rnn_layer.bidirectional else 1)         self.vocab_size = vocab_size        self.dense = nn.Linear(self.hidden_size, vocab_size)  #线性层作为输出层    def forward(self, inputs, state):        # inputs.shape: (batch_size, num_steps)        X = to_onehot(inputs, vocab_size)        X = torch.stack(X)  # X.shape: (num_steps, batch_size, vocab_size)        hiddens, state = self.rnn(X, state)        hiddens = hiddens.view(-1, hiddens.shape[-1])  # hiddens.shape: (num_steps * batch_size, hidden_size)        output = self.dense(hiddens)  # 用Linear线性函数作输出        return output, state</code></pre><p>类似的，我们需要实现一个<code>预测函数</code>，与前面的区别在于前向计算和初始化隐藏状态。</p><pre><code class="lang-python">def predict_rnn_pytorch(prefix, num_chars, model, vocab_size, device, idx_to_char,                      char_to_idx):    state = None    output = [char_to_idx[prefix[0]]]  # output记录prefix加上预测的num_chars个字符    for t in range(num_chars + len(prefix) - 1):        X = torch.tensor([output[-1]], device=device).view(1, 1)        (Y, state) = model(X, state)  # 前向计算不需要传入模型参数        if t &lt; len(prefix) - 1:            output.append(char_to_idx[prefix[t + 1]])        else:            output.append(Y.argmax(dim=1).item())    return &#39;&#39;.join([idx_to_char[i] for i in output])</code></pre><p>使用权重为随机值的模型来预测一次。</p><pre><code class="lang-python">model = RNNModel(rnn_layer, vocab_size).to(device)predict_rnn_pytorch(&#39;分开&#39;, 10, model, vocab_size, device, idx_to_char, char_to_idx)</code></pre><pre><code>&#39;分开老忧忧老钩忧老忧忧老&#39;</code></pre><p>接下来实现<code>训练函数</code>，这里只使用了相邻采样。</p><pre><code class="lang-python">def train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,                                corpus_indices, idx_to_char, char_to_idx,                                num_epochs, num_steps, lr, clipping_theta,                                batch_size, pred_period, pred_len, prefixes):    loss = nn.CrossEntropyLoss()    optimizer = torch.optim.Adam(model.parameters(), lr=lr)    model.to(device)    for epoch in range(num_epochs):        l_sum, n, start = 0.0, 0, time.time()        data_iter = d2l.data_iter_consecutive(corpus_indices, batch_size, num_steps, device) # 相邻采样        state = None        for X, Y in data_iter:            if state is not None:                # 使用detach函数从计算图分离隐藏状态                if isinstance (state, tuple): # LSTM, state:(h, c)                      state[0].detach_()                    state[1].detach_()                else:                     state.detach_()            (output, state) = model(X, state) # output.shape: (num_steps * batch_size, vocab_size)            y = torch.flatten(Y.T)            l = loss(output, y.long())            optimizer.zero_grad()            l.backward()            grad_clipping(model.parameters(), clipping_theta, device)            optimizer.step()            l_sum += l.item() * y.shape[0]            n += y.shape[0]        if (epoch + 1) % pred_period == 0:            print(&#39;epoch %d, perplexity %f, time %.2f sec&#39; % (                epoch + 1, math.exp(l_sum / n), time.time() - start))            for prefix in prefixes:                print(&#39; -&#39;, predict_rnn_pytorch(                    prefix, pred_len, model, vocab_size, device, idx_to_char,                    char_to_idx))</code></pre><p>训练模型。</p><pre><code class="lang-python">num_epochs, batch_size, lr, clipping_theta = 250, 32, 1e-3, 1e-2pred_period, pred_len, prefixes = 50, 50, [&#39;分开&#39;, &#39;不分开&#39;]train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,                            corpus_indices, idx_to_char, char_to_idx,                            num_epochs, num_steps, lr, clipping_theta,                            batch_size, pred_period, pred_len, prefixes)</code></pre><pre><code>epoch 50, perplexity 10.107632, time 0.58 sec - 分开 我不了口  一定 我的走  有一颗四颗 连成线背著背默默许下心愿 在小村外的溪边河口 默默默默 我 - 不分开 我想要你的手 我有多烦恼  说你你你不我  想要你的让我面红的可爱女人 坏坏的让我疯狂的可爱女人 epoch 100, perplexity 1.234571, time 0.56 sec - 分开 我面心事 爱来一 蓝色眼睛的凯萨琳公主 专吃 有AB血型的公老鼠 恍恍惚惚 是谁的脚步 银制茶壶  - 不分开 我没有话不想就多烦恼多难熬  说你再爸不会不 多想就这样没担你 我着你的想活 说想要你的微笑每天都epoch 150, perplexity 1.060467, time 0.57 sec - 分开 我面心事 从来开 我妈妈没有你 我有多烦恼  没有你烦我有多烦恼多难熬  穿过云层 我试著努力向你 - 不分开 了没有多难过 是什么 干什么 我想想和你说你听的让我的听你就到 我想你说你 我不了口让她知道 就是epoch 200, perplexity 1.030212, time 0.64 sec - 分开 我面心外 相来开  家庭你 你打我不要 想  穿什么 你就想一我 你这样打我妈 你跟你 没有 我不 - 不分开 了我不知不要 这样的甜蜜 让我开始想相 命和你当许在这里在听飞就一片 说有一个风慢的老斑鸠 印地安epoch 250, perplexity 1.020210, time 0.60 sec - 分开 我面心仪 相来方 不知道这 杵水伊 坦堡 却只想你和汉堡 我想要你的微笑每天都能看到  我知道这里 - 不分开 了我水也睡活 我想多受就能再这样的节奏 谁都无可奈何 没有你以后 我灵魂失控 黑云在降落 我被它拖</code></pre><h2 id="练习题"><a href="#练习题" class="headerlink" title="练习题"></a>练习题</h2><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://s2.ax1x.com/2020/02/19/3VkgpT.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动手深度学习(六):语言模型</title>
      <link href="/2020/02/19/dl-notes6-lang-model/"/>
      <url>/2020/02/19/dl-notes6-lang-model/</url>
      
        <content type="html"><![CDATA[<h1 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h1><p>一段自然语言文本可以看作是一个离散时间序列，给定一个长度为$T$的词的序列$w_1, w_2, \ldots, w_T$，语言模型的目标就是评估该序列是否合理，即计算该序列的概率：</p><script type="math/tex; mode=display">P(w_1, w_2, \ldots, w_T).</script><p>本节我们介绍基于统计的语言模型，主要是$n$元语法（$n$-gram）。在后续内容中，我们将会介绍基于神经网络的语言模型。</p><h2 id="语言模型-1"><a href="#语言模型-1" class="headerlink" title="语言模型"></a>语言模型</h2><p>假设序列$w_1, w_2, \ldots, w_T$中的每个词是依次生成的，我们有</p><script type="math/tex; mode=display">\begin{aligned}P\left(w_{1}, w_{2}, \ldots, w_{T}\right) &=\prod_{t=1}^{T} P\left(w_{t} | w_{1}, \ldots, w_{t-1}\right) \\&=P\left(w_{1}\right) P\left(w_{2} | w_{1}\right) \cdots P\left(w_{T} | w_{1} w_{2} \cdots w_{T-1}\right)\end{aligned}</script><p>例如，一段含有4个词的文本序列的概率</p><script type="math/tex; mode=display">P(w_1, w_2, w_3, w_4) =  P(w_1) P(w_2 \mid w_1) P(w_3 \mid w_1, w_2) P(w_4 \mid w_1, w_2, w_3).</script><p>语言模型的参数就是词的概率以及给定前几个词情况下的条件概率。设训练数据集为一个大型文本语料库，如维基百科的所有条目，词的概率可以通过该词在训练数据集中的相对词频来计算，例如，$w_1$的概率可以计算为：</p><script type="math/tex; mode=display">\hat P(w_1) = \frac{n(w_1)}{n}</script><p>其中$n(w_1)$为语料库中以$w_1$作为第一个词的文本的数量，$n$为语料库中文本的总数量。</p><p>类似的，给定$w_1$情况下，$w_2$的条件概率可以计算为：</p><script type="math/tex; mode=display">\hat P(w_2 \mid w_1) = \frac{n(w_1, w_2)}{n(w_1)}</script><p>其中$n(w_1, w_2)$为语料库中以$w_1$作为第一个词，$w_2$作为第二个词的文本的数量。</p><h2 id="n元语法"><a href="#n元语法" class="headerlink" title="n元语法"></a>n元语法</h2><p>序列长度增加，计算和存储多个词共同出现的概率的复杂度会呈指数级增加。$n$元语法通过马尔可夫假设简化模型，<code>马尔科夫假设</code>是指一个词的出现<code>只与前面n个词相关</code>，即$n$阶马尔可夫链（Markov chain of order $n$），如果$n=1$，那么有$P(w_3 \mid w_1, w_2) = P(w_3 \mid w_2)$。基于$n-1$阶马尔可夫链，我们可以将语言模型改写为</p><script type="math/tex; mode=display">P(w_1, w_2, \ldots, w_T) = \prod_{t=1}^T P(w_t \mid w_{t-(n-1)}, \ldots, w_{t-1}) .</script><p>以上也叫<code>n元语法</code>（$n$-grams），它是基于<code>n - 1阶马尔可夫链</code>的概率语言模型。例如，当$n=2$时，含有4个词的文本序列的概率就可以改写为：</p><script type="math/tex; mode=display">\begin{aligned}P\left(w_{1}, w_{2}, w_{3}, w_{4}\right) &=P\left(w_{1}\right) P\left(w_{2} | w_{1}\right) P\left(w_{3} | w_{1}, w_{2}\right) P\left(w_{4} | w_{1}, w_{2}, w_{3}\right) \\&=P\left(w_{1}\right) P\left(w_{2} | w_{1}\right) P\left(w_{3} | w_{2}\right) P\left(w_{4} | w_{3}\right)\end{aligned}</script><p>当$n$分别为1、2和3时，我们将其分别称作一元语法（unigram）、二元语法（bigram）和三元语法（trigram）。例如，长度为4的序列$w_1, w_2, w_3, w_4$在一元语法、二元语法和三元语法中的概率分别为</p><script type="math/tex; mode=display">\begin{aligned}P(w_1, w_2, w_3, w_4) &=  P(w_1) P(w_2) P(w_3) P(w_4) ,\\P(w_1, w_2, w_3, w_4) &=  P(w_1) P(w_2 \mid w_1) P(w_3 \mid w_2) P(w_4 \mid w_3) ,\\P(w_1, w_2, w_3, w_4) &=  P(w_1) P(w_2 \mid w_1) P(w_3 \mid w_1, w_2) P(w_4 \mid w_2, w_3) .\end{aligned}</script><p>当$n$较小时，$n$元语法往往并不准确。例如，在一元语法中，由三个词组成的句子“你走先”和“你先走”的概率是一样的。然而，当$n$较大时，$n$元语法需要计算并存储大量的词频和多词相邻频率。</p><p>思考：$n$元语法可能有哪些缺陷？</p><ol><li>参数空间过大</li><li>数据稀疏</li></ol><h1 id="语言模型数据集"><a href="#语言模型数据集" class="headerlink" title="语言模型数据集"></a>语言模型数据集</h1><h2 id="读取数据集"><a href="#读取数据集" class="headerlink" title="读取数据集"></a>读取数据集</h2><pre><code class="lang-python">with open(&#39;/home/kesci/input/jaychou_lyrics4703/jaychou_lyrics.txt&#39;) as f:    corpus_chars = f.read()print(len(corpus_chars))print(corpus_chars[: 40])corpus_chars = corpus_chars.replace(&#39;\n&#39;, &#39; &#39;).replace(&#39;\r&#39;, &#39; &#39;)corpus_chars = corpus_chars[: 10000]</code></pre><pre><code>63282想要有直升机想要和你飞到宇宙去想要和你融化在一起融化在宇宙里我每天每天每</code></pre><h2 id="建立字符索引"><a href="#建立字符索引" class="headerlink" title="建立字符索引"></a>建立字符索引</h2><pre><code class="lang-python">idx_to_char = list(set(corpus_chars)) # 去重，得到索引到字符的映射char_to_idx = {char: i for i, char in enumerate(idx_to_char)} # 字符到索引的映射vocab_size = len(char_to_idx)print(vocab_size)corpus_indices = [char_to_idx[char] for char in corpus_chars]  # 将每个字符转化为索引，得到一个索引的序列sample = corpus_indices[: 20]print(&#39;chars:&#39;, &#39;&#39;.join([idx_to_char[idx] for idx in sample]))print(&#39;indices:&#39;, sample)</code></pre><pre><code>1027chars: 想要有直升机 想要和你飞到宇宙去 想要和indices: [153, 983, 692, 753, 789, 806, 502, 153, 983, 312, 582, 908, 664, 965, 211, 857, 502, 153, 983, 312]</code></pre><p>定义函数<code>load_data_jay_lyrics</code>，在后续章节中直接调用。</p><pre><code class="lang-python">def load_data_jay_lyrics():    with open(&#39;/home/kesci/input/jaychou_lyrics4703/jaychou_lyrics.txt&#39;) as f:        corpus_chars = f.read()    corpus_chars = corpus_chars.replace(&#39;\n&#39;, &#39; &#39;).replace(&#39;\r&#39;, &#39; &#39;)    corpus_chars = corpus_chars[0:10000]    idx_to_char = list(set(corpus_chars))    char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])    vocab_size = len(char_to_idx)    corpus_indices = [char_to_idx[char] for char in corpus_chars]    return corpus_indices, char_to_idx, idx_to_char, vocab_size</code></pre><h2 id="时序数据的采样"><a href="#时序数据的采样" class="headerlink" title="时序数据的采样"></a>时序数据的采样</h2><p>在训练中我们需要每次随机读取小批量样本和标签。与之前章节的实验数据不同的是，时序数据的一个样本通常包含连续的字符。假设时间步数为5，样本序列为5个字符，即“想”“要”“有”“直”“升”。该样本的标签序列为这些字符分别在训练集中的下一个字符，即“要”“有”“直”“升”“机”，即$X$=“想要有直升”，$Y$=“要有直升机”。</p><p>现在我们考虑序列“想要有直升机，想要和你飞到宇宙去”，如果时间步数为5，有以下可能的样本和标签：</p><ul><li>$X$：“想要有直升”，$Y$：“要有直升机”</li><li>$X$：“要有直升机”，$Y$：“有直升机，”</li><li>$X$：“有直升机，”，$Y$：“直升机，想”</li><li>…</li><li>$X$：“要和你飞到”，$Y$：“和你飞到宇”</li><li>$X$：“和你飞到宇”，$Y$：“你飞到宇宙”</li><li>$X$：“你飞到宇宙”，$Y$：“飞到宇宙去”</li></ul><p>可以看到，如果序列的长度为$T$，时间步数为$n$，那么一共有$T-n$个合法的样本，但是这些样本有大量的重合，我们通常采用更加高效的采样方式。我们有两种方式对时序数据进行采样，分别是随机采样和相邻采样。</p><h3 id="随机采样"><a href="#随机采样" class="headerlink" title="随机采样"></a>随机采样</h3><p>下面的代码每次从数据里随机采样一个小批量。其中批量大小<code>batch_size</code>是每个小批量的样本数，<code>num_steps</code>是每个样本所包含的时间步数。<br>在随机采样中，每个样本是原始序列上任意截取的一段序列，相邻的两个随机小批量在原始序列上的位置不一定相毗邻。</p><pre><code class="lang-python">import torchimport randomdef data_iter_random(corpus_indices, batch_size, num_steps, device=None):    # 减1是因为对于长度为n的序列，X最多只有包含其中的前n - 1个字符    num_examples = (len(corpus_indices) - 1) // num_steps  # 下取整，得到不重叠情况下的样本个数    example_indices = [i * num_steps for i in range(num_examples)]  # 每个样本的第一个字符在corpus_indices中的下标    random.shuffle(example_indices)    def _data(i):        # 返回从i开始的长为num_steps的序列        return corpus_indices[i: i + num_steps]    if device is None:        device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)    for i in range(0, num_examples, batch_size):        # 每次选出batch_size个随机样本        batch_indices = example_indices[i: i + batch_size]  # 当前batch的各个样本的首字符的下标        X = [_data(j) for j in batch_indices]        Y = [_data(j + 1) for j in batch_indices]        yield torch.tensor(X, device=device), torch.tensor(Y, device=device)</code></pre><p>测试一下这个函数，我们输入从0到29的连续整数作为一个人工序列，设批量大小和时间步数分别为2和6，打印随机采样每次读取的小批量样本的输入<code>X</code>和标签<code>Y</code>。</p><pre><code class="lang-python">my_seq = list(range(30))for X, Y in data_iter_random(my_seq, batch_size=2, num_steps=6):    #batch_size 为 2（每个 batch 有两行), num_steps 时间步数为 6 (每行有 5 个元素) batch_num 为 2，一共两个 batch    print(&#39;X: &#39;, X, &#39;\nY:&#39;, Y, &#39;\n&#39;)</code></pre><pre><code>X:  tensor([[18, 19, 20, 21, 22, 23],        [12, 13, 14, 15, 16, 17]]) Y: tensor([[19, 20, 21, 22, 23, 24],        [13, 14, 15, 16, 17, 18]]) X:  tensor([[ 6,  7,  8,  9, 10, 11],        [ 0,  1,  2,  3,  4,  5]]) Y: tensor([[ 7,  8,  9, 10, 11, 12],        [ 1,  2,  3,  4,  5,  6]]) </code></pre><h3 id="相邻采样"><a href="#相邻采样" class="headerlink" title="相邻采样"></a>相邻采样</h3><p>在相邻采样中，相邻的两个随机小批量在原始序列上的位置相毗邻.（交替出现）<br><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://s2.ax1x.com/2020/02/19/3VSeKI.png" alt=""></p><pre><code class="lang-python">def data_iter_consecutive(corpus_indices, batch_size, num_steps, device=None):    # bach_size 为 n, 数据就会被拆成 n 块    if device is None:        device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)    corpus_len = len(corpus_indices) // batch_size * batch_size  # 保留下来的序列的长度    corpus_indices = corpus_indices[: corpus_len]  # 仅保留前corpus_len个字符 保证可以整除批量大小    indices = torch.tensor(corpus_indices, device=device)    indices = indices.view(batch_size, -1)  # resize成(batch_size, )    batch_num = (indices.shape[1] - 1) // num_steps   # -1 是因为不能包含最后一个字符，因为最后一个就没有标签了    for i in range(batch_num):  # 每次构建一个批量        i = i * num_steps        X = indices[:, i: i + num_steps]        Y = indices[:, i + 1: i + num_steps + 1]        yield X, Y</code></pre><p>同样的设置下，打印相邻采样每次读取的小批量样本的输入<code>X</code>和标签<code>Y</code>。相邻的两个随机小批量在原始序列上的位置相毗邻。<br>X 的标签 Y 就是它的下一个位置。</p><pre><code class="lang-python">for X, Y in data_iter_consecutive(my_seq, batch_size=2, num_steps=6):    print(&#39;X: &#39;, X, &#39;\nY:&#39;, Y, &#39;\n&#39;)my_seq = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]for X, Y in data_iter_consecutive(my_seq, batch_size=2, num_steps=2):    print(&#39;X: &#39;, X, &#39;\nY:&#39;, Y, &#39;\n&#39;)</code></pre><pre><code>X:  tensor([[ 0,  1,  2,  3,  4,  5],        [15, 16, 17, 18, 19, 20]]) Y: tensor([[ 1,  2,  3,  4,  5,  6],        [16, 17, 18, 19, 20, 21]]) X:  tensor([[ 6,  7,  8,  9, 10, 11],        [21, 22, 23, 24, 25, 26]]) Y: tensor([[ 7,  8,  9, 10, 11, 12],        [22, 23, 24, 25, 26, 27]]) X:  tensor([[0, 1],        [5, 6]]) Y: tensor([[1, 2],        [6, 7]]) X:  tensor([[2, 3],        [7, 8]]) Y: tensor([[3, 4],        [8, 9]]) </code></pre><h2 id="练习题"><a href="#练习题" class="headerlink" title="练习题"></a>练习题</h2><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/RlvJN7BnbPGfp2Y.png" alt="语言模型"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动手深度学习(五):梯度消失、梯度爆炸</title>
      <link href="/2020/02/19/dl-notes5-grad/"/>
      <url>/2020/02/19/dl-notes5-grad/</url>
      
        <content type="html"><![CDATA[<h1 id="梯度消失、梯度爆炸以及Kaggle房价预测"><a href="#梯度消失、梯度爆炸以及Kaggle房价预测" class="headerlink" title="梯度消失、梯度爆炸以及Kaggle房价预测"></a>梯度消失、梯度爆炸以及Kaggle房价预测</h1><ol><li>梯度消失和梯度爆炸</li><li>考虑到环境因素的其他问题</li><li>Kaggle房价预测</li></ol><h1 id="梯度消失和梯度爆炸"><a href="#梯度消失和梯度爆炸" class="headerlink" title="梯度消失和梯度爆炸"></a>梯度消失和梯度爆炸</h1><p>深度模型有关数值稳定性的典型问题是消失（vanishing）和爆炸（explosion）。</p><p><strong>当神经网络的层数较多时，模型的数值稳定性容易变差。</strong></p><p>假设一个层数为$L$的多层感知机的第$l$层$\boldsymbol{H}^{(l)}$的权重参数为$\boldsymbol{W}^{(l)}$，输出层$\boldsymbol{H}^{(L)}$的权重参数为$\boldsymbol{W}^{(L)}$。为了便于讨论，不考虑偏差参数，且设所有隐藏层的激活函数为恒等映射（identity mapping）$\phi(x) = x$。给定输入$\boldsymbol{X}$，多层感知机的第$l$层的输出$\boldsymbol{H}^{(l)} = \boldsymbol{X} \boldsymbol{W}^{(1)} \boldsymbol{W}^{(2)} \ldots \boldsymbol{W}^{(l)}$。此时，如果层数$l$较大，$\boldsymbol{H}^{(l)}$的计算可能会出现衰减或爆炸。举个例子，假设输入和所有层的权重参数都是标量，如权重参数为0.2和5，多层感知机的第30层输出为输入$\boldsymbol{X}$分别与$0.2^{30} \approx 1 \times 10^{-21}$（消失）和$5^{30} \approx 9 \times 10^{20}$（爆炸）的乘积。当层数较多时，梯度的计算也容易出现消失或爆炸。</p><h1 id="随机初始化模型参数"><a href="#随机初始化模型参数" class="headerlink" title="随机初始化模型参数"></a>随机初始化模型参数</h1><p>在神经网络中，通常需要随机初始化模型参数。下面我们来解释这样做的原因。</p><p>回顾多层感知机一节描述的多层感知机。为了方便解释，假设输出层只保留一个输出单元$o_1$（删去$o_2$和$o_3$以及指向它们的箭头），且隐藏层使用相同的激活函数。如果将每个隐藏单元的参数都初始化为相等的值，那么在正向传播时每个隐藏单元将根据相同的输入计算出相同的值，并传递至输出层。在反向传播中，每个隐藏单元的参数梯度值相等。因此，这些参数在使用基于梯度的优化算法迭代后值依然相等。之后的迭代也是如此。在这种情况下，无论隐藏单元有多少，隐藏层本质上只有1个隐藏单元在发挥作用。因此，正如在前面的实验中所做的那样，我们通常将神经网络的模型参数，特别是权重参数，进行随机初始化。</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/Yvn1aRXFSdqm5ED.png" alt=""></p><h3 id="PyTorch的默认随机初始化"><a href="#PyTorch的默认随机初始化" class="headerlink" title="PyTorch的默认随机初始化"></a>PyTorch的默认随机初始化</h3><p>随机初始化模型参数的方法有很多。在线性回归的简洁实现中，我们使用<code>torch.nn.init.normal_()</code>使模型<code>net</code>的权重参数采用正态分布的随机初始化方式。不过，PyTorch中<code>nn.Module</code>的模块参数都采取了较为合理的初始化策略（不同类型的layer具体采样的哪一种初始化方法的可参考<a href="https://github.com/pytorch/pytorch/tree/master/torch/nn/modules" target="_blank" rel="noopener">源代码</a>），因此一般不用我们考虑。</p><h3 id="Xavier随机初始化"><a href="#Xavier随机初始化" class="headerlink" title="Xavier随机初始化"></a>Xavier随机初始化</h3><p>还有一种比较常用的随机初始化方法叫作Xavier随机初始化。<br>假设某全连接层的输入个数为$a$，输出个数为$b$，Xavier随机初始化将使该层中权重参数的每个元素都随机采样于均匀分布</p><script type="math/tex; mode=display">U\left(-\sqrt{\frac{6}{a+b}}, \sqrt{\frac{6}{a+b}}\right).</script><p>它的设计主要考虑到，模型参数初始化后，每层输出的方差不该受该层输入个数影响，且每层梯度的方差也不该受该层输出个数影响。</p><h1 id="考虑环境因素"><a href="#考虑环境因素" class="headerlink" title="考虑环境因素"></a>考虑环境因素</h1><h2 id="协变量偏移"><a href="#协变量偏移" class="headerlink" title="协变量偏移"></a>协变量偏移</h2><p>这里我们假设，虽然输入的分布可能随时间而改变，但是标记函数，即条件分布P（y∣x）不会改变。虽然这个问题容易理解，但在实践中也容易忽视。</p><p>想想区分猫和狗的一个例子。我们的训练数据使用的是猫和狗的真实的照片，但是在测试时，我们被要求对猫和狗的卡通图片进行分类。</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://s2.ax1x.com/2020/02/19/3VWGV0.png" alt="协变量偏移"></p><p>显然，这不太可能奏效。训练集由照片组成，而测试集只包含卡通。在一个看起来与测试集有着本质不同的数据集上进行训练，而不考虑如何适应新的情况，这是不是一个好主意。不幸的是，这是一个非常常见的陷阱。</p><p>统计学家称这种协变量变化是因为问题的根源在于特征分布的变化（即协变量的变化）。数学上，我们可以说P（x）改变了，但P（y∣x）保持不变。尽管它的有用性并不局限于此，当我们认为x导致y时，协变量移位通常是正确的假设。</p><h2 id="标签偏移"><a href="#标签偏移" class="headerlink" title="标签偏移"></a>标签偏移</h2><p>当我们认为导致偏移的是标签P（y）上的边缘分布的变化，但类条件分布是不变的P（x∣y）时，就会出现相反的问题。当我们认为y导致x时，标签偏移是一个合理的假设。例如，通常我们希望根据其表现来预测诊断结果。在这种情况下，我们认为诊断引起的表现，即疾病引起的症状。有时标签偏移和协变量移位假设可以同时成立。例如，当真正的标签函数是确定的和不变的，那么协变量偏移将始终保持，包括如果标签偏移也保持。有趣的是，当我们期望标签偏移和协变量偏移保持时，使用来自标签偏移假设的方法通常是有利的。这是因为这些方法倾向于操作看起来像标签的对象，这（在深度学习中）与处理看起来像输入的对象（在深度学习中）相比相对容易一些。</p><p>病因（要预测的诊断结果）导致 症状（观察到的结果）。  </p><p>训练数据集，数据很少只包含流感p(y)的样本。  </p><p>而测试数据集有流感p(y)和流感q(y)，其中不变的是流感症状p(x|y)。</p><h2 id="概念偏移"><a href="#概念偏移" class="headerlink" title="概念偏移"></a>概念偏移</h2><p>另一个相关的问题出现在概念转换中，即标签本身的定义发生变化的情况。这听起来很奇怪，毕竟猫就是猫。的确，猫的定义可能不会改变，但我们能不能对软饮料也这么说呢？事实证明，如果我们周游美国，按地理位置转移数据来源，我们会发现，即使是如图所示的这个简单术语的定义也会发生相当大的概念转变。</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/XZM92uq4EvNQmYT.png" alt="美国软饮料名称的概念转变"></p><p>如果我们要建立一个机器翻译系统，分布P（y∣x）可能因我们的位置而异。这个问题很难发现。另一个可取之处是P（y∣x）通常只是逐渐变化。</p><h1 id="Kaggle-房价预测实战"><a href="#Kaggle-房价预测实战" class="headerlink" title="Kaggle 房价预测实战"></a>Kaggle 房价预测实战</h1><p>作为深度学习基础篇章的总结，我们将对本章内容学以致用。下面，让我们动手实战一个Kaggle比赛：房价预测。本节将提供未经调优的数据的预处理、模型的设计和超参数的选择。我们希望读者通过动手操作、仔细观察实验现象、认真分析实验结果并不断调整方法，得到令自己满意的结果。</p><pre><code class="lang-python">%matplotlib inlineimport torchimport torch.nn as nnimport numpy as npimport pandas as pdimport syssys.path.append(&quot;/home/kesci/input&quot;)import d2lzh1981 as d2lprint(torch.__version__)torch.set_default_tensor_type(torch.FloatTensor)</code></pre><pre><code>1.3.0</code></pre><h2 id="获取和读取数据集"><a href="#获取和读取数据集" class="headerlink" title="获取和读取数据集"></a>获取和读取数据集</h2><p>比赛数据分为训练数据集和测试数据集。两个数据集都包括每栋房子的特征，如街道类型、建造年份、房顶类型、地下室状况等特征值。这些特征值有连续的数字、离散的标签甚至是缺失值“na”。只有训练数据集包括了每栋房子的价格，也就是标签。我们可以访问比赛网页，点击“Data”标签，并下载这些数据集。</p><p>我们将通过<code>pandas</code>库读入并处理数据。在导入本节需要的包前请确保已安装<code>pandas</code>库。<br>假设解压后的数据位于<code>/home/kesci/input/houseprices2807/</code>目录，它包括两个csv文件。下面使用<code>pandas</code>读取这两个文件。</p><pre><code class="lang-python">test_data = pd.read_csv(&quot;/home/kesci/input/houseprices2807/house-prices-advanced-regression-techniques/test.csv&quot;)train_data = pd.read_csv(&quot;/home/kesci/input/houseprices2807/house-prices-advanced-regression-techniques/train.csv&quot;)</code></pre><p>训练数据集包括1460个样本、80个特征和1个标签。</p><pre><code class="lang-python">train_data.shape</code></pre><pre><code>(1460, 81)</code></pre><p>测试数据集包括1459个样本和80个特征。我们需要将测试数据集中每个样本的标签预测出来。</p><pre><code class="lang-python">test_data.shape</code></pre><pre><code>(1459, 80)</code></pre><p>让我们来查看前4个样本的前4个特征、后2个特征和标签（SalePrice）：</p><pre><code class="lang-python">train_data.iloc[0:4, [0, 1, 2, 3, -3, -2, -1]]</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>Id</th>      <th>MSSubClass</th>      <th>MSZoning</th>      <th>LotFrontage</th>      <th>SaleType</th>      <th>SaleCondition</th>      <th>SalePrice</th>    </tr>  </thead>  <tbody>    <tr>      <td>0</td>      <td>1</td>      <td>60</td>      <td>RL</td>      <td>65.0</td>      <td>WD</td>      <td>Normal</td>      <td>208500</td>    </tr>    <tr>      <td>1</td>      <td>2</td>      <td>20</td>      <td>RL</td>      <td>80.0</td>      <td>WD</td>      <td>Normal</td>      <td>181500</td>    </tr>    <tr>      <td>2</td>      <td>3</td>      <td>60</td>      <td>RL</td>      <td>68.0</td>      <td>WD</td>      <td>Normal</td>      <td>223500</td>    </tr>    <tr>      <td>3</td>      <td>4</td>      <td>70</td>      <td>RL</td>      <td>60.0</td>      <td>WD</td>      <td>Abnorml</td>      <td>140000</td>    </tr>  </tbody></table></div><p>可以看到第一个特征是Id，它能帮助模型记住每个训练样本，但难以推广到测试样本，所以我们不使用它来训练。我们将所有的训练数据和测试数据的79个特征按样本连结。</p><pre><code class="lang-python">all_features = pd.concat((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:]))</code></pre><h2 id="预处理数据"><a href="#预处理数据" class="headerlink" title="预处理数据"></a>预处理数据</h2><p>我们对连续数值的特征做标准化（standardization）：设该特征在整个数据集上的均值为$\mu$，标准差为$\sigma$。那么，我们可以将该特征的每个值先减去$\mu$再除以$\sigma$得到标准化后的每个特征值。对于缺失的特征值，我们将其替换成该特征的均值。</p><pre><code class="lang-python">numeric_features = all_features.dtypes[all_features.dtypes != &#39;object&#39;].indexall_features[numeric_features] = all_features[numeric_features].apply(    lambda x: (x - x.mean()) / (x.std()))# 标准化后，每个数值特征的均值变为0，所以可以直接用0来替换缺失值all_features[numeric_features] = all_features[numeric_features].fillna(0)</code></pre><p>接下来将离散数值转成指示特征。举个例子，假设特征MSZoning里面有两个不同的离散值RL和RM，那么这一步转换将去掉MSZoning特征，并新加两个特征MSZoning_RL和MSZoning_RM，其值为0或1。如果一个样本原来在MSZoning里的值为RL，那么有MSZoning_RL=1且MSZoning_RM=0。</p><pre><code class="lang-python"># dummy_na=True将缺失值也当作合法的特征值并为其创建指示特征all_features = pd.get_dummies(all_features, dummy_na=True)all_features.shape</code></pre><pre><code>(2919, 331)</code></pre><p>可以看到这一步转换将特征数从79增加到了331。</p><p>最后，通过<code>values</code>属性得到NumPy格式的数据，并转成<code>Tensor</code>方便后面的训练。</p><pre><code class="lang-python">n_train = train_data.shape[0]train_features = torch.tensor(all_features[:n_train].values, dtype=torch.float)test_features = torch.tensor(all_features[n_train:].values, dtype=torch.float)train_labels = torch.tensor(train_data.SalePrice.values, dtype=torch.float).view(-1, 1)</code></pre><h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><pre><code class="lang-python">loss = torch.nn.MSELoss()def get_net(feature_num):    net = nn.Linear(feature_num, 1)    for param in net.parameters():        nn.init.normal_(param, mean=0, std=0.01)    return net</code></pre><p>下面定义比赛用来评价模型的对数均方根误差。给定预测值$\hat y_1, \ldots, \hat y_n$和对应的真实标签$y_1,\ldots, y_n$，它的定义为</p><script type="math/tex; mode=display">\sqrt{\frac{1}{n}\sum_{i=1}^n\left(\log(y_i)-\log(\hat y_i)\right)^2}.</script><p>对数均方根误差的实现如下。</p><pre><code class="lang-python">def log_rmse(net, features, labels):    with torch.no_grad():        # 将小于1的值设成1，使得取对数时数值更稳定        clipped_preds = torch.max(net(features), torch.tensor(1.0))        rmse = torch.sqrt(2 * loss(clipped_preds.log(), labels.log()).mean())    return rmse.item()</code></pre><p>下面的训练函数跟本章中前几节的不同在于使用了Adam优化算法。相对之前使用的小批量随机梯度下降，它对学习率相对不那么敏感。我们将在之后的“优化算法”一章里详细介绍它。</p><pre><code class="lang-python">def train(net, train_features, train_labels, test_features, test_labels,          num_epochs, learning_rate, weight_decay, batch_size):    train_ls, test_ls = [], []    dataset = torch.utils.data.TensorDataset(train_features, train_labels)    train_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True)    # 这里使用了Adam优化算法    optimizer = torch.optim.Adam(params=net.parameters(), lr=learning_rate, weight_decay=weight_decay)     net = net.float()    for epoch in range(num_epochs):        for X, y in train_iter:            l = loss(net(X.float()), y.float())            optimizer.zero_grad()            l.backward()            optimizer.step()        train_ls.append(log_rmse(net, train_features, train_labels))        if test_labels is not None:            test_ls.append(log_rmse(net, test_features, test_labels))    return train_ls, test_ls</code></pre><h2 id="K折交叉验证"><a href="#K折交叉验证" class="headerlink" title="K折交叉验证"></a>K折交叉验证</h2><p>我们在模型选择、欠拟合和过拟合中介绍了$K$折交叉验证。它将被用来选择模型设计并调节超参数。下面实现了一个函数，它返回第<code>i</code>折交叉验证时所需要的训练和验证数据。</p><pre><code class="lang-python">def get_k_fold_data(k, i, X, y):    # 返回第i折交叉验证时所需要的训练和验证数据    assert k &gt; 1    fold_size = X.shape[0] // k    X_train, y_train = None, None    for j in range(k):        idx = slice(j * fold_size, (j + 1) * fold_size)        X_part, y_part = X[idx, :], y[idx]        if j == i:            X_valid, y_valid = X_part, y_part        elif X_train is None:            X_train, y_train = X_part, y_part        else:            X_train = torch.cat((X_train, X_part), dim=0)            y_train = torch.cat((y_train, y_part), dim=0)    return X_train, y_train, X_valid, y_valid</code></pre><p>在$K$折交叉验证中我们训练$K$次并返回训练和验证的平均误差</p><pre><code class="lang-python">def k_fold(k, X_train, y_train, num_epochs,           learning_rate, weight_decay, batch_size):    train_l_sum, valid_l_sum = 0, 0    for i in range(k):        data = get_k_fold_data(k, i, X_train, y_train)        net = get_net(X_train.shape[1])        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate,                                   weight_decay, batch_size)        train_l_sum += train_ls[-1]        valid_l_sum += valid_ls[-1]        if i == 0:            d2l.semilogy(range(1, num_epochs + 1), train_ls, &#39;epochs&#39;, &#39;rmse&#39;,                         range(1, num_epochs + 1), valid_ls,                         [&#39;train&#39;, &#39;valid&#39;])        print(&#39;fold %d, train rmse %f, valid rmse %f&#39; % (i, train_ls[-1], valid_ls[-1]))    return train_l_sum / k, valid_l_sum / k</code></pre><h2 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h2><p>我们使用一组未经调优的超参数并计算交叉验证误差。可以改动这些超参数来尽可能减小平均测试误差。<br>有时候你会发现一组参数的训练误差可以达到很低，但是在$K$折交叉验证上的误差可能反而较高。这种现象很可能是由过拟合造成的。因此，当训练误差降低时，我们要观察$K$折交叉验证上的误差是否也相应降低。</p><pre><code class="lang-python">k, num_epochs, lr, weight_decay, batch_size = 5, 100, 5, 0, 64train_l, valid_l = k_fold(k, train_features, train_labels, num_epochs, lr, weight_decay, batch_size)print(&#39;%d-fold validation: avg train rmse %f, avg valid rmse %f&#39; % (k, train_l, valid_l))</code></pre><pre><code>fold 0, train rmse 0.241365, valid rmse 0.223083fold 1, train rmse 0.229118, valid rmse 0.267488fold 2, train rmse 0.232072, valid rmse 0.237995fold 3, train rmse 0.238050, valid rmse 0.218671fold 4, train rmse 0.231004, valid rmse 0.2591855-fold validation: avg train rmse 0.234322, avg valid rmse 0.241284</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://s2.ax1x.com/2020/02/19/3VfyOs.png" alt="损失不断下降"></p><h1 id="预测并在Kaggle中提交结果"><a href="#预测并在Kaggle中提交结果" class="headerlink" title="预测并在Kaggle中提交结果"></a>预测并在Kaggle中提交结果</h1><p>下面定义预测函数。在预测之前，我们会使用完整的训练数据集来重新训练模型，并将预测结果存成提交所需要的格式。</p><pre><code class="lang-python">def train_and_pred(train_features, test_features, train_labels, test_data,                   num_epochs, lr, weight_decay, batch_size):    net = get_net(train_features.shape[1])    train_ls, _ = train(net, train_features, train_labels, None, None,                        num_epochs, lr, weight_decay, batch_size)    d2l.semilogy(range(1, num_epochs + 1), train_ls, &#39;epochs&#39;, &#39;rmse&#39;)    print(&#39;train rmse %f&#39; % train_ls[-1])    preds = net(test_features).detach().numpy()    test_data[&#39;SalePrice&#39;] = pd.Series(preds.reshape(1, -1)[0])    submission = pd.concat([test_data[&#39;Id&#39;], test_data[&#39;SalePrice&#39;]], axis=1)    submission.to_csv(&#39;./submission.csv&#39;, index=False)    # sample_submission_data = pd.read_csv(&quot;../input/house-prices-advanced-regression-techniques/sample_submission.csv&quot;)</code></pre><p>设计好模型并调好超参数之后，下一步就是对测试数据集上的房屋样本做价格预测。如果我们得到与交叉验证时差不多的训练误差，那么这个结果很可能是理想的，可以在Kaggle上提交结果。</p><pre><code class="lang-python">train_and_pred(train_features, test_features, train_labels, test_data, num_epochs, lr, weight_decay, batch_size)</code></pre><p>希望大家自己动手完成房价预测的实现，多参与讨论。</p><h1 id="练习题"><a href="#练习题" class="headerlink" title="练习题"></a>练习题</h1><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/GJjuZ5mLcKX7Hsd.png" alt="(1)"></p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/qAFfVdXbyMgrHkj.png" alt="(2)"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动手深度学习(四):过拟合与欠拟合</title>
      <link href="/2020/02/14/dl-notes4-ov-und-fit/"/>
      <url>/2020/02/14/dl-notes4-ov-und-fit/</url>
      
        <content type="html"><![CDATA[<h1 id="过拟合、欠拟合及其解决方案"><a href="#过拟合、欠拟合及其解决方案" class="headerlink" title="过拟合、欠拟合及其解决方案"></a>过拟合、欠拟合及其解决方案</h1><ol><li>过拟合、欠拟合的概念</li><li>权重衰减</li><li>丢弃法</li></ol><h1 id="模型选择、过拟合和欠拟合"><a href="#模型选择、过拟合和欠拟合" class="headerlink" title="模型选择、过拟合和欠拟合"></a>模型选择、过拟合和欠拟合</h1><h2 id="训练误差和泛化误差"><a href="#训练误差和泛化误差" class="headerlink" title="训练误差和泛化误差"></a>训练误差和泛化误差</h2><p>在解释上述现象之前，我们需要区分训练误差（training error）和泛化误差（generalization error）。通俗来讲，前者指模型在训练数据集上表现出的误差，后者指模型在任意一个测试数据样本上表现出的误差的期望，并常常通过测试数据集上的误差来近似。计算训练误差和泛化误差可以使用之前介绍过的损失函数，例如线性回归用到的平方损失函数和softmax回归用到的交叉熵损失函数。</p><p>机器学习模型应关注降低泛化误差。</p><h2 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h2><h3 id="验证数据集"><a href="#验证数据集" class="headerlink" title="验证数据集"></a>验证数据集</h3><p>从严格意义上讲，测试集只能在所有超参数和模型参数选定后使用一次。不可以使用测试数据选择模型，如调参。由于无法从训练误差估计泛化误差，因此也不应只依赖训练数据选择模型。鉴于此，我们可以预留一部分在训练数据集和测试数据集以外的数据来进行模型选择。这部分数据被称为验证数据集，简称验证集（validation set）。例如，我们可以从给定的训练集中随机选取一小部分作为验证集，而将剩余部分作为真正的训练集。</p><h3 id="K折交叉验证"><a href="#K折交叉验证" class="headerlink" title="K折交叉验证"></a>K折交叉验证</h3><p>由于验证数据集不参与模型训练，当训练数据不够用时，预留大量的验证数据显得太奢侈。一种改善的方法是K折交叉验证（K-fold cross-validation）。在K折交叉验证中，我们把原始训练数据集分割成K个不重合的子数据集，然后我们做K次模型训练和验证。每一次，我们使用一个子数据集验证模型，并使用其他K-1个子数据集来训练模型。在这K次训练和验证中，每次用来验证模型的子数据集都不同。最后，我们对这K次训练误差和验证误差分别求平均。</p><h2 id="过拟合和欠拟合"><a href="#过拟合和欠拟合" class="headerlink" title="过拟合和欠拟合"></a>过拟合和欠拟合</h2><p>接下来，我们将探究模型训练中经常出现的两类典型问题：</p><ul><li>一类是模型无法得到较低的训练误差，我们将这一现象称作欠拟合（underfitting）；</li><li>另一类是模型的训练误差远小于它在测试数据集上的误差，我们称该现象为过拟合（overfitting）。<br>在实践中，我们要尽可能同时应对欠拟合和过拟合。虽然有很多因素可能导致这两种拟合问题，在这里我们重点讨论两个因素：模型复杂度和训练数据集大小。</li></ul><h3 id="模型复杂度"><a href="#模型复杂度" class="headerlink" title="模型复杂度"></a>模型复杂度</h3><p>为了解释模型复杂度，我们以多项式函数拟合为例。给定一个由标量数据特征$x$和对应的标量标签$y$组成的训练数据集，多项式函数拟合的目标是找一个$K$阶多项式函数</p><script type="math/tex; mode=display"> \hat{y} = b + \sum_{k=1}^K x^k w_k</script><p>来近似 $y$。在上式中，$w_k$是模型的权重参数，$b$是偏差参数。与线性回归相同，多项式函数拟合也使用平方损失函数。特别地，一阶多项式函数拟合又叫线性函数拟合。</p><p>给定训练数据集，模型复杂度和误差之间的关系：</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/upload/image/q5jc27wxoj.png?imageView2/0/w/960/h/960" alt="Image Name"></p><h3 id="训练数据集大小"><a href="#训练数据集大小" class="headerlink" title="训练数据集大小"></a>训练数据集大小</h3><p>影响欠拟合和过拟合的另一个重要因素是训练数据集的大小。一般来说，如果训练数据集中样本数过少，特别是比模型参数数量（按元素计）更少时，过拟合更容易发生。此外，泛化误差不会随训练数据集里样本数量增加而增大。因此，在计算资源允许的范围之内，我们通常希望训练数据集大一些，特别是在模型复杂度较高时，例如层数较多的深度学习模型。</p><h1 id="多项式函数拟合实验"><a href="#多项式函数拟合实验" class="headerlink" title="多项式函数拟合实验"></a>多项式函数拟合实验</h1><pre><code class="lang-python">%matplotlib inlineimport torchimport numpy as npimport syssys.path.append(&quot;/home/kesci/input&quot;)import d2lzh1981 as d2lprint(torch.__version__)</code></pre><pre><code>1.3.0</code></pre><h2 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h2><pre><code class="lang-python">n_train, n_test, true_w, true_b = 100, 100, [1.2, -3.4, 5.6], 5features = torch.randn((n_train + n_test, 1))poly_features = torch.cat((features, torch.pow(features, 2), torch.pow(features, 3)), 1) labels = (true_w[0] * poly_features[:, 0] + true_w[1] * poly_features[:, 1]          + true_w[2] * poly_features[:, 2] + true_b)labels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float)</code></pre><pre><code class="lang-python">features[:2], poly_features[:2], labels[:2]</code></pre><pre><code>(tensor([[-0.8589],         [-0.2534]]), tensor([[-0.8589,  0.7377, -0.6335],         [-0.2534,  0.0642, -0.0163]]), tensor([-2.0794,  4.4039]))</code></pre><h2 id="定义、训练和测试模型"><a href="#定义、训练和测试模型" class="headerlink" title="定义、训练和测试模型"></a>定义、训练和测试模型</h2><pre><code class="lang-python">def semilogy(x_vals, y_vals, x_label, y_label, x2_vals=None, y2_vals=None,             legend=None, figsize=(3.5, 2.5)):    # d2l.set_figsize(figsize)    d2l.plt.xlabel(x_label)    d2l.plt.ylabel(y_label)    d2l.plt.semilogy(x_vals, y_vals)    if x2_vals and y2_vals:        d2l.plt.semilogy(x2_vals, y2_vals, linestyle=&#39;:&#39;)        d2l.plt.legend(legend)</code></pre><pre><code class="lang-python">num_epochs, loss = 100, torch.nn.MSELoss()def fit_and_plot(train_features, test_features, train_labels, test_labels):    # 初始化网络模型    net = torch.nn.Linear(train_features.shape[-1], 1)    # 通过Linear文档可知，pytorch已经将参数初始化了，所以我们这里就不手动初始化了    # 设置批量大小    batch_size = min(10, train_labels.shape[0])        dataset = torch.utils.data.TensorDataset(train_features, train_labels)      # 设置数据集    train_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True) # 设置获取数据方式    optimizer = torch.optim.SGD(net.parameters(), lr=0.01)                      # 设置优化函数，使用的是随机梯度下降优化    train_ls, test_ls = [], []    for _ in range(num_epochs):        for X, y in train_iter:                                                 # 取一个批量的数据            l = loss(net(X), y.view(-1, 1))                                     # 输入到网络中计算输出，并和标签比较求得损失函数            optimizer.zero_grad()                                               # 梯度清零，防止梯度累加干扰优化            l.backward()                                                        # 求梯度            optimizer.step()                                                    # 迭代优化函数，进行参数优化        train_labels = train_labels.view(-1, 1)        test_labels = test_labels.view(-1, 1)        train_ls.append(loss(net(train_features), train_labels).item())         # 将训练损失保存到train_ls中        test_ls.append(loss(net(test_features), test_labels).item())            # 将测试损失保存到test_ls中    print(&#39;final epoch: train loss&#39;, train_ls[-1], &#39;test loss&#39;, test_ls[-1])        semilogy(range(1, num_epochs + 1), train_ls, &#39;epochs&#39;, &#39;loss&#39;,             range(1, num_epochs + 1), test_ls, [&#39;train&#39;, &#39;test&#39;])    print(&#39;weight:&#39;, net.weight.data,          &#39;\nbias:&#39;, net.bias.data)</code></pre><h2 id="三阶多项式函数拟合（正常）"><a href="#三阶多项式函数拟合（正常）" class="headerlink" title="三阶多项式函数拟合（正常）"></a>三阶多项式函数拟合（正常）</h2><pre><code class="lang-python">fit_and_plot(poly_features[:n_train, :], poly_features[n_train:, :], labels[:n_train], labels[n_train:])</code></pre><pre><code>final epoch: train loss 8887.298828125 test loss 1145.94287109375weight: tensor([[-8.5120, 19.0351, 12.8616]]) bias: tensor([-5.4607])</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/CD685B472B744329A1CFC47C9F0B5E89/q5jf5azjcn.png"></p><h2 id="线性函数拟合（欠拟合）"><a href="#线性函数拟合（欠拟合）" class="headerlink" title="线性函数拟合（欠拟合）"></a>线性函数拟合（欠拟合）</h2><pre><code class="lang-python">fit_and_plot(features[:n_train, :], features[n_train:, :], labels[:n_train], labels[n_train:])</code></pre><pre><code>final epoch: train loss 781.689453125 test loss 329.79852294921875weight: tensor([[26.8753]]) bias: tensor([6.1426])</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/33AD626DA0B94DB7A28D47697312B45D/q5jf5al2tv.png"></p><h2 id="训练样本不足（过拟合）"><a href="#训练样本不足（过拟合）" class="headerlink" title="训练样本不足（过拟合）"></a>训练样本不足（过拟合）</h2><pre><code class="lang-python">fit_and_plot(poly_features[0:2, :], poly_features[n_train:, :], labels[0:2], labels[n_train:])</code></pre><pre><code>final epoch: train loss 6.23520565032959 test loss 409.9844665527344weight: tensor([[ 0.9729, -0.9612,  0.7259]]) bias: tensor([1.6334])</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/AB13F65A70A9484788F8004E427EC290/q5jf5bd11u.png"></p><h1 id="权重衰减"><a href="#权重衰减" class="headerlink" title="权重衰减"></a>权重衰减</h1><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>权重衰减等价于 $L_2$ 范数正则化（regularization）。正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常用手段。</p><h2 id="L2-范数正则化（regularization）"><a href="#L2-范数正则化（regularization）" class="headerlink" title="L2 范数正则化（regularization）"></a>L2 范数正则化（regularization）</h2><p>$L_2$范数正则化在模型原损失函数基础上添加$L_2$范数惩罚项，从而得到训练所需要最小化的函数。$L_2$范数惩罚项指的是模型权重参数每个元素的平方和与一个正的常数的乘积。以线性回归中的线性回归损失函数为例</p><script type="math/tex; mode=display"> \ell(w_1, w_2, b) = \frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right)^2</script><p>其中$w_1, w_2$是权重参数，$b$是偏差参数，样本$i$的输入为$x_1^{(i)}, x_2^{(i)}$，标签为$y^{(i)}$，样本数为$n$。将权重参数用向量$\boldsymbol{w} = [w_1, w_2]$表示，带有$L_2$范数惩罚项的新损失函数为</p><script type="math/tex; mode=display">\ell(w_1, w_2, b) + \frac{\lambda}{2n} |\boldsymbol{w}|^2,</script><p>其中超参数$\lambda &gt; 0$。当权重参数均为0时，惩罚项最小。当$\lambda$较大时，惩罚项在损失函数中的比重较大，这通常会使学到的权重参数的元素较接近0。当$\lambda$设为0时，惩罚项完全不起作用。上式中$L_2$范数平方$|\boldsymbol{w}|^2$展开后得到$w_1^2 + w_2^2$。<br>有了$L_2$范数惩罚项后，在小批量随机梯度下降中，我们将线性回归一节中权重$w_1$和$w_2$的迭代方式更改为</p><script type="math/tex; mode=display"> \begin{aligned} w_1 &\leftarrow \left(1- \frac{\eta\lambda}{|\mathcal{B}|} \right)w_1 - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_1^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right),\\ w_2 &\leftarrow \left(1- \frac{\eta\lambda}{|\mathcal{B}|} \right)w_2 - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_2^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right). \end{aligned}</script><p>可见，$L_2$范数正则化令权重$w_1$和$w_2$先自乘小于1的数，再减去不含惩罚项的梯度。因此，$L_2$范数正则化又叫权重衰减。权重衰减通过惩罚绝对值较大的模型参数为需要学习的模型增加了限制，这可能对过拟合有效。</p><h2 id="高维线性回归实验从零开始的实现"><a href="#高维线性回归实验从零开始的实现" class="headerlink" title="高维线性回归实验从零开始的实现"></a>高维线性回归实验从零开始的实现</h2><p>下面，我们以高维线性回归为例来引入一个过拟合问题，并使用权重衰减来应对过拟合。设数据样本特征的维度为$p$。对于训练数据集和测试数据集中特征为$x_1, x_2, \ldots, x_p$的任一样本，我们使用如下的线性函数来生成该样本的标签：</p><script type="math/tex; mode=display"> y = 0.05 + \sum_{i = 1}^p 0.01x_i + \epsilon</script><p>其中噪声项$\epsilon$服从均值为0、标准差为0.01的正态分布。为了较容易地观察过拟合，我们考虑高维线性回归问题，如设维度$p=200$；同时，我们特意把训练数据集的样本数设低，如20。</p><pre><code class="lang-python">%matplotlib inlineimport torchimport torch.nn as nnimport numpy as npimport syssys.path.append(&quot;/home/kesci/input&quot;)import d2lzh1981 as d2lprint(torch.__version__)</code></pre><pre><code>1.3.0</code></pre><h2 id="初始化模型参数-1"><a href="#初始化模型参数-1" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h2><p>与前面观察过拟合和欠拟合现象的时候相似，在这里不再解释。</p><pre><code class="lang-python">n_train, n_test, num_inputs = 20, 100, 200true_w, true_b = torch.ones(num_inputs, 1) * 0.01, 0.05features = torch.randn((n_train + n_test, num_inputs))labels = torch.matmul(features, true_w) + true_blabels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float)train_features, test_features = features[:n_train, :], features[n_train:, :]train_labels, test_labels = labels[:n_train], labels[n_train:]</code></pre><pre><code class="lang-python"># 定义参数初始化函数，初始化模型参数并且附上梯度def init_params():    w = torch.randn((num_inputs, 1), requires_grad=True)    b = torch.zeros(1, requires_grad=True)    return [w, b]</code></pre><h2 id="定义L2范数惩罚项"><a href="#定义L2范数惩罚项" class="headerlink" title="定义L2范数惩罚项"></a>定义L2范数惩罚项</h2><pre><code class="lang-python">def l2_penalty(w):    return (w**2).sum() / 2</code></pre><h2 id="定义训练和测试"><a href="#定义训练和测试" class="headerlink" title="定义训练和测试"></a>定义训练和测试</h2><pre><code class="lang-python">batch_size, num_epochs, lr = 1, 100, 0.003net, loss = d2l.linreg, d2l.squared_lossdataset = torch.utils.data.TensorDataset(train_features, train_labels)train_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True)def fit_and_plot(lambd):    w, b = init_params()    train_ls, test_ls = [], []    for _ in range(num_epochs):        for X, y in train_iter:            # 添加了L2范数惩罚项            l = loss(net(X, w, b), y) + lambd * l2_penalty(w)            l = l.sum()            if w.grad is not None:                w.grad.data.zero_()                b.grad.data.zero_()            l.backward()            d2l.sgd([w, b], lr, batch_size)        train_ls.append(loss(net(train_features, w, b), train_labels).mean().item())        test_ls.append(loss(net(test_features, w, b), test_labels).mean().item())    d2l.semilogy(range(1, num_epochs + 1), train_ls, &#39;epochs&#39;, &#39;loss&#39;,                 range(1, num_epochs + 1), test_ls, [&#39;train&#39;, &#39;test&#39;])    print(&#39;L2 norm of w:&#39;, w.norm().item())</code></pre><h2 id="观察过拟合"><a href="#观察过拟合" class="headerlink" title="观察过拟合"></a>观察过拟合</h2><pre><code class="lang-python">fit_and_plot(lambd=0)</code></pre><pre><code>L2 norm of w: 11.6444091796875</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/C27406AAA0FD41C6801D55ED4B25D5EA/q5jf5cs7lp.svg"></p><h2 id="使用权重衰减"><a href="#使用权重衰减" class="headerlink" title="使用权重衰减"></a>使用权重衰减</h2><pre><code class="lang-python">fit_and_plot(lambd=3)</code></pre><pre><code>L2 norm of w: 0.04063604772090912</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/0770D8C23B8144C59D13D24390E471F0/q5jf5d4mwp.svg"></p><h2 id="简洁实现"><a href="#简洁实现" class="headerlink" title="简洁实现"></a>简洁实现</h2><pre><code class="lang-python">def fit_and_plot_pytorch(wd):    # 对权重参数衰减。权重名称一般是以weight结尾    net = nn.Linear(num_inputs, 1)    nn.init.normal_(net.weight, mean=0, std=1)    nn.init.normal_(net.bias, mean=0, std=1)    optimizer_w = torch.optim.SGD(params=[net.weight], lr=lr, weight_decay=wd) # 对权重参数衰减    optimizer_b = torch.optim.SGD(params=[net.bias], lr=lr)  # 不对偏差参数衰减    train_ls, test_ls = [], []    for _ in range(num_epochs):        for X, y in train_iter:            l = loss(net(X), y).mean()            optimizer_w.zero_grad()            optimizer_b.zero_grad()            l.backward()            # 对两个optimizer实例分别调用step函数，从而分别更新权重和偏差            optimizer_w.step()            optimizer_b.step()        train_ls.append(loss(net(train_features), train_labels).mean().item())        test_ls.append(loss(net(test_features), test_labels).mean().item())    d2l.semilogy(range(1, num_epochs + 1), train_ls, &#39;epochs&#39;, &#39;loss&#39;,                 range(1, num_epochs + 1), test_ls, [&#39;train&#39;, &#39;test&#39;])    print(&#39;L2 norm of w:&#39;, net.weight.data.norm().item())</code></pre><pre><code class="lang-python">fit_and_plot_pytorch(0)</code></pre><pre><code>L2 norm of w: 13.361410140991211</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/525D01167F0E40509495588D6B0A0FB9/q5jf5e5i21.svg"></p><pre><code class="lang-python">fit_and_plot_pytorch(3)</code></pre><pre><code>L2 norm of w: 0.051789578050374985</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/3FAACA854B9545A8ADADDEB6EE17A680/q5jf5fa51u.svg"></p><h1 id="丢弃法"><a href="#丢弃法" class="headerlink" title="丢弃法"></a>丢弃法</h1><p>多层感知机中神经网络图描述了一个单隐藏层的多层感知机。其中输入个数为4，隐藏单元个数为5，且隐藏单元$h_i$（$i=1, \ldots, 5$）的计算表达式为</p><script type="math/tex; mode=display"> h_i = \phi\left(x_1 w_{1i} + x_2 w_{2i} + x_3 w_{3i} + x_4 w_{4i} + b_i\right)</script><p>这里$\phi$是激活函数，$x_1, \ldots, x_4$是输入，隐藏单元$i$的权重参数为$w_{1i}, \ldots, w_{4i}$，偏差参数为$b_i$。当对该隐藏层使用丢弃法时，该层的隐藏单元将有一定概率被丢弃掉。设丢弃概率为$p$，那么有$p$的概率$h_i$会被清零，有$1-p$的概率$h_i$会除以$1-p$做拉伸。丢弃概率是丢弃法的超参数。具体来说，设随机变量$\xi_i$为0和1的概率分别为$p$和$1-p$。使用丢弃法时我们计算新的隐藏单元$h_i’$</p><script type="math/tex; mode=display"> h_i' = \frac{\xi_i}{1-p} h_i</script><p>由于$E(\xi_i) = 1-p$，因此</p><script type="math/tex; mode=display"> E(h_i') = \frac{E(\xi_i)}{1-p}h_i = h_i</script><p>即丢弃法不改变其输入的期望值。让我们对之前多层感知机的神经网络中的隐藏层使用丢弃法，一种可能的结果如图所示，其中$h_2$和$h_5$被清零。这时输出值的计算不再依赖$h_2$和$h_5$，在反向传播时，与这两个隐藏单元相关的权重的梯度均为0。由于在训练中隐藏层神经元的丢弃是随机的，即$h_1, \ldots, h_5$都有可能被清零，输出层的计算无法过度依赖$h_1, \ldots, h_5$中的任一个，从而在训练模型时起到正则化的作用，并可以用来应对过拟合。在测试模型时，我们为了拿到更加确定性的结果，一般不使用丢弃法</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/upload/image/q5jd69in3m.png?imageView2/0/w/960/h/960" alt="Image Name"></p><h2 id="丢弃法从零开始的实现"><a href="#丢弃法从零开始的实现" class="headerlink" title="丢弃法从零开始的实现"></a>丢弃法从零开始的实现</h2><pre><code class="lang-python">%matplotlib inlineimport torchimport torch.nn as nnimport numpy as npimport syssys.path.append(&quot;/home/kesci/input&quot;)import d2lzh1981 as d2lprint(torch.__version__)</code></pre><pre><code>1.3.0</code></pre><pre><code class="lang-python">def dropout(X, drop_prob):    X = X.float()    assert 0 &lt;= drop_prob &lt;= 1    keep_prob = 1 - drop_prob    # 这种情况下把全部元素都丢弃    if keep_prob == 0:        return torch.zeros_like(X)    mask = (torch.rand(X.shape) &lt; keep_prob).float()    return mask * X / keep_prob</code></pre><pre><code class="lang-python">X = torch.arange(16).view(2, 8)dropout(X, 0)</code></pre><pre><code>tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],        [ 8.,  9., 10., 11., 12., 13., 14., 15.]])</code></pre><pre><code class="lang-python">dropout(X, 0.5)</code></pre><pre><code>tensor([[ 0.,  0.,  0.,  6.,  8., 10.,  0., 14.],        [ 0.,  0., 20.,  0.,  0.,  0., 28.,  0.]])</code></pre><pre><code class="lang-python">dropout(X, 1.0)</code></pre><pre><code>tensor([[0., 0., 0., 0., 0., 0., 0., 0.],        [0., 0., 0., 0., 0., 0., 0., 0.]])</code></pre><pre><code class="lang-python"># 参数的初始化num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256W1 = torch.tensor(np.random.normal(0, 0.01, size=(num_inputs, num_hiddens1)), dtype=torch.float, requires_grad=True)b1 = torch.zeros(num_hiddens1, requires_grad=True)W2 = torch.tensor(np.random.normal(0, 0.01, size=(num_hiddens1, num_hiddens2)), dtype=torch.float, requires_grad=True)b2 = torch.zeros(num_hiddens2, requires_grad=True)W3 = torch.tensor(np.random.normal(0, 0.01, size=(num_hiddens2, num_outputs)), dtype=torch.float, requires_grad=True)b3 = torch.zeros(num_outputs, requires_grad=True)params = [W1, b1, W2, b2, W3, b3]</code></pre><pre><code class="lang-python">drop_prob1, drop_prob2 = 0.2, 0.5def net(X, is_training=True):    X = X.view(-1, num_inputs)    H1 = (torch.matmul(X, W1) + b1).relu()    if is_training:  # 只在训练模型时使用丢弃法        H1 = dropout(H1, drop_prob1)  # 在第一层全连接后添加丢弃层    H2 = (torch.matmul(H1, W2) + b2).relu()    if is_training:        H2 = dropout(H2, drop_prob2)  # 在第二层全连接后添加丢弃层    return torch.matmul(H2, W3) + b3</code></pre><pre><code class="lang-python">def evaluate_accuracy(data_iter, net):    acc_sum, n = 0.0, 0    for X, y in data_iter:        if isinstance(net, torch.nn.Module):            net.eval() # 评估模式, 这会关闭dropout            acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()            net.train() # 改回训练模式        else: # 自定义的模型            if(&#39;is_training&#39; in net.__code__.co_varnames): # 如果有is_training这个参数                # 将is_training设置成False                acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item()             else:                acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()         n += y.shape[0]    return acc_sum / n</code></pre><pre><code class="lang-python">num_epochs, lr, batch_size = 5, 100.0, 256  # 这里的学习率设置的很大，原因与之前相同。loss = torch.nn.CrossEntropyLoss()train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, root=&#39;/home/kesci/input/FashionMNIST2065&#39;)d2l.train_ch3(    net,    train_iter,    test_iter,    loss,    num_epochs,    batch_size,    params,    lr)</code></pre><pre><code>epoch 1, loss 0.0046, train acc 0.549, test acc 0.704epoch 2, loss 0.0023, train acc 0.785, test acc 0.737epoch 3, loss 0.0019, train acc 0.825, test acc 0.834epoch 4, loss 0.0017, train acc 0.842, test acc 0.763epoch 5, loss 0.0016, train acc 0.848, test acc 0.813</code></pre><h2 id="简洁实现-1"><a href="#简洁实现-1" class="headerlink" title="简洁实现"></a>简洁实现</h2><pre><code class="lang-python">net = nn.Sequential(        d2l.FlattenLayer(),        nn.Linear(num_inputs, num_hiddens1),        nn.ReLU(),        nn.Dropout(drop_prob1),        nn.Linear(num_hiddens1, num_hiddens2),         nn.ReLU(),        nn.Dropout(drop_prob2),        nn.Linear(num_hiddens2, 10)        )for param in net.parameters():    nn.init.normal_(param, mean=0, std=0.01)</code></pre><pre><code class="lang-python">optimizer = torch.optim.SGD(net.parameters(), lr=0.5)d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, optimizer)</code></pre><pre><code>epoch 1, loss 0.0046, train acc 0.553, test acc 0.736epoch 2, loss 0.0023, train acc 0.785, test acc 0.803epoch 3, loss 0.0019, train acc 0.818, test acc 0.756epoch 4, loss 0.0018, train acc 0.835, test acc 0.829epoch 5, loss 0.0016, train acc 0.848, test acc 0.851</code></pre><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul><li><p>欠拟合现象：模型无法达到一个较低的误差</p></li><li><p>过拟合现象：训练误差较低但是泛化误差依然较高，二者相差较大</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动手深度学习(三):多层感知机</title>
      <link href="/2020/02/14/dl-notes3-mlp/"/>
      <url>/2020/02/14/dl-notes3-mlp/</url>
      
        <content type="html"><![CDATA[<h1 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h1><ol><li>多层感知机的基本知识</li><li>使用多层感知机图像分类的从零开始的实现</li><li>使用pytorch的简洁实现</li></ol><h2 id="多层感知机的基本知识"><a href="#多层感知机的基本知识" class="headerlink" title="多层感知机的基本知识"></a>多层感知机的基本知识</h2><p>深度学习主要关注多层模型。在这里，我们将以多层感知机（multilayer perceptron，MLP）为例，介绍多层神经网络的概念。</p><h3 id="隐藏层"><a href="#隐藏层" class="headerlink" title="隐藏层"></a>隐藏层</h3><p>下图展示了一个多层感知机的神经网络图，它含有一个隐藏层，该层中有5个隐藏单元。</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/upload/image/q5ho684jmh.png" alt="Image Name"></p><h3 id="表达公式"><a href="#表达公式" class="headerlink" title="表达公式"></a>表达公式</h3><p>具体来说，给定一个小批量样本$\boldsymbol{X} \in \mathbb{R}^{n \times d}$，其批量大小为$n$，输入个数为$d$。假设多层感知机只有一个隐藏层，其中隐藏单元个数为$h$。记隐藏层的输出（也称为隐藏层变量或隐藏变量）为$\boldsymbol{H}$，有$\boldsymbol{H} \in \mathbb{R}^{n \times h}$。因为隐藏层和输出层均是全连接层，可以设隐藏层的权重参数和偏差参数分别为$\boldsymbol{W}_h \in \mathbb{R}^{d \times h}$和 $\boldsymbol{b}_h \in \mathbb{R}^{1 \times h}$，输出层的权重和偏差参数分别为$\boldsymbol{W}_o \in \mathbb{R}^{h \times q}$和$\boldsymbol{b}_o \in \mathbb{R}^{1 \times q}$。</p><p>我们先来看一种含单隐藏层的多层感知机的设计。其输出$\boldsymbol{O} \in \mathbb{R}^{n \times q}$的计算为</p><script type="math/tex; mode=display"> \begin{aligned} \boldsymbol{H} &= \boldsymbol{X} \boldsymbol{W}_h + \boldsymbol{b}_h,\\ \boldsymbol{O} &= \boldsymbol{H} \boldsymbol{W}_o + \boldsymbol{b}_o, \end{aligned}</script><p>也就是将隐藏层的输出直接作为输出层的输入。如果将以上两个式子联立起来，可以得到</p><script type="math/tex; mode=display"> \boldsymbol{O} = (\boldsymbol{X} \boldsymbol{W}_h + \boldsymbol{b}_h)\boldsymbol{W}_o + \boldsymbol{b}_o = \boldsymbol{X} \boldsymbol{W}_h\boldsymbol{W}_o + \boldsymbol{b}_h \boldsymbol{W}_o + \boldsymbol{b}_o.</script><p>从联立后的式子可以看出，虽然神经网络引入了隐藏层，却依然等价于一个单层神经网络：其中输出层权重参数为$\boldsymbol{W}_h\boldsymbol{W}_o$，偏差参数为$\boldsymbol{b}_h \boldsymbol{W}_o + \boldsymbol{b}_o$。不难发现，即便再添加更多的隐藏层，以上设计依然只能与仅含输出层的单层神经网络等价。</p><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>上述问题的根源在于全连接层只是对数据做仿射变换（affine transformation），而多个仿射变换的叠加仍然是一个仿射变换。解决问题的一个方法是引入非线性变换，例如对隐藏变量使用按元素运算的非线性函数进行变换，然后再作为下一个全连接层的输入。这个非线性函数被称为激活函数（activation function）。</p><p>下面我们介绍几个常用的激活函数：</p><h4 id="ReLU函数"><a href="#ReLU函数" class="headerlink" title="ReLU函数"></a>ReLU函数</h4><p>ReLU（rectified linear unit）函数提供了一个很简单的非线性变换。给定元素$x$，该函数定义为</p><script type="math/tex; mode=display">\text{ReLU}(x) = \max(x, 0).</script><p>可以看出，ReLU函数只保留正数元素，并将负数元素清零。为了直观地观察这一非线性变换，我们先定义一个绘图函数xyplot。</p><pre><code class="lang-python">%matplotlib inlineimport torchimport numpy as npimport matplotlib.pyplot as pltimport syssys.path.append(&quot;/home/kesci/input&quot;)import d2lzh1981 as d2lprint(torch.__version__)</code></pre><pre><code>1.3.0</code></pre><pre><code class="lang-python">def xyplot(x_vals, y_vals, name):    # d2l.set_figsize(figsize=(5, 2.5))    plt.plot(x_vals.detach().numpy(), y_vals.detach().numpy())    plt.xlabel(&#39;x&#39;)    plt.ylabel(name + &#39;(x)&#39;)</code></pre><pre><code class="lang-python">x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)y = x.relu()xyplot(x, y, &#39;relu&#39;)</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/070825B6A382411DA5BD7D14E67E8D54/q5hv7cdtna.png"></p><pre><code class="lang-python">y.sum().backward()xyplot(x, x.grad, &#39;grad of relu&#39;)</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/BFB05150DBD1474D9A9ECCB9CDF1DD39/q5hv7c3pxb.png"></p><h4 id="Sigmoid函数"><a href="#Sigmoid函数" class="headerlink" title="Sigmoid函数"></a>Sigmoid函数</h4><p>sigmoid函数可以将元素的值变换到0和1之间：</p><script type="math/tex; mode=display">\text{sigmoid}(x) = \frac{1}{1 + \exp(-x)}.</script><pre><code class="lang-python">y = x.sigmoid()xyplot(x, y, &#39;sigmoid&#39;)</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/68FCB4E8142144458F13128B370D1C91/q5hv7dor11.png"></p><p>依据链式法则，sigmoid函数的导数</p><script type="math/tex; mode=display">\text{sigmoid}'(x) = \text{sigmoid}(x)\left(1-\text{sigmoid}(x)\right).</script><p>下面绘制了sigmoid函数的导数。当输入为0时，sigmoid函数的导数达到最大值0.25；当输入越偏离0时，sigmoid函数的导数越接近0。</p><pre><code class="lang-python">x.grad.zero_()y.sum().backward()xyplot(x, x.grad, &#39;grad of sigmoid&#39;)</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/878C7B8823304F72860965E119A21412/q5hv7dpse9.png"></p><h4 id="tanh函数"><a href="#tanh函数" class="headerlink" title="tanh函数"></a>tanh函数</h4><p>tanh（双曲正切）函数可以将元素的值变换到-1和1之间：</p><script type="math/tex; mode=display">\text{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}.</script><p>我们接着绘制tanh函数。当输入接近0时，tanh函数接近线性变换。虽然该函数的形状和sigmoid函数的形状很像，但tanh函数在坐标系的原点上对称。</p><pre><code class="lang-python">y = x.tanh()xyplot(x, y, &#39;tanh&#39;)</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/92D16076309F42169482834C0B6ABB24/q5hv7dfeso.png"></p><p>依据链式法则，tanh函数的导数</p><script type="math/tex; mode=display">\text{tanh}'(x) = 1 - \text{tanh}^2(x).</script><p>下面绘制了tanh函数的导数。当输入为0时，tanh函数的导数达到最大值1；当输入越偏离0时，tanh函数的导数越接近0。</p><pre><code class="lang-python">x.grad.zero_()y.sum().backward()xyplot(x, x.grad, &#39;grad of tanh&#39;)</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/CB16F4B33E664E14BCE8E52D8B37C47F/q5hv7ejc8y.png"></p><h3 id="关于激活函数的选择"><a href="#关于激活函数的选择" class="headerlink" title="关于激活函数的选择"></a>关于激活函数的选择</h3><p>ReLu函数是一个通用的激活函数，目前在大多数情况下使用。但是，ReLU函数只能在隐藏层中使用。</p><p>用于分类器时，sigmoid函数及其组合通常效果更好。由于梯度消失问题，有时要避免使用sigmoid和tanh函数。  </p><p>在神经网络层数较多的时候，最好使用ReLu函数，ReLu函数比较简单计算量少，而sigmoid和tanh函数计算量大很多。</p><p>在选择激活函数的时候可以先选用ReLu函数如果效果不理想可以尝试其他激活函数。</p><h3 id="多层感知机-1"><a href="#多层感知机-1" class="headerlink" title="多层感知机"></a>多层感知机</h3><p>多层感知机就是含有至少一个隐藏层的由全连接层组成的神经网络，且每个隐藏层的输出通过激活函数进行变换。多层感知机的层数和各隐藏层中隐藏单元个数都是超参数。以单隐藏层为例并沿用本节之前定义的符号，多层感知机按以下方式计算输出：</p><script type="math/tex; mode=display"> \begin{aligned} \boldsymbol{H} &= \phi(\boldsymbol{X} \boldsymbol{W}_h + \boldsymbol{b}_h),\\ \boldsymbol{O} &= \boldsymbol{H} \boldsymbol{W}_o + \boldsymbol{b}_o, \end{aligned}</script><p>其中$\phi$表示激活函数。</p><h2 id="多层感知机从零开始的实现"><a href="#多层感知机从零开始的实现" class="headerlink" title="多层感知机从零开始的实现"></a>多层感知机从零开始的实现</h2><pre><code class="lang-python">import torchimport numpy as npimport syssys.path.append(&quot;/home/kesci/input&quot;)import d2lzh1981 as d2lprint(torch.__version__)</code></pre><pre><code>1.3.0</code></pre><h3 id="获取训练集"><a href="#获取训练集" class="headerlink" title="获取训练集"></a>获取训练集</h3><pre><code class="lang-python">batch_size = 256train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size,root=&#39;/home/kesci/input/FashionMNIST2065&#39;)</code></pre><h3 id="定义模型参数"><a href="#定义模型参数" class="headerlink" title="定义模型参数"></a>定义模型参数</h3><pre><code class="lang-python">num_inputs, num_outputs, num_hiddens = 784, 10, 256W1 = torch.tensor(np.random.normal(0, 0.01, (num_inputs, num_hiddens)), dtype=torch.float)b1 = torch.zeros(num_hiddens, dtype=torch.float)W2 = torch.tensor(np.random.normal(0, 0.01, (num_hiddens, num_outputs)), dtype=torch.float)b2 = torch.zeros(num_outputs, dtype=torch.float)params = [W1, b1, W2, b2]for param in params:    param.requires_grad_(requires_grad=True)</code></pre><h3 id="定义激活函数"><a href="#定义激活函数" class="headerlink" title="定义激活函数"></a>定义激活函数</h3><pre><code class="lang-python">def relu(X):    return torch.max(input=X, other=torch.tensor(0.0))</code></pre><h3 id="定义网络"><a href="#定义网络" class="headerlink" title="定义网络"></a>定义网络</h3><pre><code class="lang-python">def net(X):    X = X.view((-1, num_inputs))    H = relu(torch.matmul(X, W1) + b1)    return torch.matmul(H, W2) + b2</code></pre><h3 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h3><pre><code class="lang-python">loss = torch.nn.CrossEntropyLoss()</code></pre><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><pre><code class="lang-python">num_epochs, lr = 5, 100.0# def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,#               params=None, lr=None, optimizer=None):#     for epoch in range(num_epochs):#         train_l_sum, train_acc_sum, n = 0.0, 0.0, 0#         for X, y in train_iter:#             y_hat = net(X)#             l = loss(y_hat, y).sum()#             #             # 梯度清零#             if optimizer is not None:#                 optimizer.zero_grad()#             elif params is not None and params[0].grad is not None:#                 for param in params:#                     param.grad.data.zero_()#            #             l.backward()#             if optimizer is None:#                 d2l.sgd(params, lr, batch_size)#             else:#                 optimizer.step()  # “softmax回归的简洁实现”一节将用到#             #             #             train_l_sum += l.item()#             train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()#             n += y.shape[0]#         test_acc = evaluate_accuracy(test_iter, net)#         print(&#39;epoch %d, loss %.4f, train acc %.3f, test acc %.3f&#39;#               % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr)</code></pre><pre><code>epoch 1, loss 0.0030, train acc 0.712, test acc 0.806epoch 2, loss 0.0019, train acc 0.821, test acc 0.806epoch 3, loss 0.0017, train acc 0.847, test acc 0.825epoch 4, loss 0.0015, train acc 0.856, test acc 0.834epoch 5, loss 0.0015, train acc 0.863, test acc 0.847</code></pre><h2 id="多层感知机pytorch实现"><a href="#多层感知机pytorch实现" class="headerlink" title="多层感知机pytorch实现"></a>多层感知机pytorch实现</h2><pre><code class="lang-python">import torchfrom torch import nnfrom torch.nn import initimport numpy as npimport syssys.path.append(&quot;/home/kesci/input&quot;)import d2lzh1981 as d2lprint(torch.__version__)</code></pre><pre><code>1.3.0</code></pre><h3 id="初始化模型和各个参数"><a href="#初始化模型和各个参数" class="headerlink" title="初始化模型和各个参数"></a>初始化模型和各个参数</h3><pre><code class="lang-python">num_inputs, num_outputs, num_hiddens = 784, 10, 256net = nn.Sequential(        d2l.FlattenLayer(),        nn.Linear(num_inputs, num_hiddens),        nn.ReLU(),        nn.Linear(num_hiddens, num_outputs),         )for params in net.parameters():    init.normal_(params, mean=0, std=0.01)</code></pre><h3 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h3><pre><code class="lang-python">batch_size = 256train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size,root=&#39;/home/kesci/input/FashionMNIST2065&#39;)loss = torch.nn.CrossEntropyLoss()optimizer = torch.optim.SGD(net.parameters(), lr=0.5)num_epochs = 5d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, optimizer)</code></pre><pre><code>epoch 1, loss 0.0031, train acc 0.701, test acc 0.774epoch 2, loss 0.0019, train acc 0.821, test acc 0.806epoch 3, loss 0.0017, train acc 0.841, test acc 0.805epoch 4, loss 0.0015, train acc 0.855, test acc 0.834epoch 5, loss 0.0014, train acc 0.866, test acc 0.840</code></pre><h2 id="练习题"><a href="#练习题" class="headerlink" title="练习题"></a>练习题</h2><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/amdb6W3tQBRyuXE.png" alt=""></p><p>256x256x1000 + 1000x10 = 65546000</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动手深度学习(二):Softmax</title>
      <link href="/2020/02/14/dl-notes2-softmax/"/>
      <url>/2020/02/14/dl-notes2-softmax/</url>
      
        <content type="html"><![CDATA[<h1 id="softmax和分类模型"><a href="#softmax和分类模型" class="headerlink" title="softmax和分类模型"></a>softmax和分类模型</h1><p>内容包含：</p><ol><li>softmax回归的基本概念</li><li>如何获取Fashion-MNIST数据集和读取数据</li><li>softmax回归模型的从零开始实现，实现一个对Fashion-MNIST训练集中的图像数据进行分类的模型</li><li>使用pytorch重新实现softmax回归模型</li></ol><h2 id="softmax的基本概念"><a href="#softmax的基本概念" class="headerlink" title="softmax的基本概念"></a>softmax的基本概念</h2><ul><li><p>分类问题<br>一个简单的图像分类问题，输入图像的高和宽均为2像素，色彩为灰度。<br>图像中的4像素分别记为$x_1, x_2, x_3, x_4$。<br>假设真实标签为狗、猫或者鸡，这些标签对应的离散值为$y_1, y_2, y_3$。<br>我们通常使用离散的数值来表示类别，例如$y_1=1, y_2=2, y_3=3$。</p></li><li><p>权重矢量  </p><script type="math/tex; mode=display">\begin{aligned} o_1 &= x_1 w_{11} + x_2 w_{21} + x_3 w_{31} + x_4 w_{41} + b_1 \end{aligned}</script></li></ul><script type="math/tex; mode=display"> \begin{aligned} o_2 &= x_1 w_{12} + x_2 w_{22} + x_3 w_{32} + x_4 w_{42} + b_2 \end{aligned}</script><script type="math/tex; mode=display"> \begin{aligned} o_3 &= x_1 w_{13} + x_2 w_{23} + x_3 w_{33} + x_4 w_{43} + b_3 \end{aligned}</script><ul><li>神经网络图<br>下图用神经网络图描绘了上面的计算。softmax回归同线性回归一样，也是一个单层神经网络。由于每个输出$o_1, o_2, o_3$的计算都要依赖于所有的输入$x_1, x_2, x_3, x_4$，softmax回归的输出层也是一个全连接层。</li></ul><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/upload/image/q5hmymezog.png" alt="Image Name"></p><script type="math/tex; mode=display">\begin{aligned}softmax回归是一个单层神经网络\end{aligned}</script><p>既然分类问题需要得到离散的预测输出，一个简单的办法是将输出值$o_i$当作预测类别是$i$的置信度，并将值最大的输出所对应的类作为预测输出，即输出 $\underset{i}{\arg\max} o_i$。例如，如果$o_1,o_2,o_3$分别为$0.1,10,0.1$，由于$o_2$最大，那么预测类别为2，其代表猫。</p><ul><li>输出问题<br>直接使用输出层的输出有两个问题：<ol><li>一方面，由于输出层的输出值的范围不确定，我们难以直观上判断这些值的意义。例如，刚才举的例子中的输出值10表示“很置信”图像类别为猫，因为该输出值是其他两类的输出值的100倍。但如果$o_1=o_3=10^3$，那么输出值10却又表示图像类别为猫的概率很低。</li><li>另一方面，由于真实标签是离散值，这些离散值与不确定范围的输出值之间的误差难以衡量。</li></ol></li></ul><p>softmax运算符（softmax operator）解决了以上两个问题。它通过下式将输出值变换成值为正且和为1的概率分布：</p><script type="math/tex; mode=display"> \hat{y}_1, \hat{y}_2, \hat{y}_3 = \text{softmax}(o_1, o_2, o_3)</script><p>其中</p><script type="math/tex; mode=display"> \hat{y}1 = \frac{ \exp(o_1)}{\sum_{i=1}^3 \exp(o_i)},\quad \hat{y}2 = \frac{ \exp(o_2)}{\sum_{i=1}^3 \exp(o_i)},\quad \hat{y}3 = \frac{ \exp(o_3)}{\sum_{i=1}^3 \exp(o_i)}.</script><p>容易看出$\hat{y}_1 + \hat{y}_2 + \hat{y}_3 = 1$且$0 \leq \hat{y}_1, \hat{y}_2, \hat{y}_3 \leq 1$，因此$\hat{y}_1, \hat{y}_2, \hat{y}_3$是一个合法的概率分布。这时候，如果$\hat{y}_2=0.8$，不管$\hat{y}_1$和$\hat{y}_3$的值是多少，我们都知道图像类别为猫的概率是80%。此外，我们注意到</p><script type="math/tex; mode=display"> \underset{i}{\arg\max} o_i = \underset{i}{\arg\max} \hat{y}_i</script><p>因此softmax运算不改变预测类别输出。</p><ul><li>计算效率<ul><li>单样本矢量计算表达式<br>为了提高计算效率，我们可以将单样本分类通过矢量计算来表达。在上面的图像分类问题中，假设softmax回归的权重和偏差参数分别为</li></ul></li></ul><script type="math/tex; mode=display"> \boldsymbol{W} = \begin{bmatrix} w_{11} & w_{12} & w_{13} \\ w_{21} & w_{22} & w_{23} \\ w_{31} & w_{32} & w_{33} \\ w_{41} & w_{42} & w_{43} \end{bmatrix},\quad \boldsymbol{b} = \begin{bmatrix} b_1 & b_2 & b_3 \end{bmatrix},</script><p>设高和宽分别为2个像素的图像样本$i$的特征为</p><script type="math/tex; mode=display">\boldsymbol{x}^{(i)} = \begin{bmatrix}x_1^{(i)} & x_2^{(i)} & x_3^{(i)} & x_4^{(i)}\end{bmatrix},</script><p>输出层的输出为</p><script type="math/tex; mode=display">\boldsymbol{o}^{(i)} = \begin{bmatrix}o_1^{(i)} & o_2^{(i)} & o_3^{(i)}\end{bmatrix},</script><p>预测为狗、猫或鸡的概率分布为</p><script type="math/tex; mode=display">\boldsymbol{\hat{y}}^{(i)} = \begin{bmatrix}\hat{y}_1^{(i)} & \hat{y}_2^{(i)} & \hat{y}_3^{(i)}\end{bmatrix}.</script><p>softmax回归对样本$i$分类的矢量计算表达式为</p><script type="math/tex; mode=display"> \begin{aligned} \boldsymbol{o}^{(i)} &= \boldsymbol{x}^{(i)} \boldsymbol{W} + \boldsymbol{b},\\ \boldsymbol{\hat{y}}^{(i)} &= \text{softmax}(\boldsymbol{o}^{(i)}). \end{aligned}</script><ul><li>小批量矢量计算表达式<br>  为了进一步提升计算效率，我们通常对小批量数据做矢量计算。广义上讲，给定一个小批量样本，其批量大小为$n$，输入个数（特征数）为$d$，输出个数（类别数）为$q$。设批量特征为$\boldsymbol{X} \in \mathbb{R}^{n \times d}$。假设softmax回归的权重和偏差参数分别为$\boldsymbol{W} \in \mathbb{R}^{d \times q}$和$\boldsymbol{b} \in \mathbb{R}^{1 \times q}$。softmax回归的矢量计算表达式为</li></ul><script type="math/tex; mode=display"> \begin{aligned} \boldsymbol{O} &= \boldsymbol{X} \boldsymbol{W} + \boldsymbol{b},\\ \boldsymbol{\hat{Y}} &= \text{softmax}(\boldsymbol{O}), \end{aligned}</script><p>其中的加法运算使用了广播机制，$\boldsymbol{O}, \boldsymbol{\hat{Y}} \in \mathbb{R}^{n \times q}$且这两个矩阵的第$i$行分别为样本$i$的输出$\boldsymbol{o}^{(i)}$和概率分布$\boldsymbol{\hat{y}}^{(i)}$。</p><h2 id="交叉熵损失函数"><a href="#交叉熵损失函数" class="headerlink" title="交叉熵损失函数"></a>交叉熵损失函数</h2><p>对于样本$i$，我们构造向量$\boldsymbol{y}^{(i)}\in \mathbb{R}^{q}$ ，使其第$y^{(i)}$（样本$i$类别的离散数值）个元素为1，其余为0。这样我们的训练目标可以设为使预测概率分布$\boldsymbol{\hat y}^{(i)}$尽可能接近真实的标签概率分布$\boldsymbol{y}^{(i)}$。</p><ul><li>平方损失估计  </li></ul><script type="math/tex; mode=display">\begin{aligned}Loss = |\boldsymbol{\hat y}^{(i)}-\boldsymbol{y}^{(i)}|^2/2\end{aligned}</script><p>然而，想要预测分类结果正确，我们其实并不需要预测概率完全等于标签概率。例如，在图像分类的例子里，如果$y^{(i)}=3$，那么我们只需要$\hat{y}^{(i)}_3$比其他两个预测值$\hat{y}^{(i)}_1$和$\hat{y}^{(i)}_2$大就行了。即使$\hat{y}^{(i)}_3$值为0.6，不管其他两个预测值为多少，类别预测均正确。而平方损失则过于严格，例如$\hat y^{(i)}_1=\hat y^{(i)}_2=0.2$比$\hat y^{(i)}_1=0, \hat y^{(i)}_2=0.4$的损失要小很多，虽然两者都有同样正确的分类预测结果。</p><p>改善上述问题的一个方法是使用更适合衡量两个概率分布差异的测量函数。其中，交叉熵（cross entropy）是一个常用的衡量方法：</p><script type="math/tex; mode=display">H\left(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}\right ) = -\sum_{j=1}^q y_j^{(i)} \log \hat y_j^{(i)},</script><p>其中带下标的$y_j^{(i)}$是向量$\boldsymbol y^{(i)}$中非0即1的元素，需要注意将它与样本$i$类别的离散数值，即不带下标的$y^{(i)}$区分。在上式中，我们知道向量$\boldsymbol y^{(i)}$中只有第$y^{(i)}$个元素$y^{(i)}{y^{(i)}}$为1，其余全为0，于是$H(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}) = -\log \hat y_{y^{(i)}}^{(i)}$。也就是说，交叉熵只关心对正确类别的预测概率，因为只要其值足够大，就可以确保分类结果正确。当然，遇到一个样本有多个标签时，例如图像里含有不止一个物体时，我们并不能做这一步简化。但即便对于这种情况，交叉熵同样只关心对图像中出现的物体类别的预测概率。</p><p>假设训练数据集的样本数为$n$，交叉熵损失函数定义为 </p><script type="math/tex; mode=display">\ell(\boldsymbol{\Theta}) = \frac{1}{n} \sum_{i=1}^n H\left(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}\right ),</script><p>其中$\boldsymbol{\Theta}$代表模型参数。同样地，如果每个样本只有一个标签，那么交叉熵损失可以简写成$\ell(\boldsymbol{\Theta}) = -(1/n) \sum_{i=1}^n \log \hat y_{y^{(i)}}^{(i)}$。从另一个角度来看，我们知道最小化$\ell(\boldsymbol{\Theta})$等价于最大化$\exp(-n\ell(\boldsymbol{\Theta}))=\prod_{i=1}^n \hat y_{y^{(i)}}^{(i)}$，即最小化交叉熵损失函数等价于最大化训练数据集所有标签类别的联合预测概率。</p><h2 id="模型训练和预测"><a href="#模型训练和预测" class="headerlink" title="模型训练和预测"></a>模型训练和预测</h2><p>在训练好softmax回归模型后，给定任一样本特征，就可以预测每个输出类别的概率。通常，我们把预测概率最大的类别作为输出类别。如果它与真实类别（标签）一致，说明这次预测是正确的。在3.6节的实验中，我们将使用准确率（accuracy）来评价模型的表现。它等于正确预测数量与总预测数量之比。</p><h1 id="获取Fashion-MNIST训练集和读取数据"><a href="#获取Fashion-MNIST训练集和读取数据" class="headerlink" title="获取Fashion-MNIST训练集和读取数据"></a>获取Fashion-MNIST训练集和读取数据</h1><p>在介绍softmax回归的实现前我们先引入一个多类图像分类数据集。它将在后面的章节中被多次使用，以方便我们观察比较算法之间在模型精度和计算效率上的区别。图像分类数据集中最常用的是手写数字识别数据集MNIST[1]。但大部分模型在MNIST上的分类精度都超过了95%。为了更直观地观察算法之间的差异，我们将使用一个图像内容更加复杂的数据集Fashion-MNIST[2]。</p><p>我这里我们会使用torchvision包，它是服务于PyTorch深度学习框架的，主要用来构建计算机视觉模型。torchvision主要由以下几部分构成：</p><ol><li>torchvision.datasets: 一些加载数据的函数及常用的数据集接口；</li><li>torchvision.models: 包含常用的模型结构（含预训练模型），例如AlexNet、VGG、ResNet等；</li><li>torchvision.transforms: 常用的图片变换，例如裁剪、旋转等；</li><li>torchvision.utils: 其他的一些有用的方法。</li></ol><pre><code class="lang-python"># import needed package%matplotlib inlinefrom IPython import displayimport matplotlib.pyplot as pltimport torchimport torchvisionimport torchvision.transforms as transformsimport timeimport syssys.path.append(&quot;/home/kesci/input&quot;)import d2lzh1981 as d2lprint(torch.__version__)print(torchvision.__version__)</code></pre><pre><code>---------------------------------------------------------------------------ModuleNotFoundError                       Traceback (most recent call last)&lt;ipython-input-1-e3b06ab588d2&gt; in &lt;module&gt;      4 import matplotlib.pyplot as plt      5 ----&gt; 6 import torch      7 import torchvision      8 import torchvision.transforms as transformsModuleNotFoundError: No module named &#39;torch&#39;</code></pre><h2 id="get-dataset"><a href="#get-dataset" class="headerlink" title="get dataset"></a>get dataset</h2><pre><code class="lang-python">mnist_train = torchvision.datasets.FashionMNIST(root=&#39;/home/kesci/input/FashionMNIST2065&#39;, train=True, download=True, transform=transforms.ToTensor())mnist_test = torchvision.datasets.FashionMNIST(root=&#39;/home/kesci/input/FashionMNIST2065&#39;, train=False, download=True, transform=transforms.ToTensor())</code></pre><p>class torchvision.datasets.FashionMNIST(root, train=True, transform=None, target_transform=None, download=False)</p><ul><li>root（string）– 数据集的根目录，其中存放processed/training.pt和processed/test.pt文件。</li><li>train（bool, 可选）– 如果设置为True，从training.pt创建数据集，否则从test.pt创建。</li><li>download（bool, 可选）– 如果设置为True，从互联网下载数据并放到root文件夹下。如果root目录下已经存在数据，不会再次下载。</li><li>transform（可被调用 , 可选）– 一种函数或变换，输入PIL图片，返回变换之后的数据。如：transforms.RandomCrop。</li><li>target_transform（可被调用 , 可选）– 一种函数或变换，输入目标，进行变换。</li></ul><pre><code class="lang-python"># show result print(type(mnist_train))print(len(mnist_train), len(mnist_test))</code></pre><pre><code>&lt;class &#39;torchvision.datasets.mnist.FashionMNIST&#39;&gt;60000 10000</code></pre><pre><code class="lang-python"># 我们可以通过下标来访问任意一个样本feature, label = mnist_train[0]print(feature.shape, label)  # Channel x Height x Width</code></pre><pre><code>torch.Size([1, 28, 28]) 9</code></pre><p>如果不做变换输入的数据是图像，我们可以看一下图片的类型参数：</p><pre><code class="lang-python">mnist_PIL = torchvision.datasets.FashionMNIST(root=&#39;/home/kesci/input/FashionMNIST2065&#39;, train=True, download=True)PIL_feature, label = mnist_PIL[0]print(PIL_feature)</code></pre><pre><code>&lt;PIL.Image.Image image mode=L size=28x28 at 0x7F54A41612E8&gt;</code></pre><pre><code class="lang-python"># 本函数已保存在d2lzh包中方便以后使用def get_fashion_mnist_labels(labels):    text_labels = [&#39;t-shirt&#39;, &#39;trouser&#39;, &#39;pullover&#39;, &#39;dress&#39;, &#39;coat&#39;,                   &#39;sandal&#39;, &#39;shirt&#39;, &#39;sneaker&#39;, &#39;bag&#39;, &#39;ankle boot&#39;]    return [text_labels[int(i)] for i in labels]</code></pre><pre><code class="lang-python">def show_fashion_mnist(images, labels):    d2l.use_svg_display()    # 这里的_表示我们忽略（不使用）的变量    _, figs = plt.subplots(1, len(images), figsize=(12, 12))    for f, img, lbl in zip(figs, images, labels):        f.imshow(img.view((28, 28)).numpy())        f.set_title(lbl)        f.axes.get_xaxis().set_visible(False)        f.axes.get_yaxis().set_visible(False)    plt.show()</code></pre><pre><code class="lang-python">X, y = [], []for i in range(10):    X.append(mnist_train[i][0]) # 将第i个feature加到X中    y.append(mnist_train[i][1]) # 将第i个label加到y中show_fashion_mnist(X, get_fashion_mnist_labels(y))</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/056F457B00454FFD81A3CB6AD966C508/q5j7ehijw7.svg"></p><pre><code class="lang-python"># 读取数据batch_size = 256num_workers = 4train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)</code></pre><pre><code class="lang-python">start = time.time()for X, y in train_iter:    continueprint(&#39;%.2f sec&#39; % (time.time() - start))</code></pre><pre><code>4.95 sec</code></pre><h1 id="softmax从零开始的实现"><a href="#softmax从零开始的实现" class="headerlink" title="softmax从零开始的实现"></a>softmax从零开始的实现</h1><pre><code class="lang-python">import torchimport torchvisionimport numpy as npimport syssys.path.append(&quot;/home/kesci/input&quot;)import d2lzh1981 as d2lprint(torch.__version__)print(torchvision.__version__)</code></pre><pre><code>1.3.00.4.1a0+d94043a</code></pre><h2 id="获取训练集数据和测试集数据"><a href="#获取训练集数据和测试集数据" class="headerlink" title="获取训练集数据和测试集数据"></a>获取训练集数据和测试集数据</h2><pre><code class="lang-python">batch_size = 256train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, root=&#39;/home/kesci/input/FashionMNIST2065&#39;)</code></pre><h2 id="模型参数初始化"><a href="#模型参数初始化" class="headerlink" title="模型参数初始化"></a>模型参数初始化</h2><pre><code class="lang-python">num_inputs = 784print(28*28)num_outputs = 10W = torch.tensor(np.random.normal(0, 0.01, (num_inputs, num_outputs)), dtype=torch.float)b = torch.zeros(num_outputs, dtype=torch.float)</code></pre><pre><code>784</code></pre><pre><code class="lang-python">W.requires_grad_(requires_grad=True)b.requires_grad_(requires_grad=True)</code></pre><pre><code>tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)</code></pre><h2 id="对多维Tensor按维度操作"><a href="#对多维Tensor按维度操作" class="headerlink" title="对多维Tensor按维度操作"></a>对多维Tensor按维度操作</h2><pre><code class="lang-python">X = torch.tensor([[1, 2, 3], [4, 5, 6]])print(X.sum(dim=0, keepdim=True))  # dim为0，按照相同的列求和，并在结果中保留列特征print(X.sum(dim=1, keepdim=True))  # dim为1，按照相同的行求和，并在结果中保留行特征print(X.sum(dim=0, keepdim=False)) # dim为0，按照相同的列求和，不在结果中保留列特征print(X.sum(dim=1, keepdim=False)) # dim为1，按照相同的行求和，不在结果中保留行特征</code></pre><pre><code>tensor([[5, 7, 9]])tensor([[ 6],        [15]])tensor([5, 7, 9])tensor([ 6, 15])</code></pre><h2 id="定义softmax操作"><a href="#定义softmax操作" class="headerlink" title="定义softmax操作"></a>定义softmax操作</h2><script type="math/tex; mode=display"> \hat{y}_j = \frac{ \exp(o_j)}{\sum_{i=1}^3 \exp(o_i)}</script><pre><code class="lang-python">def softmax(X):    X_exp = X.exp()    partition = X_exp.sum(dim=1, keepdim=True)    # print(&quot;X size is &quot;, X_exp.size())    # print(&quot;partition size is &quot;, partition, partition.size())    return X_exp / partition  # 这里应用了广播机制</code></pre><pre><code class="lang-python">X = torch.rand((2, 5))X_prob = softmax(X)print(X_prob, &#39;\n&#39;, X_prob.sum(dim=1))</code></pre><pre><code>tensor([[0.2253, 0.1823, 0.1943, 0.2275, 0.1706],        [0.1588, 0.2409, 0.2310, 0.1670, 0.2024]])  tensor([1.0000, 1.0000])</code></pre><h2 id="softmax回归模型"><a href="#softmax回归模型" class="headerlink" title="softmax回归模型"></a>softmax回归模型</h2><script type="math/tex; mode=display"> \begin{aligned} \boldsymbol{o}^{(i)} &= \boldsymbol{x}^{(i)} \boldsymbol{W} + \boldsymbol{b},\\ \boldsymbol{\hat{y}}^{(i)} &= \text{softmax}(\boldsymbol{o}^{(i)}). \end{aligned}</script><pre><code class="lang-python">def net(X):    return softmax(torch.mm(X.view((-1, num_inputs)), W) + b)</code></pre><h2 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h2><script type="math/tex; mode=display">H\left(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}\right ) = -\sum_{j=1}^q y_j^{(i)} \log \hat y_j^{(i)},</script><script type="math/tex; mode=display">\ell(\boldsymbol{\Theta}) = \frac{1}{n} \sum_{i=1}^n H\left(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}\right ),</script><script type="math/tex; mode=display">\ell(\boldsymbol{\Theta}) = -(1/n) \sum_{i=1}^n \log \hat y_{y^{(i)}}^{(i)}</script><pre><code class="lang-python">y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])y = torch.LongTensor([0, 2])y_hat.gather(1, y.view(-1, 1))</code></pre><pre><code>tensor([[0.1000],        [0.5000]])</code></pre><pre><code class="lang-python">def cross_entropy(y_hat, y):    return - torch.log(y_hat.gather(1, y.view(-1, 1)))</code></pre><h2 id="定义准确率"><a href="#定义准确率" class="headerlink" title="定义准确率"></a>定义准确率</h2><p>我们模型训练完了进行模型预测的时候，会用到我们这里定义的准确率。</p><pre><code class="lang-python">def accuracy(y_hat, y):    return (y_hat.argmax(dim=1) == y).float().mean().item()</code></pre><pre><code class="lang-python">print(accuracy(y_hat, y))</code></pre><pre><code>0.5</code></pre><pre><code class="lang-python"># 本函数已保存在d2lzh_pytorch包中方便以后使用。该函数将被逐步改进：它的完整实现将在“图像增广”一节中描述def evaluate_accuracy(data_iter, net):    acc_sum, n = 0.0, 0    for X, y in data_iter:        acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()        n += y.shape[0]    return acc_sum / n</code></pre><pre><code class="lang-python">print(evaluate_accuracy(test_iter, net))</code></pre><pre><code>0.1445</code></pre><h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><pre><code class="lang-python">num_epochs, lr = 5, 0.1# 本函数已保存在d2lzh_pytorch包中方便以后使用def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,              params=None, lr=None, optimizer=None):    for epoch in range(num_epochs):        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0        for X, y in train_iter:            y_hat = net(X)            l = loss(y_hat, y).sum()            # 梯度清零            if optimizer is not None:                optimizer.zero_grad()            elif params is not None and params[0].grad is not None:                for param in params:                    param.grad.data.zero_()            l.backward()            if optimizer is None:                d2l.sgd(params, lr, batch_size)            else:                optimizer.step()             train_l_sum += l.item()            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()            n += y.shape[0]        test_acc = evaluate_accuracy(test_iter, net)        print(&#39;epoch %d, loss %.4f, train acc %.3f, test acc %.3f&#39;              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, batch_size, [W, b], lr)</code></pre><pre><code>epoch 1, loss 0.7851, train acc 0.750, test acc 0.791epoch 2, loss 0.5704, train acc 0.814, test acc 0.810epoch 3, loss 0.5258, train acc 0.825, test acc 0.819epoch 4, loss 0.5014, train acc 0.832, test acc 0.824epoch 5, loss 0.4865, train acc 0.836, test acc 0.827</code></pre><h2 id="模型预测"><a href="#模型预测" class="headerlink" title="模型预测"></a>模型预测</h2><p>现在我们的模型训练完了，可以进行一下预测，我们的这个模型训练的到底准确不准确。<br>现在就可以演示如何对图像进行分类了。给定一系列图像（第三行图像输出），我们比较一下它们的真实标签（第一行文本输出）和模型预测结果（第二行文本输出）。</p><pre><code class="lang-python">X, y = iter(test_iter).next()true_labels = d2l.get_fashion_mnist_labels(y.numpy())pred_labels = d2l.get_fashion_mnist_labels(net(X).argmax(dim=1).numpy())titles = [true + &#39;\n&#39; + pred for true, pred in zip(true_labels, pred_labels)]d2l.show_fashion_mnist(X[0:9], titles[0:9])</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/1DA8927186304BEBA2B3DCC4A9E027DD/q5j7fq2jer.svg"></p><h1 id="softmax的简洁实现"><a href="#softmax的简洁实现" class="headerlink" title="softmax的简洁实现"></a>softmax的简洁实现</h1><pre><code class="lang-python"># 加载各种包或者模块import torchfrom torch import nnfrom torch.nn import initimport numpy as npimport syssys.path.append(&quot;/home/kesci/input&quot;)import d2lzh1981 as d2lprint(torch.__version__)</code></pre><pre><code>1.3.0</code></pre><h2 id="初始化参数和获取数据"><a href="#初始化参数和获取数据" class="headerlink" title="初始化参数和获取数据"></a>初始化参数和获取数据</h2><pre><code class="lang-python">batch_size = 256train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, root=&#39;/home/kesci/input/FashionMNIST2065&#39;)</code></pre><h2 id="定义网络模型"><a href="#定义网络模型" class="headerlink" title="定义网络模型"></a>定义网络模型</h2><pre><code class="lang-python">num_inputs = 784num_outputs = 10class LinearNet(nn.Module):    def __init__(self, num_inputs, num_outputs):        super(LinearNet, self).__init__()        self.linear = nn.Linear(num_inputs, num_outputs)    def forward(self, x): # x 的形状: (batch, 1, 28, 28)        y = self.linear(x.view(x.shape[0], -1))        return y# net = LinearNet(num_inputs, num_outputs)class FlattenLayer(nn.Module):    def __init__(self):        super(FlattenLayer, self).__init__()    def forward(self, x): # x 的形状: (batch, *, *, ...)        return x.view(x.shape[0], -1)from collections import OrderedDictnet = nn.Sequential(        # FlattenLayer(),        # LinearNet(num_inputs, num_outputs)         OrderedDict([           (&#39;flatten&#39;, FlattenLayer()),           (&#39;linear&#39;, nn.Linear(num_inputs, num_outputs))]) # 或者写成我们自己定义的 LinearNet(num_inputs, num_outputs) 也可以        )</code></pre><h2 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h2><pre><code class="lang-python">init.normal_(net.linear.weight, mean=0, std=0.01)init.constant_(net.linear.bias, val=0)</code></pre><pre><code>Parameter containing:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)</code></pre><h2 id="定义损失函数-1"><a href="#定义损失函数-1" class="headerlink" title="定义损失函数"></a>定义损失函数</h2><pre><code class="lang-python">loss = nn.CrossEntropyLoss() # 下面是他的函数原型# class torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction=&#39;mean&#39;)</code></pre><h2 id="定义优化函数"><a href="#定义优化函数" class="headerlink" title="定义优化函数"></a>定义优化函数</h2><pre><code class="lang-python">optimizer = torch.optim.SGD(net.parameters(), lr=0.1) # 下面是函数原型# class torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False)</code></pre><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><pre><code class="lang-python">num_epochs = 5d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, optimizer)</code></pre><pre><code>epoch 1, loss 0.0031, train acc 0.751, test acc 0.795epoch 2, loss 0.0022, train acc 0.813, test acc 0.809epoch 3, loss 0.0021, train acc 0.825, test acc 0.806epoch 4, loss 0.0020, train acc 0.833, test acc 0.813epoch 5, loss 0.0019, train acc 0.837, test acc 0.822</code></pre><h2 id="练习题"><a href="#练习题" class="headerlink" title="练习题"></a>练习题</h2><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/cwADXPBIbdjvGhN.png" alt=""></p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/8pOqEmTBAYGcWMs.png" alt="解析"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动手深度学习(一):线性回归</title>
      <link href="/2020/02/14/dl-notes1-lin-re/"/>
      <url>/2020/02/14/dl-notes1-lin-re/</url>
      
        <content type="html"><![CDATA[<h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><p>主要内容包括：</p><ol><li>线性回归的基本要素</li><li>线性回归模型从零开始的实现</li><li>线性回归模型使用pytorch的简洁实现</li></ol><h2 id="线性回归的基本要素"><a href="#线性回归的基本要素" class="headerlink" title="线性回归的基本要素"></a>线性回归的基本要素</h2><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>为了简单起见，这里我们假设价格只取决于房屋状况的两个因素，即面积（平方米）和房龄（年）。接下来我们希望探索价格与这两个因素的具体关系。线性回归假设输出与各个输入之间是线性关系:</p><script type="math/tex; mode=display">\mathrm{price} = w_{\mathrm{area}} \cdot \mathrm{area} + w_{\mathrm{age}} \cdot \mathrm{age} + b</script><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>我们通常收集一系列的真实数据，例如多栋房屋的真实售出价格和它们对应的面积和房龄。我们希望在这个数据上面寻找模型参数来使模型的预测价格与真实价格的误差最小。在机器学习术语里，该数据集被称为训练数据集（training data set）或训练集（training set），一栋房屋被称为一个样本（sample），其真实售出价格叫作标签（label），用来预测标签的两个因素叫作特征（feature）。特征用来表征样本的特点。</p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>在模型训练中，我们需要衡量价格预测值与真实值之间的误差。通常我们会选取一个非负数作为误差，且数值越小表示误差越小。一个常用的选择是平方函数。 它在评估索引为 $i$ 的样本误差的表达式为</p><script type="math/tex; mode=display">l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2,</script><script type="math/tex; mode=display">L(\mathbf{w}, b) =\frac{1}{n}\sum_{i=1}^n l^{(i)}(\mathbf{w}, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2.</script><h3 id="优化函数-随机梯度下降"><a href="#优化函数-随机梯度下降" class="headerlink" title="优化函数 - 随机梯度下降"></a>优化函数 - 随机梯度下降</h3><p>当模型和损失函数形式较为简单时，上面的误差最小化问题的解可以直接用公式表达出来。这类解叫作解析解（analytical solution）。本节使用的线性回归和平方误差刚好属于这个范畴。然而，大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。这类解叫作数值解（numerical solution）。</p><p>在求数值解的优化算法中，小批量随机梯度下降（mini-batch stochastic gradient descent）在深度学习中被广泛使用。它的算法很简单：先选取一组模型参数的初始值，如随机选取；接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量（mini-batch）$\mathcal{B}$，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后用此结果与预先设定的一个正数的乘积作为模型参数在本次迭代的减小量。   </p><script type="math/tex; mode=display">(\mathbf{w},b) \leftarrow (\mathbf{w},b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(\mathbf{w},b)} l^{(i)}(\mathbf{w},b)</script><p>学习率: $\eta$代表在每次优化中，能够学习的步长的大小<br>批量大小: $\mathcal{B}$是小批量计算中的批量大小batch size   </p><p>总结一下，优化函数的有以下两个步骤：</p><ul><li>(i)初始化模型参数，一般来说使用随机初始化；</li><li>(ii)我们在数据上迭代多次，通过在负梯度方向移动参数来更新每个参数。</li></ul><h2 id="矢量计算"><a href="#矢量计算" class="headerlink" title="矢量计算"></a>矢量计算</h2><p>在模型训练或预测时，我们常常会同时处理多个数据样本并用到矢量计算。在介绍线性回归的矢量计算表达式之前，让我们先考虑对两个向量相加的两种方法。</p><ol><li>向量相加的一种方法是，将这两个向量按元素逐一做标量加法。</li><li>向量相加的另一种方法是，将这两个向量直接做矢量加法。</li></ol><pre><code class="lang-python">import torchimport time# init variable a, b as 1000 dimension vectorn = 1000a = torch.ones(n)b = torch.ones(n)</code></pre><pre><code class="lang-python"># define a timer class to record timeclass Timer(object):    &quot;&quot;&quot;Record multiple running times.&quot;&quot;&quot;    def __init__(self):        self.times = []        self.start()    def start(self):        # start the timer        self.start_time = time.time()    def stop(self):        # stop the timer and record time into a list        self.times.append(time.time() - self.start_time)        return self.times[-1]    def avg(self):        # calculate the average and return        return sum(self.times)/len(self.times)    def sum(self):        # return the sum of recorded time        return sum(self.times)</code></pre><p>现在我们可以来测试了。首先将两个向量使用for循环按元素逐一做标量加法。</p><pre><code class="lang-python">timer = Timer()c = torch.zeros(n)for i in range(n):    c[i] = a[i] + b[i]&#39;%.5f sec&#39; % timer.stop()</code></pre><pre><code>&#39;0.01162 sec&#39;</code></pre><p>另外是使用torch来将两个向量直接做矢量加法：</p><pre><code class="lang-python">timer.start()d = a + b&#39;%.5f sec&#39; % timer.stop()</code></pre><pre><code>&#39;0.00027 sec&#39;</code></pre><p>结果很明显,后者比前者运算速度更快。因此，我们应该尽可能采用矢量计算，以提升计算效率。</p><h2 id="线性回归模型从零开始的实现"><a href="#线性回归模型从零开始的实现" class="headerlink" title="线性回归模型从零开始的实现"></a>线性回归模型从零开始的实现</h2><pre><code class="lang-python"># import packages and modules%matplotlib inlineimport torchfrom IPython import displayfrom matplotlib import pyplot as pltimport numpy as npimport randomprint(torch.__version__)</code></pre><pre><code>1.3.0</code></pre><h3 id="生成数据集"><a href="#生成数据集" class="headerlink" title="生成数据集"></a>生成数据集</h3><p>使用线性模型来生成数据集，生成一个1000个样本的数据集，下面是用来生成数据的线性关系：</p><script type="math/tex; mode=display">\mathrm{price} = w_{\mathrm{area}} \cdot \mathrm{area} + w_{\mathrm{age}} \cdot \mathrm{age} + b</script><pre><code class="lang-python"># set input feature number num_inputs = 2# set example numbernum_examples = 1000# set true weight and bias in order to generate corresponded labeltrue_w = [2, -3.4]true_b = 4.2features = torch.randn(num_examples, num_inputs,                      dtype=torch.float32)labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_blabels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()),   # 不可能严格线性分布，加一个偏差                       dtype=torch.float32)</code></pre><h3 id="使用图像来展示生成的数据"><a href="#使用图像来展示生成的数据" class="headerlink" title="使用图像来展示生成的数据"></a>使用图像来展示生成的数据</h3><pre><code class="lang-python">plt.scatter(features[:, 1].numpy(), labels.numpy(), 1)</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/8E2E1E16060241C6A33E4CF1EC65DF1D/q5mgua7bvt.png"></p><h3 id="读取数据集"><a href="#读取数据集" class="headerlink" title="读取数据集"></a>读取数据集</h3><pre><code class="lang-python">def data_iter(batch_size, features, labels):    num_examples = len(features)    indices = list(range(num_examples))    random.shuffle(indices)  # random read 10 samples 顺序打乱读取    for i in range(0, num_examples, batch_size):        # the last time may be not enough for a whole batch 防止越界        j = torch.LongTensor(indices[i: min(i + batch_size, num_examples)]) #每次跳跃 b_size        yield features.index_select(0, j), labels.index_select(0, j)   # 返回一个就退出</code></pre><pre><code class="lang-python">batch_size = 10for X, y in data_iter(batch_size, features, labels):    print(X, &#39;\n&#39;, y)    break</code></pre><pre><code>tensor([[ 0.6802, -0.1529],        [-1.1139, -1.0506],        [ 1.1355,  0.6313],        [ 0.9057,  1.4527],        [-0.7477, -0.4656],        [-0.4012, -0.1810],        [ 0.0038, -1.3701],        [-0.9438, -1.6571],        [ 0.4016,  0.4233],        [ 0.8824, -1.0067]])  tensor([6.0793, 5.5343, 4.3112, 1.0657, 4.2856, 4.0040, 8.8709, 7.9642, 3.5696,        9.3887])</code></pre><h3 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><pre><code class="lang-python">w = torch.tensor(np.random.normal(0, 0.01, (num_inputs, 1)), dtype=torch.float32)b = torch.zeros(1, dtype=torch.float32)w.requires_grad_(requires_grad=True)  # 要求梯度b.requires_grad_(requires_grad=True)</code></pre><pre><code>tensor([0.], requires_grad=True)</code></pre><h3 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h3><p>定义用来训练参数的训练模型：</p><script type="math/tex; mode=display">\mathrm{price} = w_{\mathrm{area}} \cdot \mathrm{area} + w_{\mathrm{age}} \cdot \mathrm{age} + b</script><pre><code class="lang-python">def linreg(X, w, b):    return torch.mm(X, w) + b   # mm 相乘</code></pre><h3 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h3><p>我们使用的是均方误差损失函数：</p><script type="math/tex; mode=display">l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2,</script><pre><code class="lang-python">def squared_loss(y_hat, y):     return (y_hat - y.view(y_hat.size())) ** 2 / 2   # view 表示数据相同，大小不同# s = squared_loss(2.33,3.14) + squared_loss(1.07,0.98) + squared_loss(1.23,1.32)y_hat = torch.tensor([[2.33],        [ 1.07],        [ 1.23]])y = torch.tensor([3.14, 0.98, 1.32])s = squared_loss(y_hat, y)print(s.mean())</code></pre><pre><code>tensor(0.1121)</code></pre><h3 id="定义优化函数"><a href="#定义优化函数" class="headerlink" title="定义优化函数"></a>定义优化函数</h3><p>在这里优化函数使用的是小批量随机梯度下降：</p><script type="math/tex; mode=display">(\mathbf{w},b) \leftarrow (\mathbf{w},b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(\mathbf{w},b)} l^{(i)}(\mathbf{w},b)</script><pre><code class="lang-python">def sgd(params, lr, batch_size):     # params : [w, b]    for param in params:        # 在梯度的负方向累加一个值，达到优化的效果（趋于谷底）        param.data -= lr * param.grad / batch_size # ues .data to operate param without gradient track</code></pre><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>当数据集、模型、损失函数和优化函数定义完了之后就可来准备进行模型的训练了。</p><pre><code class="lang-python"># super parameters initlr = 0.03  # lr 学习率num_epochs = 5net = linregloss = squared_loss# trainingfor epoch in range(num_epochs):  # training repeats num_epochs times    # in each epoch, all the samples in dataset will be used once    # X is the feature and y is the label of a batch sample    for X, y in data_iter(batch_size, features, labels):        l = loss(net(X, w, b), y).sum()          # calculate the gradient of batch sample loss         l.backward()          # using small batch random gradient descent to iter model parameters        sgd([w, b], lr, batch_size)          # reset parameter gradient        w.grad.data.zero_()  # 每一次运算后都需要将上一次的梯度记录清空        b.grad.data.zero_()    train_l = loss(net(features, w, b), labels)    print(&#39;epoch %d, loss %f&#39; % (epoch + 1, train_l.mean().item()))</code></pre><h2 id="线性回归模型使用pytorch的简洁实现"><a href="#线性回归模型使用pytorch的简洁实现" class="headerlink" title="线性回归模型使用pytorch的简洁实现"></a>线性回归模型使用pytorch的简洁实现</h2><pre><code class="lang-python">import torchfrom torch import nnimport numpy as nptorch.manual_seed(1)   # 同样的随机初始化种子,保证结果可以复现print(torch.__version__)torch.set_default_tensor_type(&#39;torch.FloatTensor&#39;)</code></pre><pre><code>1.3.0</code></pre><h3 id="生成数据集-1"><a href="#生成数据集-1" class="headerlink" title="生成数据集"></a>生成数据集</h3><p>在这里生成数据集跟从零开始的实现中是完全一样的。</p><pre><code class="lang-python">num_inputs = 2num_examples = 1000true_w = [2, -3.4]true_b = 4.2features = torch.tensor(np.random.normal(0, 1, (num_examples, num_inputs)), dtype=torch.float)labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_blabels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float)</code></pre><h3 id="读取数据集-1"><a href="#读取数据集-1" class="headerlink" title="读取数据集"></a>读取数据集</h3><pre><code class="lang-python">import torch.utils.data as Databatch_size = 10# combine featues and labels of datasetdataset = Data.TensorDataset(features, labels)# put dataset into DataLoaderdata_iter = Data.DataLoader(    dataset=dataset,            # torch TensorDataset format    batch_size=batch_size,      # mini batch size    shuffle=True,               # whether shuffle the data or not    num_workers=2,              # read data in multithreading)</code></pre><pre><code class="lang-python">for X, y in data_iter:    print(X, &#39;\n&#39;, y)    break</code></pre><pre><code>tensor([[ 0.2134,  0.7981],        [-0.3899, -0.4544],        [ 1.4472, -1.2160],        [-0.7354,  1.2216],        [-1.3233,  0.6937],        [-0.2810,  0.5505],        [-1.6620, -0.1457],        [-0.7635,  0.7058],        [ 0.6079, -0.7497],        [ 0.4924, -0.1376]])  tensor([ 1.9045,  4.9770, 11.2503, -1.4180, -0.8159,  1.7737,  1.3910,  0.2721,         7.9648,  5.6408])</code></pre><h3 id="定义模型-1"><a href="#定义模型-1" class="headerlink" title="定义模型"></a>定义模型</h3><pre><code class="lang-python">class LinearNet(nn.Module):    def __init__(self, n_feature):        super(LinearNet, self).__init__()      # call father function to init         self.linear = nn.Linear(n_feature, 1)  # function prototype: `torch.nn.Linear(in_features, out_features, bias=True)`    def forward(self, x):        y = self.linear(x)        return ynet = LinearNet(num_inputs)print(net)</code></pre><pre><code>LinearNet(  (linear): Linear(in_features=2, out_features=1, bias=True))</code></pre><pre><code class="lang-python"># ways to init a multilayer network# method onenet = nn.Sequential(   # 一个时序容器，Modules 会以他们传入的顺序被添加到容器中    nn.Linear(num_inputs, 1)    # other layers can be added here    )# method twonet = nn.Sequential()net.add_module(&#39;linear&#39;, nn.Linear(num_inputs, 1))# net.add_module ......# method threefrom collections import OrderedDictnet = nn.Sequential(OrderedDict([          (&#39;linear&#39;, nn.Linear(num_inputs, 1))          # ......        ]))print(net[0].weight)print(net[0].bias)# print(net)</code></pre><pre><code>Parameter containing:tensor([[0.6652, 0.1260]], requires_grad=True)Parameter containing:tensor([0.5704], requires_grad=True)</code></pre><h3 id="初始化模型参数-1"><a href="#初始化模型参数-1" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><pre><code class="lang-python">from torch.nn import initinit.normal_(net[0].weight, mean=0.0, std=0.01)  # weight重新生成init.constant_(net[0].bias, val=0.0)  # bias[ 被赋值为 0print(net[0].weight)print(net[0].bias)</code></pre><pre><code>Parameter containing:tensor([[0.0016, 0.0029]], requires_grad=True)Parameter containing:tensor([0.], requires_grad=True)</code></pre><pre><code class="lang-python">for param in net.parameters():    print(param)  # param 就是上面的 weight 和 bias</code></pre><pre><code>Parameter containing:tensor([[0.0016, 0.0029]], requires_grad=True)Parameter containing:tensor([0.], requires_grad=True)</code></pre><h3 id="定义损失函数-1"><a href="#定义损失函数-1" class="headerlink" title="定义损失函数"></a>定义损失函数</h3><pre><code class="lang-python">loss = nn.MSELoss()    # nn built-in squared loss function   采用计算 MSE 的 loss                       # function prototype: `torch.nn.MSELoss(size_average=None, reduce=None, reduction=&#39;mean&#39;)`</code></pre><h3 id="定义优化函数-1"><a href="#定义优化函数-1" class="headerlink" title="定义优化函数"></a>定义优化函数</h3><pre><code class="lang-python">import torch.optim as optimoptimizer = optim.SGD(net.parameters(), lr=0.03)   # built-in random gradient descent functionprint(optimizer)  # function prototype: `torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False)`</code></pre><pre><code>SGD (Parameter Group 0    dampening: 0    lr: 0.03    momentum: 0    nesterov: False    weight_decay: 0)</code></pre><h3 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h3><pre><code class="lang-python">num_epochs = 3for epoch in range(1, num_epochs + 1):    for X, y in data_iter:        output = net(X)        l = loss(output, y.view(-1, 1))        optimizer.zero_grad() # reset gradient, equal to net.zero_grad()        l.backward()        optimizer.step()    print(&#39;epoch %d, loss: %f&#39; % (epoch, l.item()))</code></pre><pre><code>epoch 1, loss: 0.000299epoch 2, loss: 0.000058epoch 3, loss: 0.000051</code></pre><pre><code class="lang-python"># result comparisiondense = net[0]print(true_w, dense.weight.data)print(true_b, dense.bias.data)</code></pre><pre><code>[2, -3.4] tensor([[ 2.0007, -3.3992]])4.2 tensor([4.1989])</code></pre><h2 id="练习题"><a href="#练习题" class="headerlink" title="练习题"></a>练习题</h2><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/XkyKfEtjT1l95wp.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习基础</title>
      <link href="/2020/02/14/dl-notes/"/>
      <url>/2020/02/14/dl-notes/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo 踩坑记录</title>
      <link href="/2020/02/09/hexo-experience/"/>
      <url>/2020/02/09/hexo-experience/</url>
      
        <content type="html"><![CDATA[<h1 id="Hexo-踩坑记录"><a href="#Hexo-踩坑记录" class="headerlink" title="Hexo 踩坑记录"></a>Hexo 踩坑记录</h1><p>Hexo 作一个优秀的博客框架，本人也是一直参考着网上零零碎碎的博文一步步搭建的，中间遇到不少坑。每次做一个稍大的项目都要经历这种折腾，今后决定把这些经验记录下来沉淀，有未完成的也留给以后思考。</p><h3 id="1-git-分支未知出错"><a href="#1-git-分支未知出错" class="headerlink" title="1. git 分支未知出错"></a>1. git 分支未知出错</h3><p>修改 matery.css 的时候总是没法同步到 <a href="https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io" target="_blank" rel="noopener">https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io</a>… 的远端，后来发现存在分支的问题，于是在主题的 config.yml 中添加一个 <code>@master</code>:</p><pre><code class="lang-yml">jsDelivr:  url: https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io@master</code></pre><p>更改的内容部分可以生效，但仍然有一些冥顽不化的配色，本地已经渲染出来了，远端 jsdelivr 死活不改变，加了 @master 也不变，于是直接在 post/post-detail.ejs 页面中添加 css 代码强行修改:</p><pre><code class="lang-css">#articleContent a {    color: #267871; !important;}#artDetail .post-cate a {    color: #267871; !important;}blockquote {    border-left: 5px solid #267871; !important;}#artDetail .reprint a {    color: #267871; !important;}</code></pre><p>挖个坑，希望今后可以好好研究一下 git</p><h3 id="2-Hexo-无法解析代码块"><a href="#2-Hexo-无法解析代码块" class="headerlink" title="2. Hexo 无法解析代码块"></a>2. Hexo 无法解析代码块</h3><p>比如上面的 css 代码块，Hexo 在解析的时候会把它识别成网页 css 的内容，最后没有显示，只能主动适应 Hexo 这种方式：</p><ol><li>不添加 <code>&lt;style type=&quot;text/css&quot;&gt;</code>这种会与网页混淆的内容</li><li>代码块中不添加(1) (2) 序号</li></ol><h3 id="3-Hexo-代码块无法高亮的问题"><a href="#3-Hexo-代码块无法高亮的问题" class="headerlink" title="3. Hexo 代码块无法高亮的问题"></a>3. Hexo 代码块无法高亮的问题</h3><ol><li><p>主要矛盾：<br>hexo 对于 mathjax 的显示支持有问题，需要安装新的 kmarked 插件，修改对于<code>{, (</code>等等的解析规则，而修改规则支持 Mathjax 后，prism 的高亮功能又会失效，简而言之不可兼得</p></li><li><p>解决：<br>我使用的是<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank" rel="noopener">hexo-matery</a>主题，使用了大家推荐的 hexo-prism-plugin 主题，但是出现和 prism 高亮和 Mathjax 语法不兼容的问题。代码不能高亮简直失去了博客写代码的意义。最后使用 <a href="https://highlightjs.org/" target="_blank" rel="noopener">highlight.js</a>成功实现代码高亮，参考以下这位博主的设置：<a href="http://cps.ninja/2019/03/25/add-highlightjs-to-hexo-blog/" target="_blank" rel="noopener">使用 Highlight.js 优化代码块高亮效果</a></p></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数学基础备忘录</title>
      <link href="/2020/02/07/math-notes/"/>
      <url>/2020/02/07/math-notes/</url>
      
        <content type="html"><![CDATA[<h1 id="数学基础备忘录"><a href="#数学基础备忘录" class="headerlink" title="数学基础备忘录"></a>数学基础备忘录</h1><h2 id="排列组合公式"><a href="#排列组合公式" class="headerlink" title="排列组合公式"></a>排列组合公式</h2><p>(A_{n}^{m}=n(n-1) \cdots(n-m+1))  m个数递减相乘<br>(C_{n}^{m}=\frac{A_{n}^{m}}{m !}=\frac{n !}{m !(n-m) !}=C_{n}^{n-m})</p><p>$A^4_4 = 4\times3\times2\times1 = 24$</p><p>$A^4_4$ 计算方法：从4 开始，从 4 到 3 到 2…连续乘 4 个数。<br>$C^3_4$ 计算方法：（先计算 $A^3_4$ 再得到 $C^3_4$ ）$C^3_4 = \frac{A^3_4}{3!} = \frac{4\times3\times2}{3\times2\times1} = 24\div6 = 4$</p><p>比如 $A^2_4$ 表示从 4 个东西东西中抽取任意 2 样，一共有 $4\times3 = 12$ <code>排列</code>方式。<br>$C^3_4$ 表示从 4 个东西东西中抽取任意 3 样，一共有 $\frac{4\times3\times2}{3\times2\times1} = 24\div6 = 4$ 两种<code>组成</code>方式。</p><p>A 表示排列，是有序的，而 C 表示组合，表示有多少种<code>组成</code>方式，只看成员，不看顺序。</p><p>对于 A（排列）来说，<code>4 3 1</code>和<code>1 3 4</code>是两种构成，而对于 C（组合）来说，<code>4 3 1</code>和<code>1 3 4</code>就是同一回事。</p><p>$C^3_5$ = 多少？ </p>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> math </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习基础(一) 概述</title>
      <link href="/2020/02/06/ml-study-notes/"/>
      <url>/2020/02/06/ml-study-notes/</url>
      
        <content type="html"><![CDATA[<h1 id="机器学习概述"><a href="#机器学习概述" class="headerlink" title="机器学习概述"></a>机器学习概述</h1><h2 id="机器学习分类"><a href="#机器学习分类" class="headerlink" title="机器学习分类"></a>机器学习分类</h2><ol><li>监督学习：已经有数据，和数据对应的标签。</li><li>非监督学习：给定的样本无需输出/标签，让机器自己学习样本中隐含的内部结构。</li><li>半监督学习：二者结合。</li><li>强化学习：通过打分/评价的形式，类似于监督学习中的标签。</li></ol><h2 id="机器学习模型"><a href="#机器学习模型" class="headerlink" title="机器学习模型"></a>机器学习模型</h2><p>机器学习 = 数据 data + 模型 model + 优化方法 optimal strategy</p><h2 id="偏差-方差权衡"><a href="#偏差-方差权衡" class="headerlink" title="偏差/方差权衡"></a>偏差/方差权衡</h2><p>variance 和 bias，分别对应过拟合和欠拟合</p><p>来自 Wikipedia：</p><blockquote><p>在监督学习中，如果能将模型的方差与误差权衡好，那么可以认为该模型的泛化性能（对于新数据）将会表现出好的结果。</p><p>偏差刻画的是算法本身的性能。高偏差将会造成欠拟合(Underfitting) [miss the relevant relations between features and target outputs]。换句话说，模型越复杂偏差就越小；而模型越简单，偏差就越大。</p><p>方差用来衡量因训练集数据波动(fluctuations)而造成的误差影响。高方差将会造成过拟合(Overfitting)。</p></blockquote><p>在周志华老师&lt;机器学习&gt;书中是这样阐述的：</p><blockquote><p><em>偏差</em> 度量了学习算法的期望预测与真实结果的偏离程度，即刻画了算法本身的拟合能力；</p><p><em>方差</em> 度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响(在不同数据集上表现很不稳定);</p><p><em>噪声</em> 则表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题的本身难度</p><p>偏差-方差分解说明，泛化性能是由学习算法的能力、数据的充分性以及学习任务本身的难度所共同决定的。给定的学习任务，为了取得好的泛化性能，则需使偏差较小，即能够充分拟合数据，并且使方差较小，即使数据扰动产生的影响小。一般来说方差与偏差是有冲突的，这称为方差-偏差窘境。</p></blockquote><h2 id="常见机器学习算法概览"><a href="#常见机器学习算法概览" class="headerlink" title="常见机器学习算法概览"></a>常见机器学习算法概览</h2><h3 id="1-Linear-Algorithm-线性算法"><a href="#1-Linear-Algorithm-线性算法" class="headerlink" title="1. Linear Algorithm 线性算法"></a>1. Linear Algorithm 线性算法</h3><ol><li><p><strong>Linear Regression 线性回归</strong>：使用最小二乘法 Least Squares 拟合一条直线 → 计算 R<sup>2</sup> → 计算 R<sup>2</sup> 的 p 值。R<sup>2</sup> 表示 x 能多大程度反映 y 的变化，p 值表示可靠程度。拟合直线的过程使用「随机梯度下降」（SGD）</p></li><li><p><strong>Lasso 回归 和 Ridge 回归</strong>：都可以减少共线性带来的影响，即 X 自变量之间有相互关联。区别可以归结为L2和L1正则化的性质差异。</p></li><li><p><strong>Polynomial Regression 多项式回归</strong>：能够模拟非线性可分的数据（曲线），线性回归不能做到这一点。但容易过拟合。</p></li><li><p><strong>Logistic Regression 逻辑回归</strong>：判断 True or False，Y 值为 0-1 表示概率，用于分类。线性回归使用「Residual 偏差」，而逻辑回归使用「maximum likelihood 最大似然」</p></li></ol><h4 id="逻辑回归回顾"><a href="#逻辑回归回顾" class="headerlink" title="逻辑回归回顾"></a>逻辑回归回顾</h4><ul><li>逻辑回归是一种二分类模型, 给出的是划分为 (0, 1) 的概率. 通过$\theta^Tx$通过一个 sigmoid 函数映射到 0-1 范围内. sigmoid 将一个得分转化为概率输出. 设置一个阈值$h$, 大于$h$则判断为 1.</li></ul><script type="math/tex; mode=display">\begin{array}{l}p_{\theta}(y=1 | x)=\sigma\left(\theta^{T} x\right)=\frac{1}{1+e^{-\theta^{T} x}} \\p_{\theta}(y=0 | x)=\frac{e^{-\theta^{T} x}}{1+e^{-\theta^{T} x}}\end{array}</script><ul><li>均方误差 MSE<script type="math/tex; mode=display">M S E=\frac{1}{n} \sum_{i}^{n}\left(\hat{y}_{i}-y_{i}\right)^{2}</script>标准差 RMSE <script type="math/tex">RMSE=\sqrt{MSE}</script></li></ul><blockquote><p>使用MSE的一个缺点就是其偏导值在输出概率值接近0或者接近1的时候非常小，这可能会造成模型刚开始训练时，偏导值几乎消失. 这导致模型在一开始学习的时候速率非常慢，而使用交叉熵作为损失函数则不会导致这样的情况发生.</p></blockquote><ul><li>交叉熵损失函数<br>该函数是<code>凸函数</code>，求导时能够得到全局最优值. (记得前面有一个负号)<br>多分类情况下:<script type="math/tex; mode=display">L=\frac{1}{N} \sum_{i} L_{i}=\frac{1}{N} \sum_{i}-\sum_{c=1}^{M} y_{i c} \log \left(p_{i c}\right)</script>在二分的情况下，模型最后需要预测的结果只有两种情况，对于每个类别我们的预测得到的概率为 p 和 1-p. 此时表达式为：<script type="math/tex; mode=display">L=\frac{1}{N} \sum_{i} L_{i}=\frac{1}{N} \sum_{i}-\left[y_{i} \cdot \log \left(p_{i}\right)+\left(1-y_{i}\right) \cdot \log \left(1-p_{i}\right)\right]</script>其中：<br>$y_i$ —— 表示样本i的label，正类为1，负类为0<br>$p_i$ —— 表示样本i预测为正的概率<br><code>pi</code> 就是 sigmoid 函数输出的结果, 也可以写作为:<script type="math/tex; mode=display">\mathcal{L}\left(y, x, p_{\theta}\right)=-y \log \sigma\left(\theta^{T} x\right)-(1-y) \log \left(1-\sigma\left(\theta^{T} x\right)\right)</script>对应的梯度函数(导数):<script type="math/tex; mode=display">\frac{\partial \mathcal{L}\left(y, x, p_{\theta}\right)}{\partial \theta}=(\sigma(\theta^{T} x)-y) x</script>给定一个学习率 $\eta$, 步长 = 梯度 x 学习率, 那么 $\theta$ 学习/更新的过程就是:<script type="math/tex; mode=display">\theta \leftarrow \theta+\eta\left(y-\sigma\left(\theta^{T} x\right)\right) x</script></li><li>逻辑回归打分函数:<script type="math/tex; mode=display">s(x)=\theta_0+\theta_1x_1+\theta_2x_2</script>也就是$\theta^{T} x$ ,也就是 $b+w^Tx$ . 在直线的上方, 函数值就会大于 0, sigmoid 的函数值就会越接近 1, 分类归为 1. 打分越高, 越远离决策边界, 具有更高的分类<code>置信度</code>.</li></ul><h3 id="2-Decision-Tree-决策树"><a href="#2-Decision-Tree-决策树" class="headerlink" title="2. Decision Tree 决策树"></a>2. Decision Tree 决策树</h3><ol><li><p><strong>ID3</strong>: 计算「信息熵」 $Entropy(D)$，值越小，说明样本集合D的纯度就越高，进而选择用样本的某一个属性a来划分样本集合D时，就可以得出用属性a对样本D进行划分所带来的「信息增益」 $Gain(D, a)$，值越大，说明如果用属性a来划分样本集合D，那么纯度会提升。 <script type="math/tex">Entropy(t)=-\sum_{k} p\left(c_{k} | t\right) \log p\left(c_{k} | t\right)</script>  <script type="math/tex">Classificationerror (t)=1-\max _{k}\left[p\left(c_{k} | t\right)\right]</script></p></li><li><p><strong>C4.5</strong>: 提出Gainratio 「增益率」，解决ID3决策树的一个缺点，当一个属性的可取值数目较多时，那么可能在这个属性对应的可取值下的样本只有一个或者是很少个，那么这个时候它的信息增益是非常高的，这个时候纯度很高，ID3决策树会认为这个属性很适合划分，但是较多取值的属性来进行划分带来的问题是它的泛化能力比较弱。用 $I(·)$ 表示不纯度——可以是熵可以是基尼，信息增益：<script type="math/tex">\Delta=I(\text { parent })-\sum_{i=1}^{n} \frac{N\left(a_{i}\right)}{N} I\left(a_{i}\right)</script>信息增益率：<script type="math/tex">Gainratio =\frac{\Delta}{Entropy({parent})}</script></p></li><li><p><strong>CART(Classification and Regression Tree)</strong>: 通过计算 Gini 基尼系数（尽可能小），判断 impurity 不纯洁度。离散数据用「是否」划分子树，连续数据可以用「两两之间平均值」划分子树。<script type="math/tex">{Gini}(t)=1-\sum_{k}\left[p\left(c_{k} | t\right)\right]^{2}</script>D 分裂为 DL 和 DR，分裂后的信息增益<script type="math/tex">Gain(D, A)=\frac{\left|D_{L}\right|}{|D|} \operatorname{Gini}\left(D_{L}\right)+\frac{\left|D_{R}\right|}{|D|} \operatorname{Gini}\left(D_{R}\right)</script></p></li></ol><h3 id="3-SVM-支持向量机"><a href="#3-SVM-支持向量机" class="headerlink" title="3. SVM 支持向量机"></a>3. SVM 支持向量机</h3><h4 id="找到最小间隔"><a href="#找到最小间隔" class="headerlink" title="找到最小间隔"></a>找到最小间隔</h4><ul><li>$y_i$ 取 -1 或 1</li><li>思想: 找到离 decision boundry 最近的点, 找到 w 和 b, 让最小的 margin 获得比较大的值. <code>最大化最小的几何间隔</code></li><li>点到直线的距离:<script type="math/tex; mode=display">d=\frac{\left|A x_{0}+B y_{0}+C\right|}{\sqrt{A^{2}+B^{2}}}</script>即<script type="math/tex; mode=display">d=\frac{\left|(A, B)\left(\begin{array}{c}x 0 \\y 0\end{array}\right)+C\right|}{\|(A, B)\|}</script>如果数据是 n 维时，此时直线就变成了平面，用 $w$ 表示平面的法向量得：<script type="math/tex; mode=display">d=\frac{|w \cdot x+b|}{\|w\|}</script></li><li><strong>函数间隔</strong>:<br>由于分母${|w|}$都是相同的, 我们只需比较分子, 于是有了函数间隔<script type="math/tex; mode=display">\hat{\gamma}^{(i)}=y^{(i)}\left(w^{T} x^{(i)}+b\right).</script>乘以 $y^{(i)}$ 是为了<code>取绝对值</code>, 非 -1 即 1</li><li><p><strong>几何间隔</strong>:<br>w 和 b 的赋值可以同时改变, 比如同时$*2$, $\hat{y}^{(i)}$ 值会变大, 但是分割超平面不会变化 (因为=0的解是一样的)<br>所以需要对法向量加某些约束, 我们可以除以一个||w||, 作为 normalizaiton<code>归一化</code>, 使得<code>函数间隔 -&gt; 几何间隔</code>. 即对 $w$ 使用<code>L2范数</code>进行规范化，使得间隔是确定的.</p><script type="math/tex; mode=display">\hat{\gamma}^{(i)}=y^{(i)}\left[\left(\frac{w}{\|w\|}\right)^{T} x^{(i)}+\frac{b}{\|w\|}\right]</script><p>给定一个训练集, 就可以依次计算 $\hat{\gamma}^{(i)}$, 选出最小的几何间隔 ${\gamma}$, 作为整个数据集到超平面的几何间隔. 满足这个几何间隔的点, 就是<code>支撑向量</code>.</p></li><li><p><strong>优化目标</strong>:<br>对于几何间隔</p><script type="math/tex; mode=display">\begin{aligned}&\max _{\gamma, \omega, b} \gamma\\&\text { s.t. } \quad y^{(i)}\left(w^{T} x^{(i)}+b\right) \geq \gamma, \quad i=1, \dots, m\\\end{aligned}</script><p>限定的条件是 $||w||=1$, 也就是 w 在一个环上, 不是一个凸集, 所以是一个<code>非凸约束</code>.<br>等同于优化一个<code>非凸的目标函数</code>, 这种情况下,  $||w||$ 作为分母, 仍然是一个非凸函数.(如果是分子, 就是凸函数)</p><script type="math/tex; mode=display">\begin{aligned}&\max _{\gamma, w, b} {\frac{\hat{\gamma}}{\|w\|}}\\&\text { s.t. } \quad y^{(i)}\left(w^{T} x^{(i)}+b\right) \geq \hat{\gamma}, \quad i=1, \ldots, m\end{aligned}</script></li><li><p>调整后的目标函数:<br>由于分类间隔的变化, 即 w, b 的放缩不会影响决策边界, 将函数间隔<code>固定为 1</code>, 即$\hat{\gamma}=1$, 目标函数重写为 (二次规划)</p><script type="math/tex; mode=display">\max _{w, b} \frac{1}{\|w\|}</script><p>等同于</p><script type="math/tex; mode=display">\begin{aligned}&\min _{w, b} \frac{1}{2}{\|w\|}^2\\&\text { s.t. } \quad y^{(i)}\left(w^{T} x^{(i)}+b\right) \geq \hat{\gamma}, \quad i=1, \ldots, m\end{aligned}</script><blockquote><p>用直线画出来的区域/函数一定是一个凸集</p></blockquote></li></ul><h4 id="拉格朗日乘子法"><a href="#拉格朗日乘子法" class="headerlink" title="拉格朗日乘子法"></a>拉格朗日乘子法</h4><p>对于凸优化问题</p><script type="math/tex; mode=display">\begin{array}{ll}\min _{w} & f(w) \\\text { s.t. } & h_{i}(w)=0, \quad i=1, \dots, l\end{array}</script><p>拉格朗日函数定义为</p><script type="math/tex; mode=display">\mathcal{L}(w, \beta)=f(w)+\sum_{i=1}^{l} \beta_{i} h_{i}(w)</script><blockquote><p>其中 $\beta_{i}$ 称之为拉格朗日乘子</p></blockquote><p>问题的解, 对 $w$ 和 $\beta$ 分别求导 =0 即可</p><script type="math/tex; mode=display">\frac{\partial \mathcal{L}(w, \beta)}{\partial w}=0 \quad \frac{\partial \mathcal{L}(w, \beta)}{\partial \beta}=0</script><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/27/WFAhMKzVsPD7xZJ.png" alt="拉格朗日法-原理解析"><br>解求得的点, 就是 $h(w)=0$ 与等高线的一个切点, 在其他任何地方, $f(w)$ 都会更大. 在这里, $f(w)$ 即为 $ \frac{1}{2}{|w|}^2$</p><h4 id="通用凸优化问题"><a href="#通用凸优化问题" class="headerlink" title="通用凸优化问题"></a>通用凸优化问题</h4><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/28/97GTvousRYj8S1y.png" alt="凸优化问题"><br>满足约束条件的情况下, 后面两个式子一定 $=0$, 当约束条件不满足条件下, 一定能取 $\alpha_{i}$ 和 $\beta_{i}$, 最终拉格朗日函数 $=+\infty$<br>最后原始问题即为:</p><script type="math/tex; mode=display">\theta_{\mathcal{P}}(w)=\left\{\begin{array}{ll}f(w) & \text w 满足原始问题约束 \\+\infty & 其他\end{array}\right.</script><p>函数最小化问题</p><script type="math/tex; mode=display">\min _{w} {\theta}_{\mathcal{P}}(w)=\min _{w} \max _{\alpha, \beta: \alpha_{i} \geq 0} \mathcal{L}(w, \alpha, \beta)</script><p>等同于原来的优化任务</p><script type="math/tex; mode=display">\begin{array}{ll}\min _{w} & f(w) \\\text { s.t. } & g_{i}(w) \leq 0, \quad i=1, \ldots, k \\& h_{i}(w)=0, \quad i=1, \ldots, l\end{array}</script><p>原始问题的解即为</p><script type="math/tex; mode=display">p^\star=\min _{w} \theta_{\mathcal{P}}(w)</script><p>求解 $f(w)$ 之所以添加一个 $\theta_{\mathcal{P}}(w)$, 就是为了省略掉 $g_{i}(w)$ 和 $h_{i}(w)=0$ 这些限制条件, 最后简洁地表示成一个函数.</p><h4 id="对偶问题-amp-KKT"><a href="#对偶问题-amp-KKT" class="headerlink" title="对偶问题 &amp; KKT"></a>对偶问题 &amp; KKT</h4><p>将研究的参数转化为 $\alpha$ 和 $\beta$</p><script type="math/tex; mode=display">\max _{\alpha, \beta: \alpha_{i} \geq 0} \theta_{\mathcal{D}}(\alpha, \beta)=\max _{\alpha, \beta: \alpha_{i} \geq 0} \min _{w} \mathcal{L}(w, \alpha, \beta)</script><p>对偶问题的解为</p><script type="math/tex; mode=display">d^\star=\max _{\alpha, \beta: \alpha_{i} \geq 0} \min _{w} \mathcal{L}(w, \alpha, \beta)</script><p>两者之间的关系 (恒成立)</p><script type="math/tex; mode=display">d^\star=\max _{\alpha, \beta: \alpha_{i} \geq 0} \min _{w} \mathcal{L}(w, \alpha, \beta) \leq \min _{w} \max _{\alpha, \beta: \alpha_{i} \geq 0} \mathcal{L}(w, \alpha, \beta)=p^\star</script><p>满足一定条件, 可以使 $d^ \star =p^ \star $, 即 <code>KKT 条件</code><br>前提:</p><ul><li>假设 $f$ 以及 $g_i$ 是凸函数,并且 $h_i$ 为仿射函数,且 $g_i$ 严格满足可行域</li><li>必然存在 $(w^\star ,\alpha^\star ,\beta^\star )$, 满足: $w_\star $ 是原始问题的解, $\alpha^\star $ , $\beta^\star $ 是对偶问题的解, 且两个问题的解数值相等$d^ \star =p^ \star $</li><li>同时, $(w^\star ,\alpha^\star ,\beta^\star )$ 满足KKT条件<script type="math/tex; mode=display">\begin{aligned}&\frac{\partial}{\partial w_{i}} \mathcal{L}\left(w^{\star}, \alpha^{\star}, \beta^{\star}\right)=0, i=1, \dots, n\\&\frac{\partial}{\partial \beta_{i}} \mathcal{L}\left(w^{\star}, \alpha^{\star}, \beta^{\star}\right)=0, i=1, \ldots, l\\&\alpha_{i}^{\star} g_{i}\left(w^{\star}\right)=0, i=1, \dots, k\\&g_{i}\left(w^{\star}\right) \leq 0, i=1, \dots, k\\&{\alpha}^{\star} \geq 0, i=1, \ldots, k\end{aligned}</script></li></ul><p>对 $w$ 和 对 $\beta_i$ 导数均为 0, 才能让他俩的梯度共线, 从而找到切点.<br>其中 $\alpha_{i}^ \star  g_{i}\left(w^ \star \right)=0, i=1, \dots, k$ 称为<code>KKT对偶互补条件</code><br>因为原始的约束中, $g_{i}\left(w^ \star \right)\leq0$, 是有可能 $&lt;0$ 的, 这个时候 $\alpha_i$ 一定要 $=0$; 当 $\alpha_i\ne0$,  这个时候 $g_{i}\left(w^ \star \right)$ 一定要 $=0$</p><h4 id="SVM-求解推导"><a href="#SVM-求解推导" class="headerlink" title="SVM 求解推导"></a>SVM 求解推导</h4><ul><li><strong>第一步: 利用拉格朗日对偶, 确定目标函数</strong></li><li>目标函数: <code>寻找最优间隔分类器</code><br>由于已经固定 $\hat{\gamma}=1$, 也就意味着到超平面的距离为1的点都是支持向量<br><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/28/kOjBcHx6t8zZFNa.png" alt="支持向量"><br>代入 $\hat{\gamma}=1$, 问题变成<script type="math/tex; mode=display">\begin{aligned}&\min _{w, b} \frac{1}{2}{\|w\|}^2\\&\text { s.t. } \quad y^{(i)}\left(w^{T} x^{(i)}+b\right) \geq 1, \quad i=1, \ldots, m\end{aligned}</script>构造拉格朗日函数<script type="math/tex; mode=display">L(w, b, \alpha)=\frac{1}{2}\|w\|^2-\sum_{i=1}^{n} \alpha_{i}\left(y_{i}\left(w \cdot x_{i}+b\right)-1\right), \alpha_{i} \geq 0</script></li><li>$\alpha&gt;0$ 成立的条件, 仅当 $x_i$ 卡在支持向量上, 即 $g_{i}\left(w^\star\right)=0$ 的时候, 也即函数间隔 $\hat{\gamma}=1$ 的时候. 其他点都没有影响, 对应的 $\alpha_i=0$. 这也说明, 分割平面 $w$ 的确定, 只取决于全局函数间隔最小的这几个点的位置.</li><li>只要满足 KKT 条件, 求解对偶问题, 相当于间接地求解了原问题<br>现在需要最大化 $ L(w,b,\alpha )$，所以问题变成 <script type="math/tex; mode=display">\min _{w, b} \max _{\alpha} L(w, b, \alpha)</script>根据拉格朗日对偶性, 问题变成<script type="math/tex; mode=display">\max _{\alpha} \min _{w, b} L(w, b, \alpha)</script></li><li><p><strong>第二步: 求  $ \mathop {\min }\limits_{w,b} L(w,b,\alpha )$</strong><br>让  $L(w,b,\alpha)$  分别对 $w,b$ 求偏导，使导数为 0：<br>$ \frac{ {\partial L(w,b,\alpha )} }{ {\partial w} } = w - \sum\limits_{i = 1}^n { {\alpha _i}{y_i}{x_i} } = 0$  ————&gt;  $ w = \sum\limits_{i = 1}^n { {\alpha _i}{y_i}{x_i} }$<br>$ \frac{ {\partial L(w,b,\alpha )} }{ {\partial b} } = \sum\limits_{i = 1}^n { {\alpha _i}{y_i} } = 0$ ————————-&gt; $\sum\limits_{i = 1}^n { {\alpha _i} {y_i} } = 0 $<br>代入原式$L(w, b, \alpha)$可得</p><script type="math/tex; mode=display">\min _{w, b} L(w, b, \alpha)=-\frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} x_{j}\right)+\sum_{i=1}^{n} \alpha_{i}</script></li><li><p><strong>第三步:  求 $ \mathop {\min }\limits_{w,b} L(w,b,\alpha )$ 对 $ \alpha $ 的极大值 - SMO</strong><br>问题变为： </p><script type="math/tex; mode=display">\mathop {\max }\limits_\alpha \ \ - \frac{1}{2}\sum\limits_{i = 1}^n {\sum\limits_{j = 1}^n { {\alpha _i}{\alpha _j}{y_i}{y_j}({x_i}{x_j})} } + \sum\limits_{i = 1}^n { {\alpha _i}}</script><p>约束条件为： <script type="math/tex">\sum\limits_{i = 1}^n { {\alpha _i}{y_i}} = 0，  {\alpha _i} \ge 0,i = 1,2...n</script><br>所以只要知道 $ {\alpha }$， 也就知道了$w$. 我们使用 SMO 算法, <code>假设已经求出</code>最优解为 $ {\alpha^ \star  } = {(\alpha _1^ \star  ,\alpha _2^ \star  ….\alpha _n^ \star  )^T}$</p></li><li><p><strong>第四步:  求 ${w^ \star }$ 和 $b^ \star $ 得到超平面</strong><br>已知最优的 ${\alpha ^ \star }$, 由第一步 $w=\sum\limits_{i = 1}^n {\alpha _i}{y_i}{x_i}$ 可求出最优的 ${w^ \star  } $ </p><script type="math/tex; mode=display">{w^\star } = \sum\limits_{i = 1}^n {\alpha _i^ \star  {y_i}{x_i}}</script><p>由 KKT 条件中的<code>对偶互补条件</code>, 至少存在一个 $ \alpha _i^ \star &gt;0 $ 使得 $\alpha_{i}^ \star  g_{i}\left(w^ \star \right)=0$, 即</p><script type="math/tex; mode=display">\alpha_{i}^ \star \left(y_{i}\left(w^ \star  \cdot x_{i}+b^ \star \right)-1\right)=0, i=1,2 \ldots n</script><p>可以证明至少存在一个 $\alpha _j^  \star &gt;0$ 使等式成立，所以可得：</p><script type="math/tex; mode=display">{y_j}({w^ \star }\cdot {x_j} + {b^ \star }) - 1 = 0</script><p>因为 $ {y_i} = \{ + 1, - 1\}$ ，所以 $\frac{1}{ { {y_i}}} = {y_i}$ 进而得到</p><script type="math/tex; mode=display">b^ \star =y_{j}-\sum_{i=1}^{n} \alpha_{i}^ \star  y_{i}\left(x_{i} \cdot x_{j}\right)</script><p>至此 $ w^ \star  和  b^ \star  $ 都已经得到，所以可以得到<code>超平面</code>：$ {w^ \star }\cdot x + {b^ \star } = 0$</p></li></ul><h4 id="SMO-序列最小优化"><a href="#SMO-序列最小优化" class="headerlink" title="SMO 序列最小优化"></a>SMO 序列最小优化</h4><ul><li>上面我们只是假设已经求得了 $ {\alpha ^ \star } = {(\alpha _1^ \star ,\alpha _2^ \star ….\alpha _n^ \star )^T}$，用到的方法是<code>SMO（Sequential Minimal Optimization）即序列最小优化</code>，感觉SMO其实不属于支持向量机算法的一部分.</li><li>SMO的思想<br>拉格朗日乘子 $\alpha$ 有许多个，如果采用单个 $\alpha$ 坐标上升法, 无法满足 $\sum_{i=1}^{m} \alpha_{i} y^{(i)}=0$ 的条件.<br>于是, 每次只选择其中<code>两个</code>变量作为未知量，其他乘子作为已知量，将N个问题转变成两个变量的求解问题. 当 $W(\alpha)$ 的变化小于一个阈值, 比如 0.01, 可以判定为收敛. 比如对于线性可分支持向量机的目标函数：<script type="math/tex; mode=display">\mathop {\max }\limits_\alpha \ \ - \frac{1}{2}\sum\limits_{i = 1}^n {\sum\limits_{j = 1}^n { {\alpha _i}{\alpha _j}{y_i}{y_j}({x_i}{x_j})} } + \sum\limits_{i = 1}^n { {\alpha _i}}</script>约束条件为： <script type="math/tex">\sum\limits_{i = 1}^n { {\alpha _i}{y_i}} = 0，  {\alpha _i} \ge 0,i = 1,2...n</script><br>一开始随机选取两个变量 $ {\alpha _1},{\alpha _2} $作为未知量，其他因子 ${\alpha _i}(i = 3,4…n)$ 当作已知来求目标函数的最小值，求出 ${\alpha _1},{\alpha _2}$ 再带入进去，再选取 $ {\alpha _3},{\alpha _4}$ 求目标函数最小值，求出后继续带入，直到目标函数收敛.<br>由于每一对 $(\alpha_i, \alpha_j)$ 都互相可以表示出来, 最终转化为以单个变量 $\alpha_j$ 的二次函数优化问题, 用二次函数即可求解.<script type="math/tex; mode=display">\begin{array}{c}\max _{\alpha_{2}} W\left(\alpha_{2}\right)=a \alpha_{2}^{2}+b \alpha_{2}+c \\\text { s.t. } 0 \leq \alpha_{2} \leq C\end{array}</script></li></ul><h4 id="核函数方法-Kernel-Function"><a href="#核函数方法-Kernel-Function" class="headerlink" title="核函数方法 Kernel Function"></a>核函数方法 Kernel Function</h4><ul><li>处理线性不可分的情况, 增加一个<code>松弛变量</code> $\xi$, 允许函数间隔小于 1, 甚至小于 0. <script type="math/tex; mode=display">\begin{array}{l}\min _{w, b, \xi} \frac{1}{2}\|w\|^{2}+c \sum_{i=1}^{m} \xi_{i} \\\\\text { s.t. } y^{(i)}\left(w^{T} x^{(i)}+b\right) \geq 1-\xi_{i}, \quad i=1, \ldots, m \\\quad \xi_{i} \geq 0, \quad i=1, \ldots, m\end{array}</script></li><li>拉格朗日函数<script type="math/tex; mode=display">\mathcal{L}(w, b, \xi, \alpha, r)=\frac{1}{2} w^{T} w+C \sum_{i=1}^{m} \xi_{i}-\sum_{i=1}^{m} \alpha_{i}\left[y^{(i)}\left(x^{T} w+b\right)-1+\xi_{i}\right]-\sum_{i=1}^{m} r_{i} \xi_{i}</script></li><li>对偶问题: 此时 $\alpha$ 不再是 &gt;= 0, 而是 $0 \leq \alpha \leq C$<script type="math/tex; mode=display">\begin{aligned}&\max _{\alpha} W(\alpha)=\sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i, j=1}^{m} y^{(i)} y^{(j)} \alpha_{i} \alpha_{j} x^{(i)^{T}} x^{(j)}\\&\text { s.t. } 0 \leq \alpha_{i} \leq C, \quad i=1, \ldots, m\\&\sum_{i=1}^{m} \alpha_{i} y^{(i)}=0\end{aligned}</script></li><li>基本思想: 将原来的 x 空间中的特征, 映射到 $\phi(x)$ 高维空间中. 原本只着眼于内积运算 $x^{(i)^{T}} x^{(j)}$, 给定 $\phi(x)$, 变成<code>核函数</code>运算 <script type="math/tex; mode=display">K\left(x^{(i)}, x^{(j)}\right)=\phi\left(x^{(i)}\right)^{T} \phi\left(x^{(j)}\right)</script>目标函数变成<script type="math/tex; mode=display">W(\alpha)=\sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i, j=1}^{m} y^{(i)} y^{(j)} \alpha_{i} \alpha_{j} K\left(x^{(i)}, x^{(j)}\right)</script></li><li><code>核技巧</code> Kernel Trick<br>多数情况下, 可以直接定义  $K\left(x^{(i)}, x^{(j)}\right)$ 而不需要 $\phi(x)$, 比如直接假定 $K\left(x^{(i)}, x^{(j)}\right)=\left(x^{(i)^{T}} x^{(j)}\right)^{2}$<br>时间复杂度可以明显降低, 比如从 $O(n^2)$ 降低到  $O(n)$</li><li>给定核函数, 再通过 SMO 即可求得 $\alpha$, 再通过 $\alpha$ 求得 $w$ 和 $b$, $\phi(x)$ 的计算被约去, 得到函数间隔/目标函数为<script type="math/tex; mode=display">\begin{aligned}w^{ \star  T} x+b^ \star  &=\left(\sum_{i=1}^{m} \alpha_{i} y^{(i)} \phi\left(x^{(i)}\right)\right)^{T} \phi(x)+b^ \star  \\&=\sum_{i=1}^{m} \alpha_{i} y^{(i)} K\left(x^{(i)}, x\right)+b^ \star \end{aligned}</script></li><li>高斯核函数 - 非常常用<br>对于 x 和 z 两个样例, 我们需要刻画其<code>相似性度量</code><script type="math/tex; mode=display">K(x, z)=\exp \left(-\frac{\|{x}-z \|^{2}}{2 \sigma^{2}}\right)</script>也称为<code>径向基函数</code>(RBF)核. 将其中一个 x 设为高斯分布的中心点, 计算 z 的概率密度</li><li>核矩阵K<script type="math/tex; mode=display">\left\{K_{i, j}\right\}_{i, j=1, \ldots, m}</script></li><li>有效核: 一定能写成两个 $\phi(x)$ 函数相乘, 一定是对称矩阵, 半正定矩阵 —&gt; 对于任何向量 z, $z^TKz\geq0$. 常见的有 RBF 核, 多项式核$K(x, z)=\left(x^{T} z\right)^{d}$, 余弦相似度核$K(x, z)=\frac{x^{T} z}{|x| \cdot|z|}$, sigmoid 核等. 使用 Sigmoid核的 SVM, 相似于于一个二层的感知机.</li><li>核函数, 同样可以映射到<code>广义的线性模型</code>, 只要涉及到映射 $\phi(x)$, 就可以通过 Kernel Trick 免去计算 ${\Phi}^{T} {\Phi}$, 而直接计算核矩阵<br><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/29/gyzILRlJv1SjseU.png" alt="广义线性模型的核技巧"></li></ul><h4 id="SVM-总结"><a href="#SVM-总结" class="headerlink" title="SVM 总结"></a>SVM 总结</h4><ul><li><p>SVM 独到的特点<br>不需要去具体计算 $w$ 和 $b$ 的值是多少, 新进来需要预测的点, 只需要计算<code>和已知的支持向量点的内积</code>即可, 即只需要计算两个点之间的相似度, 而不需要去具体计算支持向量是什么.</p></li><li><p>SVM 的损失函数<br>使用铰链损失 <code>Hinge Loss</code>, 只关注函数间隔 &lt;=1 的情况, &gt;1 的情况的 loss 忽略为 0. 而逻辑回归即使计算所得的 p 再高, 也依然要计算 loss.<br><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/28/OL46fhGWiAIQPgH.png" alt="损失函数对比"></p></li><li><p>SVM 分类</p></li></ul><ol><li>硬间隔支持向量机（线性可分支持向量机）：当训练数据线性可分时，可通过硬间隔最大化学得一个线性可分支持向量机</li><li>软间隔支持向量机：当训练数据近似线性可分时，可通过软间隔最大化得到一个线性支持向量机</li><li>非线性支持向量机：当训练数据线性不可分时，可通过核方法以及软间隔最大化得一个非线性支持向量机</li></ol><ul><li><p>优点：<br>SVM在中小量样本规模的时候容易得到数据和特征之间的非线性关系，可以避免使用神经网络结构选择和局部极小值问题，可解释性强，可以解决高维问题</p></li><li><p>缺点：<br>SVM对缺失数据敏感，对非线性问题没有通用的解决方案，核函数的正确选择不容易，计算复杂度高，主流的算法可以达到O(n2)O(n2)的复杂度，这对大规模的数据是吃不消的</p></li><li><p><a href="https://www.kaggle.com/zhpan92/boyu-svm" target="_blank" rel="noopener">代码实践👆(SMO 等具体细节)</a></p></li></ul><p>调用 scikit-learn 为例, 使用 rbf 核函数</p><pre><code class="lang-py">from sklearn import svm# x, y = simple_synthetic_data(50, 5, 5)x, y = spiral_data()model = svm.SVC(kernel=&#39;rbf&#39;, gamma=50, tol=1e-6)model.fit(x, y)fig = plt.figure(figsize=(6,6))ax = fig.add_subplot(111)plot(ax, model.predict, x, &#39;SVM + RBF&#39;)plt.show()</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/29/KcCRFjglUkh3Gos.png" alt=""></p><h3 id="4-Naive-Bayes-Algorithms-朴素贝叶斯"><a href="#4-Naive-Bayes-Algorithms-朴素贝叶斯" class="headerlink" title="4. Naive Bayes Algorithms 朴素贝叶斯"></a>4. Naive Bayes Algorithms 朴素贝叶斯</h3><ol><li>Naive Bayes</li><li>Gaussian Naive Bayes</li><li>Multinomial Naive Bayes</li><li>Bayesian Belief Network (BBN)</li><li>Bayesian Network (BN)</li></ol><p>朴素贝叶斯基本公式：$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$<br><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/09/J46IjSoqFuD1vtW.png" alt="J46IjSoqFuD1vtW"></p><h3 id="5-KNN-k-NearestNeighbor-K-最邻近算法"><a href="#5-KNN-k-NearestNeighbor-K-最邻近算法" class="headerlink" title="5. KNN (k-NearestNeighbor) K 最邻近算法"></a>5. KNN (k-NearestNeighbor) K 最邻近算法</h3><p>用于分类</p><ol><li>计算测试数据与各个训练数据之间的距离；</li><li>按照距离的递增关系进行排序；</li><li>选取距离最小的K个点；</li><li>确定前K个点所在类别的出现频率；</li><li>返回前K个点中出现频率最高的类别作为测试数据的预测分类</li></ol><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/09/ScAOKyPqMGWdrLH.png" alt="KNN 算法"></p><h3 id="6-Clustering-Algorithm-聚类算法"><a href="#6-Clustering-Algorithm-聚类算法" class="headerlink" title="6. Clustering Algorithm 聚类算法"></a>6. Clustering Algorithm 聚类算法</h3><ol><li>k-Means：选取平均值</li><li>k-Medians：由选取平均值改为选取中位数</li><li>Expectation Maximisation (EM)：有隐含随机变量的概率模型的参数的估计方法，它是一种无监督的算法</li><li><p>Hierarchical Clustering 层次聚类：<br>算法如下：</p><p>(1) 将每个对象看作一类，计算两两之间的最小距离；</p><p>(2) 将距离最小的两个类合并成一个新类；</p><p>(3) 重新计算新类与所有类之间的距离；</p><p>(4) 重复(2)、(3)，直到所有类最后合并成一类。</p></li></ol><h3 id="7-K-means-算法"><a href="#7-K-means-算法" class="headerlink" title="7. K-means 算法"></a>7. K-means 算法</h3><p>算法如下：</p><pre><code>选取k个初始质心(作为初始cluster);repeat:    对每个样本点，计算得到距其最近的质心，将其类别标为该质心所对应的cluster;    重新计算k个cluser对应的质心;until 质心不再发生变化</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/09/VLCReWo92QXSNP4.jpg" alt=" K-means"></p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/05/11/xwHcWKXLUofrZdM.png" alt="KNN 与 K-means 的区别"></p><h3 id="8-Random-Forest-随机森林"><a href="#8-Random-Forest-随机森林" class="headerlink" title="8. Random Forest 随机森林"></a>8. Random Forest 随机森林</h3><h3 id="9-Dimensionality-Reduction-Algorithms-降维算法-PCA"><a href="#9-Dimensionality-Reduction-Algorithms-降维算法-PCA" class="headerlink" title="9. Dimensionality Reduction Algorithms 降维算法 PCA"></a>9. Dimensionality Reduction Algorithms 降维算法 PCA</h3><p>目的: 将数据的某一维的特征差异进行放大, 其他维度进行压缩甚至忽略. 也可以选择降到 k 维.</p><h4 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h4><ol><li><p>将数据集的中心移到 0</p><script type="math/tex; mode=display">\mu=\frac{1}{m} \sum_{i=1}^{m} x^{(i)}</script><script type="math/tex; mode=display">x^{(i)} \leftarrow x^{(i)}-\mu</script></li><li><p>统一每个变量的方差</p><script type="math/tex; mode=display">\sigma_{j}^{2}=\frac{1}{m} \sum_{i=1}^{m}\left(x_{j}^{(i)}\right)^{2}</script><script type="math/tex; mode=display">x_{j}^{(i)} \leftarrow x_{j}^{(i)} / \sigma_{j}</script></li></ol><h4 id="推导过程"><a href="#推导过程" class="headerlink" title="推导过程"></a>推导过程</h4><ul><li>利用特征值与特征向量, 找到一组标准正交基 —-&gt; 从而可以将任何向量用标准正交基表示 —-&gt; 从而可以将任何协方差矩阵用 $UWU^T$ 形式表示, U 为正交基矩阵, W 就是本矩阵的特征值.</li><li>PCA 的目的是找到一个方向 $v$, 使得在这个方向上数据 X 的投影的<code>方差最大</code>, 方差越大越能保留尽可能多的有效信息. 由于均值已经为 0, 数据 $X$ 在方向 $v$ 的方差就是 $||Xv||^2$. 最后推算的结论就是, 需要<code>最大特征值</code>对应的<code>特征向量</code>.</li><li>最终得到 X 的主成分矩阵为 $T=WX$, 其中 $W$ 为$ X^TX$ 的特征值矩阵<br><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/03/01/wXQY5E4Vpfb9SWN.png" alt=""><br><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/03/01/pR8nGAxBsk7oNrv.png" alt=""></li></ul><h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><p>主要步骤, 给定数据 X, 进行均值归 0, 方差统一处理后, 得到 Z. 然后根据协方差矩阵 $Z^TZ$ 的特征值从大到小进行排序, 得到特征向量的矩阵 $V^ \star $. 计算 $Z^ \star  = ZV^ \star<br>$</p><p><a href="https://www.kaggle.com/coverdark/pca-principal-component-analysis/" target="_blank" rel="noopener">👆PCA 实现</a></p><pre><code class="lang-py">def pca_custom(X, k):    n, p = np.shape(X)    # 设置均值为 0    Z = X - np.mean(X, axis=0)    covZ = np.cov(Z.T) # 协方差矩阵按列计算, 需要转置一下    eigValue, eigVector = np.linalg.eig(covZ)  # 计算特征值, 特征向量    index = np.argsort(-eigValue)    if k &gt; p:        print (&quot;k must lower than input data dimension！&quot;)        return 1    else:        # 要降到 k 维, 选择前 k 个最大的        selectVector = eigVector[:, index[:k]]        T = np.matmul(Z, selectVector)    return T, selectVector</code></pre><h3 id="10-Gradient-Boosting-algorithms-梯度提升算法"><a href="#10-Gradient-Boosting-algorithms-梯度提升算法" class="headerlink" title="10. Gradient Boosting algorithms 梯度提升算法"></a>10. Gradient Boosting algorithms 梯度提升算法</h3><ol><li>GBM</li><li>XGBoost</li><li>LightGBM</li><li>CatBoost</li></ol><h3 id="11-Deep-Learning-Algorithms-深度学习"><a href="#11-Deep-Learning-Algorithms-深度学习" class="headerlink" title="11. Deep Learning Algorithms 深度学习"></a>11. Deep Learning Algorithms 深度学习</h3><ol><li>Convolutional Neural Network (CNN)</li><li>Recurrent Neural Networks (RNNs)</li><li>Long Short-Term Memory Networks (LSTMs)</li><li>Stacked Auto-Encoders</li><li>Deep Boltzmann Machine (DBM)</li><li>Deep Belief Networks (DBN)</li></ol><hr><h2 id="机器学习损失函数"><a href="#机器学习损失函数" class="headerlink" title="机器学习损失函数"></a>机器学习损失函数</h2><ol><li>0-1损失函数<script type="math/tex; mode=display">L(y,f(x)) =\begin{cases}0, & \text{y = f(x)}  \\1, & \text{y $\neq$ f(x)}\end{cases}</script></li><li>绝对值损失函数<script type="math/tex; mode=display">L(y,f(x))=|y-f(x)|</script></li><li>平方损失函数<script type="math/tex; mode=display">L(y,f(x))=(y-f(x))^2</script></li><li>log对数损失函数<script type="math/tex; mode=display">L(y,f(x))=log(1+e^{-yf(x)})</script></li><li>指数损失函数<script type="math/tex; mode=display">L(y,f(x))=exp(-yf(x))</script></li><li>Hinge损失函数<script type="math/tex; mode=display">L(w,b)=max\{0,1-yf(x)\}</script></li></ol><hr><h2 id="机器学习优化方法"><a href="#机器学习优化方法" class="headerlink" title="机器学习优化方法"></a>机器学习优化方法</h2><p>梯度下降是最常用的优化方法之一，它使用梯度的反方向 $ \nabla_\theta J(\theta) $ 更新参数 $ \theta $，使得目标函数$J(\theta)$达到最小化的一种优化方法，这种方法我们叫做梯度更新. </p><ol><li>(全量)梯度下降<script type="math/tex; mode=display">\theta=\theta-\eta\nabla_\theta J(\theta)</script></li><li>随机梯度下降<script type="math/tex; mode=display">\theta=\theta-\eta\nabla_\theta J(\theta;x^{(i)},y^{(i)})</script></li><li>小批量梯度下降<script type="math/tex; mode=display">\theta=\theta-\eta\nabla_\theta J(\theta;x^{(i:i+n)},y^{(i:i+n)})</script></li><li>引入动量的梯度下降<script type="math/tex; mode=display">\begin{cases}v_t=\gamma v_{t-1}+\eta \nabla_\theta J(\theta)  \\\theta=\theta-v_t\end{cases}</script></li><li>自适应学习率的Adagrad算法<script type="math/tex; mode=display">\begin{cases}g_t= \nabla_\theta J(\theta)  \\\theta_{t+1}=\theta_{t,i}-\frac{\eta}{\sqrt{G_t+\varepsilon}} \cdot g_t\end{cases}</script></li><li><p>牛顿法</p><script type="math/tex; mode=display">\theta_{t+1}=\theta_t-H^{-1}\nabla_\theta J(\theta_t)</script><p> 其中:<br> $t$: 迭代的轮数</p><p> $\eta$: 学习率</p><p> $G_t$: 前t次迭代的梯度和</p><p> $\varepsilon:$很小的数,防止除0错误</p><p> $H$: 损失函数相当于$\theta$的Hession矩阵在$\theta_t$处的估计</p></li></ol><hr><h2 id="机器学习的评价指标"><a href="#机器学习的评价指标" class="headerlink" title="机器学习的评价指标"></a>机器学习的评价指标</h2><ol><li>MSE(Mean Squared Error)<script type="math/tex; mode=display">MSE(y,f(x))=\frac{1}{N}\sum_{i=1}^{N}(y-f(x))^2</script></li><li>MAE(Mean Absolute Error)<script type="math/tex; mode=display">MSE(y,f(x))=\frac{1}{N}\sum_{i=1}^{N}|y-f(x)|</script></li><li>RMSE(Root Mean Squard Error)<script type="math/tex; mode=display">RMSE(y,f(x))=\frac{1}{1+MSE(y,f(x))}</script></li><li>Top-k准确率<script type="math/tex; mode=display">Top_k(y,pre_y)=\begin{cases}1, {y \in pre_y}  \\0, {y \notin pre_y}\end{cases}</script></li><li>混淆矩阵</li></ol><div class="table-container"><table><thead><tr><th style="text-align:center">混淆矩阵</th><th style="text-align:center">Predicted as Positive</th><th style="text-align:center">Predicted as Negative</th></tr></thead><tbody><tr><td style="text-align:center">Labeled as Positive</td><td style="text-align:center">True Positive(TP)</td><td style="text-align:center">False Negative(FN)</td></tr><tr><td style="text-align:center">Labeled as Negative</td><td style="text-align:center">False Positive(FP)</td><td style="text-align:center">True Negative(TN)</td></tr></tbody></table></div><ul><li>真正例(True Positive, TP):真实类别为正例, 预测类别为正例</li><li>假负例(False Negative, FN): 真实类别为正例, 预测类别为负例</li><li>假正例(False Positive, FP): 真实类别为负例, 预测类别为正例 </li><li><p>真负例(True Negative, TN): 真实类别为负例, 预测类别为负例</p></li><li><p>真正率(True Positive Rate, TPR): 被预测为正的正样本数 / 正样本实际数</p><script type="math/tex; mode=display">TPR=\frac{TP}{TP+FN}</script></li><li><p>假负率(False Negative Rate, FNR): 被预测为负的正样本数/正样本实际数</p><script type="math/tex; mode=display">FNR=\frac{FN}{TP+FN}</script></li><li><p>假正率(False Positive Rate, FPR): 被预测为正的负样本数/负样本实际数，</p><script type="math/tex; mode=display">FPR=\frac{FP}{FP+TN}</script></li><li>真负率(True Negative Rate, TNR): 被预测为负的负样本数/负样本实际数，<script type="math/tex; mode=display">TNR=\frac{TN}{FP+TN}</script></li><li>准确率(Accuracy)<script type="math/tex; mode=display">ACC=\frac{TP+TN}{TP+FN+FP+TN}</script></li><li>精准率<script type="math/tex; mode=display">P=\frac{TP}{TP+FP}</script></li><li>召回率<script type="math/tex; mode=display">R=\frac{TP}{TP+FN}</script></li><li>F1-Score<script type="math/tex; mode=display">\frac{2}{F_1}=\frac{1}{P}+\frac{1}{R}</script></li><li><strong>ROC</strong></li></ul><p>ROC曲线的横轴为“假正例率”，纵轴为“真正例率”. 以FPR为横坐标，TPR为纵坐标，那么ROC曲线就是改变各种阈值后得到的所有坐标点 (FPR,TPR) 的连线，画出来如下。红线是随机乱猜情况下的ROC，曲线越<code>靠左上角</code>，分类器越佳. </p><ul><li><strong>AUC(Area Under Curve)</strong></li></ul><p>AUC就是ROC曲线下的面积. 真实情况下，由于数据是一个一个的，阈值被离散化，呈现的曲线便是锯齿状的，当然数据越多，阈值分的越细，”曲线”越光滑. </p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/09/Ky4FT1MVe3PUYai.jpg" alt="Ky4FT1MVe3PUYai"></p><p>用AUC<code>判断分类器（预测模型）优劣</code>的标准:</p><ul><li>AUC = 1 是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数预测的场合，不存在完美分类器.</li><li>0.5 &lt; AUC &lt; 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值.</li><li>AUC &lt; 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测.</li></ul><h2 id="机器学习模型选择"><a href="#机器学习模型选择" class="headerlink" title="机器学习模型选择"></a>机器学习模型选择</h2><ol><li>交叉验证</li></ol><p>所有数据分为三部分：训练集、交叉验证集和测试集。交叉验证集不仅在选择模型时有用，在超参数选择、正则项参数 [公式] 和评价模型中也很有用。</p><ol><li>k-折叠交叉验证</li></ol><ul><li>假设训练集为S ，将训练集等分为k份:$\{S_1, S_2, …, S_k\}$. </li><li>然后每次从集合中拿出k-1份进行训练</li><li>利用集合中剩下的那一份来进行测试并计算损失值</li><li>最后得到k次测试得到的损失值，并选择平均损失值最小的模型</li></ul><ol><li>Bias与Variance，欠拟合与过拟合</li></ol><p><strong>欠拟合</strong>一般表示模型对数据的表现能力不足，通常是模型的复杂度不够，并且Bias高，训练集的损失值高，测试集的损失值也高.</p><p><strong>过拟合</strong>一般表示模型对数据的表现能力过好，通常是模型的复杂度过高，并且Variance高，训练集的损失值低，测试集的损失值高.</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/09/dIiSlJVkjFL6EpB.jpg" alt="dIiSlJVkjFL6EpB"></p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/09/Dn5bB8wUVgmsIzC.jpg" alt="Dn5bB8wUVgmsIzC"></p><ol><li>解决方法</li></ol><ul><li>增加训练样本: 解决高Variance情况</li><li>减少特征维数: 解决高Variance情况</li><li>增加特征维数: 解决高Bias情况</li><li>增加模型复杂度: 解决高Bias情况</li><li>减小模型复杂度: 解决高Variance情况</li></ul><h2 id="机器学习参数调优"><a href="#机器学习参数调优" class="headerlink" title="机器学习参数调优"></a>机器学习参数调优</h2><ol><li>网格搜索</li></ol><p>一种调参手段；穷举搜索：在所有候选的参数选择中，通过循环遍历，尝试每一种可能性，表现最好的参数就是最终的结果</p><ol><li>随机搜索</li></ol><p>与网格搜索相比，随机搜索并未尝试所有参数值，而是从指定的分布中采样固定数量的参数设置。它的理论依据是，如果随即样本点集足够大，那么也可以找到全局的最大或最小值，或它们的近似值。通过对搜索范围的随机取样，随机搜索一般会比网格搜索要快一些。</p><ol><li>贝叶斯优化算法</li></ol><p>贝叶斯优化用于机器学习调参由J. Snoek(2012)提出，主要思想是，给定优化的目标函数(广义的函数，只需指定输入和输出即可，无需知道内部结构以及数学性质)，通过不断地添加样本点来更新目标函数的后验分布(高斯过程,直到后验分布基本贴合于真实分布。简单的说，就是考虑了上一次参数的信息，从而更好的调整当前的参数。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OJ 做题笔记</title>
      <link href="/2020/02/06/pat-advance/"/>
      <url>/2020/02/06/pat-advance/</url>
      
        <content type="html"><![CDATA[<h1 id="PAT-甲级-做题笔记"><a href="#PAT-甲级-做题笔记" class="headerlink" title="PAT-甲级 做题笔记"></a>PAT-甲级 做题笔记</h1><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><h3 id="基础算法"><a href="#基础算法" class="headerlink" title="基础算法"></a>基础算法</h3><p>0000 <a href="#0000">做题 Tips 基本经验</a><br>1010 <a href="#1010">Radix (进制转换/二分法)</a><br>1015 <a href="#1015">Reversible Primes (进制转换+判断素数: 可取用函数)</a><br>NK <a href="#NK-jzzh">10进制 VS 2进制 (任意长度字符串进制转换: 可取用函数)</a><br>1016 <a href="#1016">Phone Bills (日期类计算)</a><br>NK <a href="#NK-dzs">大整数乘除法 手写模拟</a></p><h3 id="查找-排序-模拟"><a href="#查找-排序-模拟" class="headerlink" title="查找/排序/模拟"></a>查找/排序/模拟</h3><p>1012 <a href="#1012">The Best Rank (应用问题，数据结构设计，多维度排序)</a><br>1014 <a href="#1014">Waiting in Line (队列应用，排队问题)</a><br>1044 <a href="#1044">Shopping in Mars (特定值子序列和, 二分查找)</a><br>1085 <a href="#1085">Perfect Sequence (二分查找, upper-bound, 双指针)</a><br>1071 <a href="#1071">Speech Patterns (词频统计)</a><br>1067 <a href="#1067">Sort with Swap(0, i) (贪心, 元素归位)</a><br>NK <a href="#NK-dcth">单词替换 (手写 split 函数)</a><br>NK <a href="#NK-sxzs">扇形着色问题</a></p><h3 id="动态规划算法-DP"><a href="#动态规划算法-DP" class="headerlink" title="动态规划算法 DP"></a>动态规划算法 DP</h3><p>NK <a href="#dp-set">常见动态规划算法合集</a><br>1007 <a href="#1007">Maximum Subsequence Sum(最大子序列和)</a><br>1040 <a href="#1040">Longest Symmetric String (求最长对称子串, DP)</a><br>1045 <a href="#1045">Favorite Color Stripe (最长不下降子序列, DP)</a><br>1068 <a href="#1068">Find More Coins (0-1 背包问题, DP)</a><br>HustOJ 1130 <a href="#H1130">分成两个尽可能接近的子数组 (背包问题, DP)</a><br>TK 21802 <a href="#T21802">采药 (最基础的容量-价值最大化, 背包问题, DP)</a><br>TK 1131 <a href="#T1131">分梨 (m 个梨子装进 n 个盘子, DP)</a><br>TK 1132 <a href="#T1132">最长公共子序列 LCS (入门经典 DP)</a><br>TK 4861 <a href="#T4861">最长不下降子序列 (入门经典 DP)</a><br>TK 5578 <a href="#T5578">最大约数和 (DP)</a><br>TK 5579 <a href="#T5579">环形石子合并问题 (区间DP)</a></p><h3 id="树算法"><a href="#树算法" class="headerlink" title="树算法"></a>树算法</h3><p>1004 <a href="#1004">Counting Leaves (计算叶节点数，DFS/BFS 树算法)</a><br>1020 <a href="#1020">Tree Traversals (已知后序和中序，转前序/层序)</a><br>1021 <a href="#1021">Deepest Root (求树中最长的路径，DFS，连通分量)</a><br>1100 <a href="#1100">Complete Binary Tree (判断完全二叉树 层序遍历)</a></p><h3 id="图算法"><a href="#图算法" class="headerlink" title="图算法"></a>图算法</h3><p>1003 <a href="#1003">Emergency (Dijkstra 算法)</a><br>1013 <a href="#1013">Battle Over Cities (图的遍历，统计强连通分量的个数，DFS)</a><br>1018 <a href="#1018">Public Bike Management (图最短路径 Dijkstra + DFS)</a><br>NK <a href="#NK-ctgc">畅通工程 (并查集, 最小生成树 Kruscal)</a><br>NK <a href="#NK-graph">常见图算法合集</a></p><hr><h2 id="0000-做题-Tips-基本经验"><a href="#0000-做题-Tips-基本经验" class="headerlink" title="0000 做题 Tips 基本经验"></a>0000 做题 Tips 基本经验<span id="0000"></span></h2><ol><li>最后千万别栽在头文件上，比如 reverse() 属于 &lt; algorithm&gt;，比如 memset 属于 string.h<br>一定要多检查 corner case, 比如 n=1, n=0</li><li>设置输出形式<pre><code class="lang-c++">cout &lt;&lt; setw(4) &lt;&lt; setfill(&#39;0&#39;) &lt;&lt; v[i];</code></pre></li><li><p>int 最大范围 $-2^{31}$=-2147483648 到 $2^{31}$-1=2147483647  (10 位数)</p></li><li><p>INT_MAX 和 INT_MIN 都在<code>#include &lt;limits.h&gt;</code>中定义的<br>其中 INT_MAX 就等于<code>0x7fffffff</code>, 也即<code>2147483647</code></p></li><li><p>输入一两个数, 用 cin 无伤大雅, 遇到下面这种</p><pre><code class="lang-c++">for (int j = 0; j &lt; m; ++j) {     cin &gt;&gt; temp; }</code></pre><p>由于 m 可以达到 $10^5$, cin 就会显得很耗时, 要用 scanf</p></li><li><p>结构体不能直接整个赋值</p></li></ol><pre><code class="lang-cpp">station st = {0, 0};scanf(&quot;%lf %lf&quot;, &amp;st.price, &amp;st.dis);v.push_back(st);</code></pre><ol><li>scanf 读取字符串注意</li></ol><p>一定要预先分配好空间，即使是数组也要. 而且 string a 一定要 <code>&amp;a[0]</code></p><pre><code class="lang-cpp">int main(){    string a;    a.resize(100); //需要预先分配空间    scanf(&quot;%s&quot;, &amp;a[0]);    puts(a.c_str());    return 0;}char str[20];scanf(&quot;%s&quot;,str);char* str=(char*)malloc(15*sizeof(char));scanf(&quot;%s&quot;,str);</code></pre><ol><li><p>map 初始化方法</p><pre><code class="lang-cpp">map&lt;string, int&gt; getMonth{ {&quot;January&quot;,1 }, {&quot;February&quot;,2}, {&quot;March&quot;,3}}</code></pre></li><li><p>字符串格式抓换</p><pre><code class="lang-cpp">scanf(&quot;%s&quot;, a); sscanf(a, &quot;%lf&quot;, &amp;temp); sprintf(b, &quot;%.2f&quot;,temp);</code></pre></li></ol><h3 id="附-甲级题目分类"><a href="#附-甲级题目分类" class="headerlink" title="附:甲级题目分类"></a>附:甲级题目分类</h3><h2 id=""><a href="#" class="headerlink" title=""></a><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/06/Y49WNI6DpA3hSKX.png" alt="甲级题目分类"></h2><p><span id="1003"></span></p><h2 id="1003-Emergency-Dijkstra-算法"><a href="#1003-Emergency-Dijkstra-算法" class="headerlink" title="1003 Emergency (Dijkstra 算法)"></a>1003 Emergency (Dijkstra 算法)</h2><h3 id="1-题目大意"><a href="#1-题目大意" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>n个城市m条路，每个城市有救援小组，所有的边的边权已知. 给定起点和终点，求从起点到终点的最短路径条数以及最短路径上的救援小组数目之和. 如果有多条就输出点权（城市救援小组数目）最大的那个</p><h3 id="2-分析"><a href="#2-分析" class="headerlink" title="2. 分析"></a>2. 分析</h3><p>用一遍Dijkstra算法～救援小组个数相当于点权，用Dijkstra求边权最小的最短路径的条数，以及这些最短路径中点权最大的值～dis[i]表示从出发点到i结点最短路径的路径长度，num[i]表示从出发点到i结点最短路径的条数，w[i]表示从出发点到i点救援队的数目之和～当判定dis[u] + e[u][v] &lt; dis[v]的时候，不仅仅要更新dis[v]，还要更新num[v] = num[u], w[v] = weight[v] + w[u]; 如果dis[u] + e[u][v] == dis[v]，还要更新num[v] += num[u]，而且判断一下是否权重w[v]更小，如果更小了就更新w[v] = weight[v] + w[u]</p><h3 id="3-个人代码"><a href="#3-个人代码" class="headerlink" title="3. 个人代码"></a>3. 个人代码</h3><p><a href="https://pintia.cn/problem-sets/994805342720868352/problems/994805523835109376" target="_blank" rel="noopener">PTA-1003</a></p><pre><code class="lang-c++">#include &lt;iostream&gt;#include &lt;algorithm&gt;using namespace std;//Dijkstra 算法：单源最短路径int n, m, c1, c2;int e[510][510], weight[510], dis[510], num[510], w[510];//边的邻接矩阵，每个点的权值，从出发点到i的距离，最短距离边数，最大权重和const int Inf = 99999999;bool visit[510];int main(){    cin &gt;&gt; n &gt;&gt; m &gt;&gt; c1 &gt;&gt; c2;    for (int i = 0; i &lt; n; ++i) {        cin &gt;&gt; weight[i];    }    fill(e[0], e[0]+510*510, Inf);  //整个邻接矩阵填正无穷，默认不可达    fill(dis, dis+510, Inf);    int a,b,c;    for (int j = 0; j &lt; m; ++j) {        cin &gt;&gt; a &gt;&gt; b &gt;&gt; c;        e[a][b] = e[b][a] = c;  //一定要对称    }    dis[c1] = 0;    num[c1] = 1;    w[c1] = weight[c1];  //初始化    for (int i = 0; i &lt; n; ++i) {    //每个点都要作为起始点 开始一遍        int u = -1, min_d = Inf;        //每次循环重置        //本次循环表示从 i 出发, i -&gt; j        for (int j = 0; j &lt; n; ++j) {            if(!visit[j] &amp;&amp; dis[j] &lt; min_d){   //找从本点出发的最小边                u = j;                min_d = dis[j];            }        }        if(u == -1) break;        visit[u] = true;        //得到一条 i-&gt;u, 即 dis[], 用来更新最短路径        //之前没更新的、在后续得到更新，达到最优        for (int v = 0; v &lt; n; ++v) {            if(!visit[v] &amp;&amp; e[u][v] != Inf){                if(dis[u] + e[u][v] &lt; dis[v]){                    dis[v] = dis[u] + e[u][v];                    w[v] = w[u] + weight[v];                    num[v] = num[u];                } else if(dis[u] + e[u][v] == dis[v]){                    num[v] += num[u];                    if(w[v] &lt; w[u]+weight[v])                        w[v] = w[u]+weight[v];   //携带尽可能多的人                }            }        }    }    cout &lt;&lt; num[c2] &lt;&lt; &quot; &quot; &lt;&lt; w[c2];   //从出发点到c2    return 0;}</code></pre><h3 id="4-学习要点"><a href="#4-学习要点" class="headerlink" title="4. 学习要点"></a>4. 学习要点</h3><blockquote><p>1.真正理解单源最短路径，每一次选择的 dis 都是相对于源点的最短路径<br>2.大数组要设为全局变量<br>3.外层循环 i：n 次循环，每次访问一个新点，保证 n 个点全访问到；内层循环 j：继续寻找还未访问的点，找 dis 最小的访问；内层循环 k：更新所有能够更新的 dis</p></blockquote><hr><p><span id="1004"></span></p><h2 id="1004-Counting-Leaves-dfs-bfs-树算法"><a href="#1004-Counting-Leaves-dfs-bfs-树算法" class="headerlink" title="1004 Counting Leaves (dfs/bfs 树算法)"></a>1004 Counting Leaves (dfs/bfs 树算法)</h2><h3 id="1-题目大意-1"><a href="#1-题目大意-1" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>给出一棵树，问每一层各有多少个叶子结点</p><h3 id="2-坑"><a href="#2-坑" class="headerlink" title="2. 坑"></a>2. 坑</h3><pre><code>list[tmp].lev = list[fa].lev + 1;  //出问题,不一定按照顺序输入</code></pre><p>有可能先 03 后 01，fa 的 lev 还没确定，不能给孩子节点 +1</p><pre><code>3 203 1 0201 1 03</code></pre><h3 id="3-个人代码-1"><a href="#3-个人代码-1" class="headerlink" title="3. 个人代码"></a>3. 个人代码</h3><p><a href="https://pintia.cn/problem-sets/994805342720868352/problems/994805521431773184" target="_blank" rel="noopener">PTA-1004</a></p><h3 id="4-正确方法"><a href="#4-正确方法" class="headerlink" title="4. 正确方法"></a>4. 正确方法</h3><pre><code class="lang-cpp">void dfs(int fa){    for (auto &amp;ch : list[fa].v) {        list[ch].lev = list[fa].lev + 1;        dfs(ch);    }}dfs(1);   //通过 DFS 一层层给叶子节点 lev+1</code></pre><hr><p><span id="1007"></span></p><h2 id="1007-Maximum-Subsequence-Sum-最大子序列和"><a href="#1007-Maximum-Subsequence-Sum-最大子序列和" class="headerlink" title="1007 Maximum Subsequence Sum (最大子序列和)"></a>1007 Maximum Subsequence Sum (最大子序列和)</h2><h3 id="1-题目大意-2"><a href="#1-题目大意-2" class="headerlink" title="1.题目大意"></a>1.题目大意</h3><p>求最大连续子序列和，输出最大的和以及这个子序列的开始值和结束值. 如果所有数都小于0，那么认为最大的和为0，并且输出首尾元素</p><h3 id="2-分析-1"><a href="#2-分析-1" class="headerlink" title="2.分析"></a>2.分析</h3><p>本质上是<strong>动态规划</strong>的思想，数组为<code>vec[]</code>，设<code>dp[i]</code> 是以<code>vec[i]</code>结尾的子数组的最大和，对于元素<code>vec[i+1]</code>, 它有两种选择：<code>vec[i+1]</code>接着前面的子数组构成最大和; <code>vec[i+1]</code>自己单独构成子数组. 则<code>dp[i+1] = max{dp[i]+vec[i+1],  vec[i+1]}</code></p><p>简化则用一个 temp_sum 和一个 temp_first 解决，真正最大和为 sum，起始点为 first 和 last，建立局部和全局的关系. 如果 <code>temp_sum &lt; 0</code>, 说明目前这一段对后续的序列和已经没有加成作用，可以舍弃另立门户，令<code>temp_sum = 0</code></p><h3 id="3-个人代码-2"><a href="#3-个人代码-2" class="headerlink" title="3. 个人代码"></a>3. 个人代码</h3><pre><code class="lang-cpp">//初始值设置容易坑：sum要为-1 才能开始int sum=-1, temp_sum=0, first=0, last=k-1, temp_first=0;for (int j = 0; j &lt; k; ++j) {    temp_sum += a[j];    if(temp_sum &lt; 0){        temp_sum = 0;        temp_first = j+1;   //本段已经可以舍弃，开始新一段    } else if(temp_sum &gt; sum){        sum = temp_sum;        first = temp_first;        last = j;    }}if(sum &lt; 0) sum = 0;  //如果全负则为 0，根据题目要求不要漏情况</code></pre><h3 id="4-类似题"><a href="#4-类似题" class="headerlink" title="4. 类似题"></a>4. 类似题</h3><h5 id="1-最长无重复子串"><a href="#1-最长无重复子串" class="headerlink" title="1) 最长无重复子串"></a>1) <a href="https://www.cnblogs.com/grandyang/p/4480780.html" target="_blank" rel="noopener">最长无重复子串</a></h5><h5 id="2-满足-gt-k-的最短连续子序列"><a href="#2-满足-gt-k-的最短连续子序列" class="headerlink" title="2) 满足 &gt;=k 的最短连续子序列"></a>2) 满足 &gt;=k 的最短连续子序列</h5><p>核心思想: 要求连续, 可以用<code>双指针法</code>, 记录 start 和 end, end 作为探路, start 作为后备, 满足之后 start 就可以前移. 整个过程中通过 min 沉淀出最短长度.</p><pre><code class="lang-cpp">int start = 0, end = 0, ans = n, sum = 0;while (end &lt; n){    if(sum &lt; k)        sum += a[end];    while (sum &gt;= k){   //一直移到小于 k 为止, 满足 sum &gt;= k 的就有计数资格        ans = min(ans, end-start+1);        sum -= a[start++];   //start 后移    }    end++;}cout &lt;&lt; ans;</code></pre><hr><p><span id="1010"></span></p><h2 id="1010-Radix-进制转换-二分法）"><a href="#1010-Radix-进制转换-二分法）" class="headerlink" title="1010 Radix (进制转换/二分法）"></a>1010 Radix (进制转换/二分法）</h2><h3 id="1-题目大意-3"><a href="#1-题目大意-3" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>给定两个相等数，已知一个数的进制，求另外一个数的进制（radix-&gt;基数）</p><h3 id="2-分析-2"><a href="#2-分析-2" class="headerlink" title="2. 分析"></a>2. 分析</h3><p>通用函数<code>cal(string str, int radix)</code>把任意进制的数转换为 10 进制数<br>在使用搜索遍历，找到另一个数对应的进制</p><h3 id="3-个人代码-3"><a href="#3-个人代码-3" class="headerlink" title="3.个人代码"></a>3.个人代码</h3><p><a href="https://pintia.cn/problem-sets/994805342720868352/problems/994805507225665536" target="_blank" rel="noopener">PTA-1010</a></p><p>转换函数</p><pre><code class="lang-cpp">//任意进制转换为 10 进制long long cal(string str, long long radix){    long long target=0, bak = 1;    reverse(str.begin(), str.end());    for (char c : str) {        if(isdigit(c))            target += (c - &#39;0&#39;) * bak;        else            target += (c - &#39;a&#39; + 10) * bak;        bak *= radix;  //也可以使用&lt;cmath&gt;的 pow()函数    }    return target;}</code></pre><p>搜索函数</p><pre><code class="lang-cpp">long long find_radix(string str, long long tar){    char c = *max_element(str.begin(), str.end());  //最大字母    //进制至少比最大字母要大    long long low = isdigit(c) ? c-&#39;0&#39; : c-&#39;a&#39;+10;    low += 1;  //必须要加一,至少多1    long long high = max(low, tar);  //进制最大不会大于目标 tar    while (low &lt;= high){        long long mid = (low+high)/2;        long long tmp = cal(str, mid);        if(tmp &lt; 0 || tmp &gt; tar) high = mid - 1;  //小于 0 也是进制太大        else if(tmp == tar) return mid;        else low = mid + 1;    }    return -1;}</code></pre><h3 id="4-学习要点-1"><a href="#4-学习要点-1" class="headerlink" title="4. 学习要点"></a>4. 学习要点</h3><blockquote><ol><li>进制转换这种大数字的乘法很容易溢出，必须要用 <code>long long</code></li><li>暴力搜索（从 2 到 ∞ 容易超时）最好用二分法，并且限定范围：最小是 <code>low</code> 至少是最大的那个字母+1，比如 fff 最少是 15+1=16 进制，最大是<code>high</code>不超过目标 <code>tar</code></li><li>可以使用反向迭代器 <code>it = n.rbegin(); it != n.rend()</code>代替<code>reverse()</code></li></ol></blockquote><hr><h2 id="1012-The-Best-Rank-数据结构设计-多维度排序"><a href="#1012-The-Best-Rank-数据结构设计-多维度排序" class="headerlink" title="1012 The Best Rank (数据结构设计/多维度排序)"></a>1012 The Best Rank (数据结构设计/多维度排序)<span id="1012"></span></h2><h3 id="1-题目大意-4"><a href="#1-题目大意-4" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>找出每个人自己最优势的科目，也就是单独排名最好的科目，优先级 A &gt; C &gt; M &gt; E</p><ul><li>Sample Input:<pre><code>5 6310101 98 85 88310102 70 95 88310103 82 87 94310104 91 91 91310105 85 90 90310101310102310103310104310105999999</code></pre></li><li>Sample Output:<pre><code>1 C1 M1 E1 A3 AN/A</code></pre></li></ul><h3 id="2-个人代码"><a href="#2-个人代码" class="headerlink" title="2. 个人代码"></a>2. 个人代码</h3><p><a href="https://pintia.cn/problem-sets/994805342720868352/problems/994805502658068480" target="_blank" rel="noopener">PTA-1012</a></p><h3 id="3-学习要点"><a href="#3-学习要点" class="headerlink" title="3. 学习要点"></a>3. 学习要点</h3><p>数据结构设计</p><pre><code class="lang-cpp">struct node{    //一定要用数组形式，避免过多变量 int c, m, e, c_r, m_r, e_r, a_r;    int id, best;    int score[4], rank[4];}stu[2001];  //输入学生序列，非学号排序int exist[1000000], flag=-1; //exist 至少 &gt;0, 顺便建立 id-输入序号 映射//使用 flag 配置作用，避免写过多重复的 cmp 函数bool cmp(node &amp;a, node &amp;b){ return a.score[flag] &gt; b.score[flag];}</code></pre><p>输入后，利用<code>exist[stu[k].id] = k + 1; k++</code> 依次建立映射<br>输出时，利用<code>cin &gt;&gt; tmp; ex = exist[tmp]</code>和<code>stu[ex-1]</code>实现映射，无需 find<br>处理时，使用<code>sort(stu, stu+n, cmp)</code> 循环 <code>flag++</code>，实现分学科多次排序，确定 <code>rank[]</code></p><hr><p><span id="1013"></span></p><h2 id="1013-Battle-Over-Cities（图的遍历，统计强连通分量的个数，dfs）"><a href="#1013-Battle-Over-Cities（图的遍历，统计强连通分量的个数，dfs）" class="headerlink" title="1013 Battle Over Cities（图的遍历，统计强连通分量的个数，dfs）"></a>1013 Battle Over Cities（图的遍历，统计强连通分量的个数，dfs）</h2><h3 id="1-题目大意-5"><a href="#1-题目大意-5" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>给出n个城市之间有相互连接的m条道路，当删除一个城市和其连接的道路的时候，问其他几个剩余的城市至少要添加多少个路线，才能让它们重新变为连通图</p><h3 id="2-分析-3"><a href="#2-分析-3" class="headerlink" title="2. 分析"></a>2. 分析</h3><p>n 连通分量，最少需要 n-1 条边相连，所以本题实质就是求「去除某点及其相连的边」后，剩余的「连通分量个数」是一个图算法，需要用图的遍历来解决 (DFS)</p><blockquote><p>求连通分量依据：一次 <code>dfs()</code>走到底，可以访问完一个连通分量</p></blockquote><p>数据结构：</p><blockquote><p>涉及图的遍历，要考虑 <code>visit[]</code>数组<br>图的路径存储，使用二维矩阵</p></blockquote><h3 id="3-个人代码-4"><a href="#3-个人代码-4" class="headerlink" title="3. 个人代码"></a>3. 个人代码</h3><p><a href="https://pintia.cn/problem-sets/994805342720868352/problems/994805500414115840" target="_blank" rel="noopener">PTA-1013</a></p><pre><code class="lang-cpp">for (int i = 0; i &lt; k; ++i) {        int cur, cnt=0;        cin &gt;&gt; cur;        fill(visited, visited+n, false);        visited[cur] = true;  //相当于把本点去除        for (int j = 1; j &lt;= n; ++j) {  //依次计算所有连通分量，而非从本点出发            if(!visited[j]){                dfs(j);                cnt++;            }        }        cout &lt;&lt; cnt-1 &lt;&lt; endl;    }</code></pre><pre><code class="lang-cpp">void dfs(int st){   //只用来 visit，不负责计数    visited[st] = true;    for (int i = 0; i &lt;= n; ++i)        if(!visited[i] &amp;&amp; e[st][i]){   //未访问且有路            dfs(i);        }}</code></pre><h3 id="4-坑点"><a href="#4-坑点" class="headerlink" title="4.坑点"></a>4.坑点</h3><p>有些题目, 没有给定 n 的范围, 开二维数组可能会内存超限. 所以可以使用<code>邻接表</code>, vector 的数组.</p><pre><code class="lang-cpp">vector&lt;int&gt; Adj[maxn];//输入时if(m!=n){    Adj[n].push_back(m);    Adj[m].push_back(n);}else{    Adj[n].push_back(m);}//遍历时void DFS(int n){    flag[n]=true;    for(int i=0; i&lt;Adj[n].size(); ++i) //只遍历自己的邻接点    {        int v=Adj[n][i];        if(flag[v]==false)        {            DFS(v);        }    }}</code></pre><hr><p><span id="1014"></span></p><h2 id="1014-Reversible-Primes（队列应用-排队问题"><a href="#1014-Reversible-Primes（队列应用-排队问题" class="headerlink" title="1014 Reversible Primes（队列应用 排队问题"></a>1014 Reversible Primes（队列应用 排队问题</h2><h3 id="1-题目大意-6"><a href="#1-题目大意-6" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>n个窗口，每个窗口可以排队m人. 有k位用户需要服务，给出了每位用户需要的minute数，所有客户在8点开始服务，如果有窗口还没排满就入队，否则就在黄线外等候. 如果有某一列有一个用户走了服务完毕了，黄线外的人就进来一个. 如果同时就选窗口数小的. 求q个人的服务结束时间.<br>如果一个客户在17:00以及以后还没有开始服务（此处不是结束服务是开始17:00）就不再服务输出sorry；如果这个服务已经开始了，无论时间多长都要等他服务完毕. </p><h3 id="2-学习要点"><a href="#2-学习要点" class="headerlink" title="2. 学习要点"></a>2. 学习要点</h3><p>面对此类题目，总想按照 timeline 每分每秒去决策，其实队列的问题用队列解决. </p><p>以每个窗口为单位设计结构体，<code>poptime</code> 用于刚入黄线的人决策，<code>endtime</code> 用于已经在队内的人是否会 sorry 决策，<code>q</code> 表示本队列内排队情况. </p><p>给定客户序列，则以每一个客户为循环单位<code>(index++)</code>，队列有一个进就会有一个出，每次变动一个单位，更新各个队列的<code>poptime endtime q</code>等变量. </p><pre><code class="lang-cpp">struct node {    int poptime, endtime;  //队首的人出队（结束）的时间, 队尾的人结束的时间    queue&lt;int&gt; q;};int index = 1;    //第 1 步：n*m 个客户先抢占黄线内, 直接瞬间涌入，不存在选择    for (int i = 1; i &lt;= cap; ++i) {    //看做行和列，先抢第一行        for (int j = 1; j &lt;= n; ++j) {            if(index &lt;= k){                win[j].q.push(time[index]);                if(win[j].endtime &gt;= 540)                    sorry[index] = true;                win[j].endtime += time[index];                if(i == 1)  //对于第一行，出队时间即本队结束时间，做一个初始化                    win[j].poptime = win[j].endtime;                res[index] = win[j].endtime;  //刚进来的，自己是队尾，队伍结束就是自己结束                index++;            }        }    }    //第 2 步：后续的客户    while (index &lt;= k){   //while一次循环表示一个客户的选择        int temp_min = win[1].poptime, temp_win = 1;  //初始化        for (int i = 2; i &lt;= n; ++i) {   //找最早出队的            if(win[i].poptime &lt; temp_min){                temp_win = i;                temp_min = win[i].poptime;            }        }        win[temp_win].q.pop();        win[temp_win].q.push(time[index]);        win[temp_win].poptime += win[temp_win].q.front(); //更新：最前面那个人完成的时间就是出队时间        if(win[temp_win].endtime &gt;= 540)  //endtime 还未更新，是看的前一个人的完成时间，是否超时            sorry[index] = true;        win[temp_win].endtime += time[index];  //更新：刚进来的这个人的完成时间就是本队的完成时间        res[index] = win[temp_win].endtime;        index++;    }</code></pre><hr><p><span id="1015"></span></p><h2 id="1015-判断素数-进制转换"><a href="#1015-判断素数-进制转换" class="headerlink" title="1015 判断素数 进制转换"></a>1015 判断素数 进制转换</h2><h3 id="1-可以直接取用的函数"><a href="#1-可以直接取用的函数" class="headerlink" title="1. 可以直接取用的函数"></a>1. 可以直接取用的函数</h3><pre><code class="lang-cpp">//计算任意进制的方法, 10进制-&gt;任意进制void cal(){    int a, b;    scanf(&quot;%d %d&quot;, &amp;a, &amp;b);    int arr[40], index = 0;    while(a != 0) {        arr[index++] = a % b;        a = a / b;    }}//判断素数bool isPrime(int x){    if(x == 2) return true;    if(x &lt;= 1 ) return false;    int loop = (int)sqrt(x * 1.0);    for (int i = 2; i &lt;= loop; ++i) {        if(x % i == 0) return false;    }    return true;}</code></pre><ol><li>任意进制字符串之间的转换<br><a href="#NK-jzzh">NK-进制转换</a></li></ol><hr><p><span id="1016"></span></p><h2 id="1016-Phone-Bills（日期类计算"><a href="#1016-Phone-Bills（日期类计算" class="headerlink" title="1016 Phone Bills（日期类计算)"></a>1016 Phone Bills（日期类计算)</h2><h3 id="1-题目大意-7"><a href="#1-题目大意-7" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>给定固定格式的日期<code>mm:dd:hh:mm</code>计算各个时间的差值，以及每个时间段有不同的费率，计算总的账单费用. </p><h3 id="2-学习要点-1"><a href="#2-学习要点-1" class="headerlink" title="2. 学习要点"></a>2. 学习要点</h3><p>1.数据结构设计<br>因为输入是给定一个个 call 序列，所以便于存储，也以每个 call 为单位，统一放在一个数组里，按照名字、时间排序，计算的时候，只要考虑 i 和 i-1 前后两个元素是否有同一个 name 以及 status 一个为 0 一个为 1 即可. </p><pre><code class="lang-cpp">struct node {    string name;    int status, month, time, day, hour, minute;};bool cmp(node a, node b) {    return a.name != b.name ? a.name &lt; b.name : a.time &lt; b.time;}</code></pre><p>2.日期差值计算<br>直接”01:05:02:24“-”01:04:23:59“ 难以计算，每个小时还有不同的费率，所以 <strong>统一从本月 0 天 0 点开始计算</strong>，以<code>dd:hh:mm</code>格式再相减即可，隔天的情况则加上这一整天</p><pre><code class="lang-cpp">double billFromZero(node call, int *rate) {    double total;    total = rate[call.hour];  //本小时的费用    total += call.minute + rate[24] * 60 * call.day;  //从本月第 0 天到今天    for (int i = 0; i &lt; call.hour; i++)        total += rate[i] * 60;  //本天内 0 点到现在累加    return total / 100.0;}</code></pre><hr><h2 id="1018-Public-Bike-Management（图最短路径-Dijkstra-DFS）"><a href="#1018-Public-Bike-Management（图最短路径-Dijkstra-DFS）" class="headerlink" title="1018 Public Bike Management（图最短路径 Dijkstra + DFS）"></a>1018 Public Bike Management（图最短路径 Dijkstra + DFS）<span id="1018"></span></h2><p>图例经过 Dijkstra 之后的结果</p><pre><code class="lang-cpp">dis[1] = 1      pre[1] = {0}dis[2] = 1      pre[2] = {0}dis[3] = 2      pre[3] = {1, 2}</code></pre><hr><p><span id="1020"></span></p><h2 id="1020-Tree-Traversals-已知后序和中序，转前序-层序"><a href="#1020-Tree-Traversals-已知后序和中序，转前序-层序" class="headerlink" title="1020 Tree Traversals (已知后序和中序，转前序/层序)"></a>1020 Tree Traversals (已知后序和中序，转前序/层序)</h2><h3 id="1-输入格式"><a href="#1-输入格式" class="headerlink" title="1. 输入格式"></a>1. 输入格式</h3><pre><code>72 3 1 5 7 6 41 2 3 4 5 6 7</code></pre><h3 id="2-分析-4"><a href="#2-分析-4" class="headerlink" title="2. 分析"></a>2. 分析</h3><p>和手动模拟差不多，找到根节点 (pre 的第一个，post 的最后一个) -&gt; 找到左右两段的端点，分别作为左右子树 -&gt; 递归重复. 关键在于递归的写法，端点的寻找. </p><h3 id="3-个人代码-5"><a href="#3-个人代码-5" class="headerlink" title="3. 个人代码"></a>3. 个人代码</h3><p><code>level</code> 的作用即按照 <code>index</code> 存储，利用 2n+1 2n+2 的公式可以定位左右孩子，而索引的顺序恰好是层序遍历的顺序（具体第几层不知道）<br>转为前序遍历，只需在 pre 递归前打印 root 即可</p><pre><code class="lang-cpp">vector&lt;int&gt; in, post, level(100000, -1);void pre(int root, int start, int end, int index){      //start end 都是相对于中序序列, 但是 root 是相对于 post 序列的！    if(start &gt; end) return;    level[index] = post[root];    int i = start;    while (i &lt; end &amp;&amp; in[i] != post[root]) i++;  //找到中序 in 序列中的根节点    int left_size = i-start, right_size = end-i;    pre(root-right_size-1, start, i-1, index*2+1);  //此 root 即为左孩子节点，可以确定为 index*2+1    pre(root-1, i+1, end, index*2+2);    //找到根节点 i，左子树 start ~ i-1, 右子树 i+1 ~ end}pre(n-1, 0, n-1, 0);</code></pre><h3 id="4-类题"><a href="#4-类题" class="headerlink" title="4. 类题"></a>4. 类题</h3><p><a href="https://pintia.cn/problem-sets/994805342720868352/problems/994805380754817024" target="_blank" rel="noopener">PTA-1086</a>（已知前序和中序遍历，求后序遍历）<br>给出用栈进行中序遍历的过程，求后序遍历. 有一个隐含条件，push 的顺序即为前序遍历，相对于已知 pre 和 in，求 post，同样找 root, start, end 即可. </p><p><a href="https://pintia.cn/problem-sets/994805342720868352/problems/994805345078067200" target="_blank" rel="noopener">PTA-1138</a> (已知中序和前序遍历, 求后序遍历的第一个数字)<br>思路一样, 后序遍历是左右根, 因此有左边就只 post 递归左边, 没有左边才递归右边.</p><pre><code class="lang-C++">int pre[50001], in[50001], n, fir;void post(int root, int start, int end){    if(start == end){        fir = start;        return;   //只有一个节点    }    int i = start;    while(i &lt;= end &amp;&amp; pre[root] != in[i]) i++;  //找到 in 序列中对应的 i    if(i &gt; start)        post(root+1, start, i-1);   //左边有就往左边遍历    else        post(root+1, i+1, end);}</code></pre><hr><p><span id="1021"></span></p><h2 id="1021-Deepest-Root-求树中最长的路径，DFS，连通分量"><a href="#1021-Deepest-Root-求树中最长的路径，DFS，连通分量" class="headerlink" title="1021 Deepest Root (求树中最长的路径，DFS，连通分量)"></a>1021 Deepest Root (求树中最长的路径，DFS，连通分量)</h2><h3 id="1-题目大意-8"><a href="#1-题目大意-8" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>给出n个结点（1~n）之间的n条边，问是否能构成一棵树，如果不能构成则输出它有的连通分量个数；如果能构成一棵树，输出能构成最深的树的高度时，树的根结点. 如果有多个，按照从小到大输出. </p><h3 id="2-分析-5"><a href="#2-分析-5" class="headerlink" title="2. 分析"></a>2. 分析</h3><p>连通分量的个数：基本套路，一次 DFS 走完一个连通分量，统计循环中 DFS 开启的次数即可<br>本题关键在于，两次 DFS 即可求出所有最远端点（即所有最长路径中的所有端点）如果是一棵树的话，线段无非是两个端点，第一次 DFS 能找到一端，在这些最远端点中（可能有多个同样最远的，可能只有一个最远的）随便选一个作为起点，再来一次 DFS，即可找到另一端的最远端点. </p><h3 id="3-个人代码-6"><a href="#3-个人代码-6" class="headerlink" title="3. 个人代码"></a>3. 个人代码</h3><p><a href="https://pintia.cn/problem-sets/994805342720868352/problems/994805482919673856" target="_blank" rel="noopener">PTA-1003</a></p><pre><code class="lang-cpp">void dfs(int node, int height) {    if(height &gt; maxheight) {        temp.clear();        temp.push_back(node);        maxheight = height;    } else if(height == maxheight){        temp.push_back(node);    }    visit[node] = true;    for(int i = 0; i &lt; v[node].size(); i++) {        if(visit[v[node][i]] == false)            dfs(v[node][i], height + 1);    }</code></pre><h3 id="4-做题-Tips"><a href="#4-做题-Tips" class="headerlink" title="4. 做题 Tips"></a>4. 做题 Tips</h3><ol><li>本题不用考虑权重，所以无需开一个二维数组用邻接矩阵存储<br><code>e[10010][10010]; e[a]=e[b]=weight;</code><br>只需使用邻接表，把和自己直接相连的点存起来即可<br><code>v[node][i]; resize(n+1);</code><br><br></li><li>DFS 基本套路<pre><code class="lang-cpp"> dfs(int node, ...){   //遍历过程中顺带做其它操作     visit[node] = true;     for 与本点直接相连的 i:         if(!visit[i])             dfs(i, ...) }</code></pre></li></ol><hr><p><span id="1102"></span></p><h2 id="1102-Invert-a-Binary-Tree-翻转二叉树"><a href="#1102-Invert-a-Binary-Tree-翻转二叉树" class="headerlink" title="1102 Invert a Binary Tree (翻转二叉树)"></a>1102 Invert a Binary Tree (翻转二叉树)</h2><h3 id="1-题目大意-9"><a href="#1-题目大意-9" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>给定一棵二叉树，先翻转好，输出层序遍历和中序遍历</p><h3 id="2-分析-6"><a href="#2-分析-6" class="headerlink" title="2. 分析"></a>2. 分析</h3><p>可以写一个 invert 函数，交换本节点左右元素，然后对左右孩子递归即可. </p><pre><code class="lang-cpp">struct node{    int left=-1, right=-1;  //-1 表示 NULL};void invert(int r){    int a = v[r].left, b = v[r].right;    if(a == -1 &amp;&amp; b == -1)        return;    v[r].left = b;    v[r].right = a;    if(v[r].left != -1) invert(v[r].left);    if(v[r].right != -1) invert(v[r].right);}</code></pre><p>也可以不用写，正常遍历即可，只是先遍历右孩子，再遍历左孩子即可. </p><pre><code class="lang-cpp">//层序遍历标准模式：void level(int r){    queue&lt;int&gt; q;    q.push(r);    while (!q.empty()){        r = q.front();        q.pop();        // if(q.empty()) break;        le_ans.push_back(r);        if(v[r].left != -1) q.push(v[r].left);        if(v[r].right != -1) q.push(v[r].right);    }}</code></pre><h3 id="3-坑点："><a href="#3-坑点：" class="headerlink" title="3. 坑点："></a>3. 坑点：</h3><ol><li>一般的题，都要考虑元素可能有重复，可能不按顺序. 本题使用    <code>mark</code>数组确定<code>root</code>是最好的方式</li><li><code>queue</code>和一般的<code>vector</code>有本质的不同，不能用迭代器遍历，不能索引，只能通过<code>front(), back()</code>访问. <code>quque</code>进行<code>pop(),front()</code>等操作时，一定要检查<code>q.empty()</code></li><li>当<code>queue</code>中只有一个元素时，<code>pop()</code>后，在访问<code>front()</code>可能会出现异常数字</li></ol><hr><p><span id="1040"></span></p><h2 id="1040-Longest-Symmetric-String-求最长对称子串-DP"><a href="#1040-Longest-Symmetric-String-求最长对称子串-DP" class="headerlink" title="1040 Longest Symmetric String (求最长对称子串 DP)"></a>1040 Longest Symmetric String (求最长对称子串 DP)</h2><h3 id="1-题目大意-10"><a href="#1-题目大意-10" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>给定一个字符串，找到最长的、对称的子串，输出长度<br>比如<code>Is PAT&amp;TAP symmetric?</code> 输出 11, 即<code>s PAT&amp;TAP s</code>的长度</p><h3 id="2-分析-7"><a href="#2-分析-7" class="headerlink" title="2. 分析"></a>2. 分析</h3><ol><li><p>基本递归的思路，大问题拆成小问题，拆成一个已知条件下的具体问题. 对于 子串/子序列 问题，要么用 <code>dp[i][j]</code>表示 i 和 j 的子串，要么用 <code>dp[i]</code>表示 0 到 i 之间的子串.</p></li><li><p>本题要求每一对<code>dp[i][j]</code>就要已知<code>dp[i+1][j-1]</code>, 再根据<code>str[i]</code>和<code>str[j]</code>来判断本段是否对称, 复杂问题被简化, 逐层逐层抛弃.</p></li><li><p>dp 的另外一个关键就在于初始化, 本题将所有<code>dp[i][i]</code>和相邻相等的所有<code>dp[i][i-1]</code>赋值为 1</p></li><li><p>本题一个难点在于, 普通的 dp 都可以从序列开头递归即可, 本题要根据子串的长度, 从短到长进行循环, 从中沉淀出最长的 length. 即区间 DP 的基本思路：区间长度作为最外层循环，端点作为内层循环.</p></li></ol><h3 id="3-个人代码-7"><a href="#3-个人代码-7" class="headerlink" title="3. 个人代码"></a>3. 个人代码</h3><p><a href="https://pintia.cn/problem-sets/994805342720868352/problems/994805446102073344" target="_blank" rel="noopener">PTA-1040</a></p><pre><code class="lang-c++">for (int L = 3; L &lt;= len; ++L) {   //线段长度由小到大        for (int i = 0; i &lt; len - L + 1; ++i) {  //能够到达的最右端点            int j = i + L - 1;  //本次 i 对应的右端点 j            if(str[i] == str[j] &amp;&amp; dp[i+1][j-1]){  //更新dp                dp[i][j] = 1;                ans = L;            }        }    }</code></pre><hr><p><span id="1045"></span></p><h2 id="1045-Favorite-Color-Stripe-最长不下降子序列-DP"><a href="#1045-Favorite-Color-Stripe-最长不下降子序列-DP" class="headerlink" title="1045 Favorite Color Stripe (最长不下降子序列 DP)"></a>1045 Favorite Color Stripe (最长不下降子序列 DP)</h2><h3 id="1-题目大意-11"><a href="#1-题目大意-11" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>给出m中颜色作为喜欢的颜色（同时也给出顺序），然后给出一串长度为L的颜色序列，现在要去掉这个序列中的不喜欢的颜色，然后求剩下序列的一个子序列，使得这个子序列表示的颜色顺序符合自己喜欢的颜色的顺序，不一定要所有喜欢的颜色都出现.<br>1 - N 表示 N 种颜色, 比如给出 2 3 1 5 6 表示自己喜欢的颜色序列</p><h3 id="2-分析-8"><a href="#2-分析-8" class="headerlink" title="2. 分析"></a>2. 分析</h3><p>在原串中要找出符合 2 3 1 5 6 顺序的所有子串, 可能需要多层循环遍历, 还要用额外数组 tmp 临时记录, 数据难以比较和判断, 很麻烦. 所以考虑转化成有序数组, 一一对应成 1 2 3 4 5, 通过比大小即可判断, 等价于转化成一个<strong>最长不下降子序列</strong>问题.</p><p>维护单调数组 dp, <code>dp[i]</code>表示从 0 - i 最长不下降子序列的长度, 可以 DP 的精髓就在于, 本位置 i 对应的<code>dp[i]</code>可以由前面的<code>dp[0]</code> — <code>dp[i-1]</code> 直接推出.</p><p>举例:</p><pre><code>i  2 1 5 3 6 4 8 9 7dp 1 1 2 2 3 3 4 5 4j  2 1 5 3 6 4 8 9 7</code></pre><p>此过程中沉淀出的 maxn 就是 5</p><h3 id="3-个人代码-8"><a href="#3-个人代码-8" class="headerlink" title="3. 个人代码"></a>3. 个人代码</h3><pre><code class="lang-c++">int book[201], a[10001], dp[10001];//dp[i]内存储的是, 从 0 到 i 为止最长的长度, 与 book 无关int main(){    int n, m, l, tmp, maxn = -1;    cin &gt;&gt; n &gt;&gt; m;    for (int i = 1; i &lt;= m; ++i) {  //颜色为 1-n 将颜色序列映射为 idx 递增序列        cin &gt;&gt; tmp;        book[tmp] = i;    }    cin &gt;&gt; l;    int num = 0;    for (int i = 0; i &lt; l; ++i) {  //关键一步，把所有颜色转化为序号，依次存入 a 中        cin &gt;&gt; tmp;        if(book[tmp] &gt;= 1){            a[num++] = book[tmp];  //a 存储 idx        }    }    for (int i = 0; i &lt; num; ++i) {   //不喜欢的颜色直接忽略了,不计入考察范围        dp[i] = 1;   //每次初始赋值为 1, 检查 0-i        for (int j = 0; j &lt; i; ++j) {            if(a[i] &gt;= a[j]){   //以 a 为评判标准, 大于等于就能做下一步                dp[i] = max(dp[i], dp[j]+1);                //核心思想，已知0 ~ i-1 的最大值，新来的 i 最多比0 ~ i-1 多 1            }        }        maxn = max(maxn, dp[i]);    }    cout &lt;&lt; maxn;    return 0;}</code></pre><hr><p><span id="T4861"></span></p><h2 id="TK-4861-最原始的最长不下降子序列-DP"><a href="#TK-4861-最原始的最长不下降子序列-DP" class="headerlink" title="TK 4861 最原始的最长不下降子序列 DP"></a>TK 4861 最原始的最长不下降子序列 DP</h2><h3 id="1-题目大意-12"><a href="#1-题目大意-12" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>输入输出样例</p><pre><code>73 7 2 5 6 3 8-----4</code></pre><h3 id="2-个人代码-1"><a href="#2-个人代码-1" class="headerlink" title="2.个人代码"></a>2.个人代码</h3><p>核心判断 <code>if(v[j] &gt; v[i]) dp[j] = max(dp[j], dp[i]+1)</code> 两层循环, 对于每个 i, 从 i 之后开始遍历, 比它大则一定可以在它的 dp[i] 基础上 +1.</p><pre><code class="lang-cpp">int main(){    int n, max_len=-1;    cin &gt;&gt; n;    vector&lt;int&gt; v(n), dp(n);  // dp[i]表示 0 ~ i 目前最长的不下降序列长度    for (int i = 0; i &lt; n; ++i) {        cin &gt;&gt; v[i];        dp[i] = 1;    }    for (int i = 0; i &lt; n; ++i) {        for (int j = i; j &lt; n; ++j) {   // 从 i 之后开始遍历            if(v[j] &gt; v[i])                dp[j] = max(dp[j], dp[i]+1);            max_len = max(max_len, dp[j]);  // 沉淀出最大的长度        }    }    cout &lt;&lt; max_len;    return  0;}</code></pre><hr><p><span id="1068"></span></p><h2 id="1068-Find-More-Coins-01-背包问题-DP"><a href="#1068-Find-More-Coins-01-背包问题-DP" class="headerlink" title="1068 Find More Coins (01 背包问题 DP)"></a>1068 Find More Coins (01 背包问题 DP)</h2><h3 id="1-题目大意-13"><a href="#1-题目大意-13" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>用n个硬币买价值为m的东西，输出使用方案，使得正好几个硬币加起来价值为m. 从小到大排列，输出最小的那个排列方案</p><h3 id="2-尝试-DFS"><a href="#2-尝试-DFS" class="headerlink" title="2. 尝试(DFS)"></a>2. 尝试(DFS)</h3><p>本题可以采用递归方案, DFS 递归到底, 直到刚好<code>target==0</code>即可, 类似于二叉树的寻找路径和. 但是某个测试点会超时.</p><p>核心代码(DFS 递归):</p><pre><code class="lang-c++">bool sear(int st, int target){   //搜索 st ~ n, 每次从 st 搜到末位    if(0 == target)        return true;    for (int i = st; i &lt; n; ++i) {        if(target &lt; a[i])   //后面的面额只会越来越大,不用搜了(剪枝)            return false;        tmp.push_back(a[i]);        if (sear(i+1, target-a[i]))  //target减去当前,大目标拆成小目标            return true;   //找到一个为 true 就不用再搜了,返回        else tmp.pop_back();  //sear()==false 表示走不通, pop出来换下一个    }    return false;}sort(a, a+n);  //从小到大排序, 便于输出最小的方案if(a[0] &gt; m){ printf(&quot;No Solution\n&quot;); return 0; } //面额太大,付不了小钱for (int i = 0; i &lt; n; ++i) {  //逐个作为起点进行搜索    tmp.clear();    if(sear(i, m))        break;}</code></pre><h3 id="3-0-1背包问题-DP"><a href="#3-0-1背包问题-DP" class="headerlink" title="3. 0-1背包问题(DP)"></a>3. 0-1背包问题(DP)</h3><p>给定 n 种物品和一个容量为 C 的背包，物品 i 的重量是 wi，其价值为 vi . </p><p>问：应该如何选择装入背包的物品，使得装入背包中的物品的总价值最大？<br>DP 的题还是要用 DP 来做, 关键在于搞清楚每一个数组的作用</p><p>🎒背包问题的经典转移公式</p><pre><code class="lang-c++">if(j &gt;= w[i])    dp[i][j] = max(dp[i-1][j], dp[i-1][j-w[i]]+v[i]);else    dp[i][j] = dp[i-1][j];  //面对这个 i 物品, 不拿(或装不下), 就等同于 i-1 的情况</code></pre><p><code>dp[i][j]</code> 表示 在面对第 i 件物品，且背包容量为 j 时所能获得的最大价值<br><code>dp[i-1][j-w[i]]+v[i]</code>则是容量空出<code>w[i]</code>的情况下( i-1表示回溯一个序号), 装入<code>v[i]</code>所能得到的价值.</p><p><a href="#T21802">👆最基本背包: 采药🌿</a></p><h3 id="4-正解-DP"><a href="#4-正解-DP" class="headerlink" title="4. 正解(DP)"></a>4. 正解(DP)</h3><pre><code>8 95 9 8 7 2 3 4 1</code></pre><pre><code>1 3 5</code></pre><pre><code class="lang-c++">int dp[101], choice[10001][101], w[10001];//dp[j] 表示容量为 j 的时候, 最多能装多少钱数; w为每个硬币的面额//choice[i][j]=true 表示总目标钱数为 j 时, i 可以作为一个选择</code></pre><p>本题不是求最大价值, 只要求刚好装满, 只需要加一个判断<code>if(dp[m] != m)</code>, 然后以 m 为目标, 添加所有 choice 筛选的 [<strong>总目标钱数为 m 时, 可以作为一个选择</strong>] 的序列</p><p>本题的判断精髓就在这一句<code>if(dp[j] &lt;= dp[j-w[i]] + w[i])</code> 含义就是, 选择 w[i] 之后, 也能凑得和<code>dp[j]</code>之前 <strong>一样的, 甚至更多的</strong> 钱数.</p><p>本题实际发挥作用的时候, 小于号<code>&lt;</code>实际发挥的作用就是初始化, 让<code>dp[j]</code>从0变成一个数值, 比如<code>dp[9]=9</code>, 9 的面额直接满足; 等于号<code>=</code>实际发挥的作用就是寻找新的路径, 更新<code>choice[i][j]</code> 让<code>1 3 5</code>也组成进来.</p><p>比如, i=5 的时候, w[i]=4 已经更新了 dp[9]=9, dp[5]=5, 此时 j 从 m 遍历递减, 又从 9 开始, 有<code>dp[9] = dp[5] + w[i]</code>, 就把 i=5, 即 w[i]=4 添加进<code>choice[9][5] = true</code>, 作为路径之一</p><pre><code class="lang-c++">sort(w + 1, w + n + 1, cmp);  //从大的面额找起for (int i = 1; i &lt;= n; ++i) {    for (int j = m; j &gt;= w[i]; --j) {    //容量从 m 递减,直到比 w[j] 小则装不下,忽略        if(dp[j] &lt;= dp[j-w[i]] + w[i]){            choice[i][j] = true;            dp[j] = dp[j-w[i]] + w[i];        }    }}if(dp[m] != m) cout &lt;&lt; &quot;No Solution&quot;;else{    int v = m, index = n;  //v 表示目标容量, n 表示初始序号在末尾(从最小的选起)    while (v &gt; 0){        if(choice[index][v]){  //目标为 v 的时候, w[index] 可以作为选项            arr.push_back(w[index]);            v -= w[index];        }        index--;    }    //输出 arr}</code></pre><h3 id="5-后记"><a href="#5-后记" class="headerlink" title="5. 后记"></a>5. 后记</h3><p>一道基础的背包问题, 居然思考了我一整个下午, 算法基础真的薄弱. 鉴于网上的题解要么不够大佬, 代码不够最优; 要么太过于大佬, 解释得很简略. 于是我吧最大佬的代码, 用最清楚的语言解释, 是对自己思路的一个锻炼, 也希望帮助后来的萌新们.</p><hr><p><span id="NK-zxyps"></span></p><h2 id="NK-最小邮票数-动态规划，特定值子序列和"><a href="#NK-最小邮票数-动态规划，特定值子序列和" class="headerlink" title="NK 最小邮票数 (动态规划，特定值子序列和)"></a>NK 最小邮票数 (动态规划，特定值子序列和)</h2><h3 id="1-题目大意-14"><a href="#1-题目大意-14" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>题目描述<br>    有若干张邮票，要求从中选取最少的邮票张数凑成一个给定的总值。     如，有1分，3分，3分，3分，4分五张邮票，要求凑成10分，则使用3张邮票：3分、3分、4分即可。<br>输入描述:<br>    有多组数据，对于每组数据，首先是要求凑成的邮票总值M，M&lt;100。然后是一个数N，N〈20，表示有N张邮票。接下来是N个正整数，分别表示这N张邮票的面值，且以升序排列。<br>输出描述:<br>      对于每组数据，能够凑成总值M的最少邮票张数。若无解，输出0。<br>输入</p><pre><code>1051 3 3 3 4</code></pre><p>输出</p><pre><code>3</code></pre><h3 id="2-题解"><a href="#2-题解" class="headerlink" title="2.题解"></a>2.题解</h3><p><a href="https://github.com/cowarder/Algorithms/blob/master/%E7%89%9B%E5%AE%A2/%E6%9C%80%E5%B0%8F%E9%82%AE%E7%A5%A8%E6%95%B0.cpp" target="_blank" rel="noopener">牛客-最小邮票数</a></p><hr><p><span id="1044"></span></p><h2 id="1044-Shopping-in-Mars-特定值子序列和-二分查找"><a href="#1044-Shopping-in-Mars-特定值子序列和-二分查找" class="headerlink" title="1044 Shopping in Mars (特定值子序列和, 二分查找)"></a>1044 Shopping in Mars (特定值子序列和, 二分查找)</h2><h3 id="1-题目大意-15"><a href="#1-题目大意-15" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>求一串的数字中连续的一段，使得这个连续的段内数字的和恰好等于所期望的值m. 如果不能找到恰好等于，就找让自己付出最少的价格（总和必须大于等于所给值）的那段区间, 求所有可能的结果.<br>简而言之, 就是找一个连续子序列, 恰好等于 target</p><h3 id="2-个人代码-2"><a href="#2-个人代码-2" class="headerlink" title="2. 个人代码"></a>2. 个人代码</h3><p>因为给定的金币序列是乱序的, 这题一开始没想到可以二分法. 其实题目要求的是连续的子段, 那么就可以逐级叠加求<code>sum</code>, <code>a[j] - a[i-1]</code>即是 i~j 这一段的子段和. 而这个逐级叠加的<code>sum</code>数组, 就是一个完美的递增序列.</p><p>注意解题技巧, 需要多个参数时, 可以采用<code>int &amp;j, int &amp;temp_sum</code>引用传参.</p><pre><code class="lang-c++">#include &lt;iostream&gt;#include &lt;stdlib.h&gt;#include &lt;vector&gt;#include &lt;map&gt;#include &lt;algorithm&gt;using namespace std;vector&lt;int&gt; sum, ans;int n, m;// 从 i 开始, 以 n 结尾的序列, 二分查找// 目标: 找到右端点, 刚好比左端点大 mvoid search(int i, int &amp;j, int &amp;temp_sum){    int left = i, right = n;    int target = sum[left-1] + m;    while (left &lt; right){        // 保留右侧的二分查找, 不求找到而退出        // 最后让 left==right 时退出, 此时是比 target 大的最小数        // 因为 mid 小了的时候, 总是舍弃左端点, mid 大了的时候, 右端点被保留        int mid = (left+right)/2;        if(sum[mid] &lt; target)            left = mid + 1;        else            right = mid;    }    j = right;    temp_sum = sum[right] - sum[i-1];}int main(){    cin &gt;&gt; n &gt;&gt; m;    sum.resize(n+1);    sum[0] = 0;    for (int i = 1; i &lt;= n; ++i) {  //转化为递增序列, 得以应用二分, 提高效率        cin &gt;&gt; sum[i];        sum[i] += sum[i-1];    }    int min_ans = sum[n];  //求比 m 大的最小和    for (int i = 1; i &lt;= n; ++i) {        int j=0, temp_sum=0;  //仅仅作为参数传回来而已        search(i, j, temp_sum);        if(temp_sum &lt; m) continue;        else{            if(temp_sum &lt; min_ans){                min_ans = temp_sum;                ans.clear();                ans.push_back(i);                ans.push_back(j);            }            else if (temp_sum == min_ans){                ans.push_back(i);                ans.push_back(j);            }        }    }    for (int i = 0; i &lt; ans.size(); i += 2) {        cout &lt;&lt; ans[i] &lt;&lt; &quot;-&quot; &lt;&lt; ans[i+1] &lt;&lt; endl;    }    return 0;}</code></pre><hr><p><span id="1085"></span></p><h2 id="1085-Perfect-Sequence-二分查找-upper-bound-双指针"><a href="#1085-Perfect-Sequence-二分查找-upper-bound-双指针" class="headerlink" title="1085 Perfect Sequence (二分查找, upper-bound, 双指针)"></a>1085 Perfect Sequence (二分查找, upper-bound, 双指针)</h2><h3 id="1-题目大意-16"><a href="#1-题目大意-16" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>设这个数列中的最大值是M，最小值是m，如果M &lt;= m * p，则称这个数列是完美数列. 现在给定一个序列, 求满足完美序列的最长子序列长度</p><h3 id="2-二分法"><a href="#2-二分法" class="headerlink" title="2.二分法"></a>2.二分法</h3><p>本题可以应用 upper_bound 来查找, 既然要求满足最大的<code>max &lt;= p*min</code>, 那么找到第一个<code>max &gt; p*min</code>即可, 即找到第一个大于target 的元素.</p><p><strong>upper-bound 代码</strong></p><pre><code class="lang-c++">int bin_sear_upper(int left, int right, int target){    int mid;    while (left &lt; right){        mid = (left+right)/2;        if(a[mid] &gt; target)            right = mid;        else            left = mid + 1;    }    return right;}</code></pre><p>关键点 1: <code>left &lt; right</code>到最后必然会夹出 left==right 返回即可.</p><p>关键点 2: 先判断<code>&gt;</code>, 让<code>right = mid</code>目的是尽可能保留大数,<code>left = mid + 1</code>小数不保留无所谓.</p><p><strong>lower-bound 代码</strong></p><pre><code class="lang-c++">if(a[mid] &gt;= target)    right = mid;else    left = mid + 1;</code></pre><p>只需换为<code>&gt;=</code>即可, 其他完全一样, 也是保大.</p><p>比如 3 4 5 5 7 8 8 8 9, target 为 8.<br>upper_bound 找到的为 3 4 5 5 7 [8] 8 8 9, lower_bound 找到的为 3 4 5 5 7 8 8 8 [9].</p><p><strong>普通的查找定值</strong><br><code>if(a[mid]==target)</code>, 需要<code>left &lt;= right</code>作为条件, 找不到就返回-1</p><pre><code class="lang-cpp">while(low &lt;= high){    int mid = (low + high)/2;    if(array[mid] &lt; key)        low = mid + 1;    else if(array[mid] &gt; key)        high = mid - 1;    else        return mid;}return -1;</code></pre><p>这三种二分法基本可以涵盖所有二分查找的应用.</p><h3 id="3-个人代码-9"><a href="#3-个人代码-9" class="headerlink" title="3. 个人代码"></a>3. 个人代码</h3><pre><code class="lang-c++">int search(int left, long long target){  // 寻找比 target 小的最大数 upper_bound    int right = n-1, mid;    if(a[right] &lt;= target){  //最大的比 target 小        return right;    }    while (left &lt; right){        mid = (left + right)/2;        if(a[mid] &gt; target)            right = mid;        else            left = mid + 1;    }    return right-1;  // 最后 left==right 找到的是第一个比 tar 大的数, -1 即可}</code></pre><p>应用的是 upper_bound, <code>right-1</code>即可得到满足<code>max &lt;= p*min</code>的最大数字.</p><h3 id="4-坑点-1"><a href="#4-坑点-1" class="headerlink" title="4. 坑点"></a>4. 坑点</h3><ol><li><code>段错误</code> 数组开的不够大, 递归调用过多</li><li><code>答案错误</code> 发现存在溢出, int 最大管到 $10^9$, 此题乘起来之后会超过, 要用 long long !</li></ol><h3 id="5-Two-points-双指针法"><a href="#5-Two-points-双指针法" class="headerlink" title="5. Two points 双指针法"></a>5. Two points 双指针法</h3><p>瞬间非常简单</p><pre><code class="lang-c++">    sort(v.begin(), v.end());    int result = 0, temp = 0;    for (int i = 0; i &lt; n; i++) {        for (int j = i + result; j &lt; n; j++) {          //result 已经是最长了, 之后的直接从 result 以上的找起            if (v[j] &lt;= v[i] * p) {                temp = j - i + 1;                if (temp &gt; result)                    result = temp;            } else {                break;            }        }    }    cout &lt;&lt; result;</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/12/Buth7AGTcCpPonS.png" alt="效率对比差距明显" width="140" height="180"/></p><hr><p><span id="1029"></span></p><h2 id="1029-Median-找中位数-计数排序"><a href="#1029-Median-找中位数-计数排序" class="headerlink" title="1029 Median (找中位数 计数排序)"></a>1029 Median (找中位数 计数排序)</h2><h3 id="1-题目大意-17"><a href="#1-题目大意-17" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><hr><p><span id="1143"></span></p><h2 id="1143-Lowest-Common-Ancestor-最近的公共祖先"><a href="#1143-Lowest-Common-Ancestor-最近的公共祖先" class="headerlink" title="1143 Lowest Common Ancestor (最近的公共祖先)"></a>1143 Lowest Common Ancestor (最近的公共祖先)</h2><h3 id="1-题目大意-18"><a href="#1-题目大意-18" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>给定二叉搜索树 BST 的 pre 前序遍历, 再给定一对 U,V 求 U,V 的 LCA</p><h3 id="2-分析-9"><a href="#2-分析-9" class="headerlink" title="2. 分析"></a>2. 分析</h3><p>本题有两个特殊条件, 一个是<code>BST</code>, 形式是<code>左边小右边大</code>, 一个是<code>前序遍历pre</code>, 对于数组形式的 pre 序列, 形式是<code>根左右</code>. 所以只需要 u,v 分别在 a 的两端, 一个比 a 大, 一个比 a 小, a 就是公共祖先. 当 <code>a==u || a==v 时</code>, 谁在前谁就是祖先</p><h3 id="3-代码"><a href="#3-代码" class="headerlink" title="3. 代码"></a>3. 代码</h3><pre><code class="lang-cpp">map&lt;int, bool&gt; mp;int main() {    int m, n, u, v, a;    scanf(&quot;%d %d&quot;, &amp;m, &amp;n);    vector&lt;int&gt; pre(n);    for (int i = 0; i &lt; n; i++) {        scanf(&quot;%d&quot;, &amp;pre[i]);        mp[pre[i]] = true;    }    for (int i = 0; i &lt; m; i++) {        scanf(&quot;%d %d&quot;, &amp;u, &amp;v);        for(int j = 0; j &lt; n; j++) {            a = pre[j];            if ((a &gt;= u &amp;&amp; a &lt;= v) || (a &gt;= v &amp;&amp; a &lt;= u)) break;        }         if (mp[u] == false &amp;&amp; mp[v] == false)            printf(&quot;ERROR: %d and %d are not found.\n&quot;, u, v);        else if (mp[u] == false || mp[v] == false)            printf(&quot;ERROR: %d is not found.\n&quot;, mp[u] == false ? u : v);        else if (a == u || a == v)            printf(&quot;%d is an ancestor of %d.\n&quot;, a, a == u ? v : u);        else            printf(&quot;LCA of %d and %d is %d.\n&quot;, u, v, a);    }    return 0;}</code></pre><hr><p><span id="1071"></span></p><h2 id="1071-Speech-Patterns-词频统计"><a href="#1071-Speech-Patterns-词频统计" class="headerlink" title="1071 Speech Patterns (词频统计)"></a>1071 Speech Patterns (词频统计)</h2><h3 id="1-题目大意-19"><a href="#1-题目大意-19" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>给定一行句子, 统计出现频率最高的 word. word 定义为仅包含<code>0-9 A-Z a-z</code>的连续串. 如果有相同词频, 就输出字典序最小的.</p><pre><code>Can1: &quot;Can a can can a can?  It can!&quot;</code></pre><pre><code>can 5</code></pre><h3 id="2-个人代码-3"><a href="#2-个人代码-3" class="headerlink" title="2. 个人代码"></a>2. 个人代码</h3><pre><code class="lang-cpp">map&lt;string, int&gt; m;  //会按照键的字典序排序int main(){    string str, t;    int maxn=-1, i=0;    getline(cin, str);    for (auto &amp;c : str) {        i++;        if(isalnum(c)){            t += tolower(c);        }        if(!isalnum(c) || i==str.length()){ //碰到间隙或者结尾时, 进行计数            if(t.length()) m[t]++;            t = &quot;&quot;;        }    }    for(auto it=m.begin(); it!=m.end(); it++){        if(it-&gt;second &gt; maxn){            maxn = it-&gt;second;            t = it-&gt;first;        }    }    cout &lt;&lt; t &lt;&lt; &quot; &quot; &lt;&lt; maxn;    return 0;}</code></pre><hr><p><span id="T21802"></span></p><h2 id="TK-21802-采药-最基础的容量-价值最大化-背包问题-DP"><a href="#TK-21802-采药-最基础的容量-价值最大化-背包问题-DP" class="headerlink" title="TK 21802  采药 (最基础的容量-价值最大化, 背包问题, DP)"></a>TK 21802  采药 (最基础的容量-价值最大化, 背包问题, DP)</h2><h3 id="1-题目大意-20"><a href="#1-题目大意-20" class="headerlink" title="1.题目大意"></a>1.题目大意</h3><p><strong>输入</strong><br>输入的第一行有两个整数T（1 &lt;= T &lt;= 1000）和M（1 &lt;= M &lt;= 100），用一个空格隔开，T代表总共能够用来采药的时间，M代表山洞里的草药的数目. 接下来的M行每行包括两个在1到100之间（包括1和100）的整数，分别表示采摘某株草药的时间和这株草药的价值.<br><strong>输出</strong><br>输出包括一行，这一行只包含一个整数，表示在规定的时间内，可以采到的草药的最大总价值.</p><pre><code>70 371 10069 11 2------3</code></pre><h3 id="2-方法一"><a href="#2-方法一" class="headerlink" title="2.方法一"></a>2.方法一</h3><p>本题只需使用最最经典原始的背包问题转移公式即可</p><pre><code class="lang-cpp">int v[110], t[110], dp[110][1010];  // dp[i][j] 表示面对第 i 个,容量为 j 时 的最大价值int main(){    int T, m;   // T 总时间, m 个药草    cin &gt;&gt; T &gt;&gt; m;    for (int i = 1; i &lt;= m; ++i) {        cin &gt;&gt; t[i] &gt;&gt; v[i];    }    for (int i = 1; i &lt;= m; ++i) {        for (int j = 0; j &lt;= T; ++j) {            if(j &gt;= t[i])                dp[i][j] = max(dp[i-1][j-t[i]] + v[i], dp[i-1][j]);            else                dp[i][j] = dp[i-1][j];  // i-1 表示不拿这一个草药, 相当于没碰见前        }    }    cout &lt;&lt; dp[m][T];    return  0;}</code></pre><p>坑点: 一定要注意 dp 数组的范围, 1010 不能开小了</p><h3 id="3-方法二"><a href="#3-方法二" class="headerlink" title="3.方法二"></a>3.方法二</h3><p>一维数组 dp, 无需每一个容量面对每一个物品, 只需设定 dp[x] 表示容量为 x 时的最大价值. 状态转移方程:<br><code>dp[v] = max (dp[v] , dp[v - t[i]] + v[i])</code><br>其中 t[i] 表示草药的采集时间 ， v[i] 指草药的价值.</p><pre><code class="lang-cpp">const int maxv=1010,maxn=110;int dp[maxv], t[maxn], v[maxn];...    for(int i=1;i&lt;=n;++i)        for(int x = T; x &gt;= t[i]; --x)  //从 T 开始遍历            dp[x] = max(dp[x], dp[x-t[i]] + v[i]);  //装 or 不装    for(int x = 0; x &lt;= T; ++x)        if(dp[x] &gt; max)            max = dp[x];...}</code></pre><hr><p><span id="H1130"></span></p><h2 id="H1130-分成两个尽可能接近的子数组-背包问题-DP"><a href="#H1130-分成两个尽可能接近的子数组-背包问题-DP" class="headerlink" title="H1130 分成两个尽可能接近的子数组 (背包问题 DP)"></a>H1130 分成两个尽可能接近的子数组 (背包问题 DP)</h2><h3 id="1-题目大意-21"><a href="#1-题目大意-21" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>小明班里要举行一次拔河比赛，班主任决定将所有人分为两队，每个人都必须参加. 两个队伍的人数之差不能超过1，并且两个队伍的体重之和<code>要尽可能相近</code>，当然相同是最好的了. 分别输出两个队伍的体重之和，按升序排序.</p><p>简而言之, 将一个数组分成两个尽可能接近的子数组.</p><h3 id="2-分析-10"><a href="#2-分析-10" class="headerlink" title="2. 分析"></a>2. 分析</h3><p>有一系列元素, 让这些元素尽可能地接近一个目标值, 这就是典型的背包 DP 问题. 本题的关键在于, 只研究一个数组, 让它尽可能接近 sum/2 即可, 另一个就与它最接近.</p><h3 id="3-核心代码"><a href="#3-核心代码" class="headerlink" title="3. 核心代码"></a>3. 核心代码</h3><pre><code class="lang-cpp">int sum = 0;int a[n+1];for (int i = 1; i &lt;= n; ++i) {    cin &gt;&gt; a[i];    sum += a[i];}vector&lt;int&gt; dp(sum);  //dp[i]表示容量为 i 时能装下的总量fill(dp.begin(), dp.end(), 0);for (int i = 1; i &lt;= n; i++){  //面对第 i 个的时候    for (int j = sum/2; j &gt;= a[i]; j--){  //容量从目标开始减少        dp[j] = max(dp[j], dp[j-a[i]] + a[i]); //标准转移公式    }}int tmp = max(dp[sum/2], sum-dp[sum/2]);printf(&quot;%d %d\n&quot;, sum-tmp, tmp);</code></pre><p>此程序的目的, 就是给定一个 target=sum/2, 让元素通过各种组合, 去无限逼近 target.</p><p>对于<code>dp[j-a[i]] + a[i])</code>要理解清楚, 表示<code>已知</code>空出 a[i] 后的最大容量, 再加入 a[i] 看效果. 因为对于每一种容量, 不可能都会<code>恰好装满</code>, 所以 dp[5] 可能等于 3, 那么面对一个 4, dp[9] 就等于 3+4=7.</p><h3 id="4-记忆点"><a href="#4-记忆点" class="headerlink" title="4. 记忆点"></a>4. 记忆点</h3><ul><li><p>两层循环, 外层一件件物品遍历, <code>for (int i = 1; i &lt;= n; i++)</code>, 内层从目标容量递减 (容量是连续值) <code>for (int j = sum/2; j &gt;= a[i]; j--)</code>, 直到装不下为止.</p></li><li><p>转移公式<code>dp[j] = max(dp[j], dp[j-a[i]] + a[i]);</code>, 对应两种情况, <code>看见了 a[i] 但不装</code>, 和 <code>装下 a[i]</code></p></li><li><p>此处使用一位数组 dp, 和二维 dp 数组区别开</p></li></ul><hr><p><span id="1067"></span></p><h2 id="1067-Sort-with-Swap-0-i-贪心-元素归位"><a href="#1067-Sort-with-Swap-0-i-贪心-元素归位" class="headerlink" title="1067 Sort with Swap(0, i) (贪心, 元素归位)"></a>1067 Sort with Swap(0, i) (贪心, 元素归位)</h2><h3 id="1-题目大意-22"><a href="#1-题目大意-22" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>给定一个 0~n-1 的乱序序列, 每次只能用 0 与某个数交换, 求需要交换多少次才能归位.</p><pre><code>103 5 7 2 6 4 9 0 8 1</code></pre><pre><code>9</code></pre><h3 id="2-分析-11"><a href="#2-分析-11" class="headerlink" title="2. 分析"></a>2. 分析</h3><p>基本思路: 把 0 当做哨兵, 根据 0 处在的位置 i , 每次让 0 与原本应处于 i 的那个数去交换. 如果 0 已经归位, 则顺次查找下一个不在本位的进行交换.</p><p>一开始的做法是, 维护原来的数组, 每次 for 循环遍历查找对应的那个数, 结果超时/出错.</p><p>本题巧妙之处在于, 由于数组是一个 0~n-1 的无重复数组, 不一定要保存值的数组, 而是保存一个索引的数组, 因为 swap 目的就是要交换位置, <code>交换索引</code>而已. 交换索引后, 不需要遍历查找对应数, 因为 a[0] 就是 0 所处在的位置index, 交换 a[0] 和 a[a[0]] 即可.</p><h3 id="3-个人代码-10"><a href="#3-个人代码-10" class="headerlink" title="3. 个人代码"></a>3. 个人代码</h3><p>一个 n 的 for 循环, 当 n 个都满足<code>a[i] != i</code>就结束</p><pre><code class="lang-cpp">vector&lt;int&gt; a(n);for (int i = 0; i &lt; n; ++i) {    cin &gt;&gt; t;    a[t] = i;   // t 对应的索引}for (int i = 1; i &lt; n; ++i) {  //a[0] 即 0 对应的索引, 每次拿 0 交换    while (a[0] != 0){        swap(a[0], a[a[0]]);   //可以一直换 0 的索引        cnt++;    }    if (a[i] != i){   //0 暂时归位,则判断当前 i 是否归位        swap(a[0], a[i]);        cnt++;    }}</code></pre><hr><p><span id="1100"></span></p><h2 id="1100-Complete-Binary-Tree-判断完全二叉树-层序遍历"><a href="#1100-Complete-Binary-Tree-判断完全二叉树-层序遍历" class="headerlink" title="1100 Complete Binary Tree (判断完全二叉树, 层序遍历)"></a>1100 Complete Binary Tree (判断完全二叉树, 层序遍历)</h2><h3 id="1-题目大意-23"><a href="#1-题目大意-23" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>给定一棵二叉树, 判断是不是完全二叉树</p><pre><code>97 8- -- -- -0 12 34 5- -- -</code></pre><h3 id="2-尝试层序遍历"><a href="#2-尝试层序遍历" class="headerlink" title="2.尝试层序遍历"></a>2.尝试层序遍历</h3><p>层序遍历分层输出: 添加一个<code>cur_len</code>表示本层目前的长度, <code>cur_len</code>通过<code>next_level</code>来维护, 每添加一个子节点进入队列, 就<code>next_level++</code>, 每 pop 一个节点, 就<code>cur_len--</code>. 直到<code>cur_len==0</code> 表示一层的结束, 可以做一个小结了.</p><pre><code class="lang-cpp">q.push(root);    int next_level = 0, cur_len = 1, no=-1;    vector&lt;int&gt; lev;    lev.push_back(1);    while (!q.empty()){        no = q.front();        cur_len--;  //本层减 1        if(v[no].left != -1){            next_level++;            q.push(v[no].left);        }        if(v[no].right != -1){            next_level++;            q.push(v[no].right);        }        if(cur_len == 0){   //本层结束            if(next_level) lev.push_back(next_level);            else break;            cur_len = next_level; //更新为下一层的长度            next_level = 0;        }        q.pop();    }</code></pre><p>但是这个方法只能判断前几层是否满足 1, 2, 4, 8 的规律, 无法判断最后一层.</p><h3 id="3-正确解法"><a href="#3-正确解法" class="headerlink" title="3.正确解法"></a>3.正确解法</h3><p>递归出最大的下标值，完全二叉树一定把前面的下标充满： 最大的下标值 == 最大的节点数；不完全二叉树前满一定有位置是空，会往后挤： 最大的下标值 &gt; 最大的节点数.</p><pre><code class="lang-cpp">struct node{    int left=-1, right=-1;};int maxn = -1, last;  //记录最大下标, 最后一个元素下标void dfs(int nod, int index){    if(index &gt; maxn){        maxn = index;        last = nod;    }    if(v[nod].left != -1)        dfs(v[nod].left, index*2+1);    if(v[nod].right != -1)        dfs(v[nod].right, index*2+2);}dfs(root, 0);if(maxn == n-1)    cout &lt;&lt; &quot;YES &quot; &lt;&lt; last;else    cout &lt;&lt; &quot;NO &quot; &lt;&lt; root;</code></pre><h3 id="4-坑点-2"><a href="#4-坑点-2" class="headerlink" title="4.坑点"></a>4.坑点</h3><p>注意会出现<code>段错误</code>, 一开始以为是数组开的不够大, 结果发现这个题真是犯了一个低级的错误，我开始用字符变量来输入，然而一个字符变量只能赋一个数字，所以如果数的数据有二位数，那么字符变量只能录入十位数字，或者就出错，出现段错误. 一定要用 string 然后 <code>stoi</code>, 不能用 char.</p><p>然后还有一种<code>运行时错误</code>, 想要通过 set 找到根节点, 不知道为什么错误.</p><pre><code class="lang-cpp">    {  ...       s.insert(stoi(a));       s.insert(stoi(b));    }   for (int i = 0; i &lt; n; ++i) {  //找到根节点       if(s.find(i) == s.end()){           root = i;           break;       }   }</code></pre><p>正确的方法应该是用数组进行标记, 找到 root.</p><pre><code class="lang-cpp">while (have[root] != 0) root++;</code></pre><hr><p><span id="T1131"></span></p><h2 id="TK-1131-分梨"><a href="#TK-1131-分梨" class="headerlink" title="TK 1131 分梨"></a>TK 1131 分梨</h2><h3 id="1-题目大意-24"><a href="#1-题目大意-24" class="headerlink" title="1.题目大意"></a>1.题目大意</h3><p>现在要把M个梨子放到N个盘子里面（我们允许盘子为空），有多少种分法？请注意，例如有三个盘子，我们将 5,1,1 和 1,1,5，视为同一种分法</p><h3 id="2-分析-12"><a href="#2-分析-12" class="headerlink" title="2.分析"></a>2.分析</h3><p>属于入门 DP, 掌握思想. 设 p（m,n）为有m个梨和n个盘子时的方案数，则可分情况讨论设计状态转移方程：</p><p>1：当 m &lt; n 时，则必有 n-m 个空盘子，因为盘子无顺序可言，可以忽略掉那 n-m 个空盘子, 则可推得 <code>p(m,n) = p(m,m)</code></p><p>2：当 m&gt;=n 时，则 <code>p(m,n) = p(m-n,n) + p(m,n-1)</code>, <code>p(m-n,n)</code> 表示 <strong>没有任何空盘子</strong>, 将所有盘子至少放一个梨子, <code>p(m,n-1)</code> 表示 <strong>至少有一个盘子为空</strong>, 多个盘子为空也包含在内.</p><p>3：最终递归的终极条件为，只有一个盘子或者没有梨</p><h3 id="个人代码"><a href="#个人代码" class="headerlink" title="个人代码"></a>个人代码</h3><pre><code class="lang-cpp">int Pear(int m, int n){//    if(m == 1 || n == 1 || m == 0 || n == 0) // 注意 1 个梨子或者 0 个盘子, 不行, 因为可以不放    if(n == 1 || m == 0)   //1 个盘子, 或者 0 个梨子, 一定为 1        return 1;    if(m &lt; n)        return Pear(m , m);    return Pear(m-n, n) + Pear(m, n-1);}int main(){    int t, m, n;    while (scanf(&quot;%d&quot;, &amp;t) != EOF){        while (t--){            cin &gt;&gt; m &gt;&gt; n;            cout &lt;&lt; Pear(m, n) &lt;&lt; endl;        }    }    return  0;}</code></pre><hr><p><span id="T1132"></span></p><h2 id="TK-1132-最长公共子序列-LCS-入门经典-DP"><a href="#TK-1132-最长公共子序列-LCS-入门经典-DP" class="headerlink" title="TK 1132 最长公共子序列 LCS (入门经典 DP)"></a>TK 1132 最长公共子序列 LCS (入门经典 DP)</h2><h3 id="1-题目大意-25"><a href="#1-题目大意-25" class="headerlink" title="1.题目大意"></a>1.题目大意</h3><p>样例输入</p><pre><code>abcfbc abfcabprogramming contest abcd mnp</code></pre><p>样例输出</p><pre><code>420</code></pre><h3 id="2-分析-amp-代码"><a href="#2-分析-amp-代码" class="headerlink" title="2.分析 &amp; 代码"></a>2.分析 &amp; 代码</h3><p>注意一个坑点, 下标一定要从 1 开始. 以及注意一下 scanf 读入字符串, 此题最好使用 char[] 数组.</p><pre><code class="lang-cpp">char a[110], b[110];  //string 无法读入int dp[110][110];  //dp[i][j] 表示 A_i 和 B_j 之间的最长公共长度// 注意坑点, 下标必须从 1 开始, 因为涉及 dp[i-1] 这种int main(){    while (scanf(&quot;%s %s&quot;, a+1, b+1) != EOF){  // 从 1 开始存储        int lenA=1, lenB=1;        while (a[lenA++] != &#39;\0&#39;);        while (b[lenB++] != &#39;\0&#39;);        lenA -= 2; lenB -= 2;        for (int i = 1; i &lt;= lenA; ++i)            for (int j = 1; j &lt;= lenB; ++j)                if(a[i] == b[j])    // 遇到共同字符直接 +1                    dp[i][j] = dp[i-1][j-1] + 1;                else     //没有遇到取最大                    dp[i][j] = max(dp[i-1][j], dp[i][j-1]);        cout &lt;&lt; dp[lenA][lenB] &lt;&lt; endl;    }    return  0;}</code></pre><p>附: 如果没有给定数据范围, 可以使用二维 vector 做 dp 数组</p><pre><code class="lang-cpp">int len1 = s1.length();int len2 = s2.length();vector&lt;vector&lt;int&gt;&gt;dp;dp.resize(len1);for (int i = 0; i &lt; len1; i++)    dp[i].resize(len2);</code></pre><hr><p><span id="T5578"></span></p><h2 id="TK-5578-最大约数和-DP"><a href="#TK-5578-最大约数和-DP" class="headerlink" title="TK 5578 最大约数和 (DP)"></a>TK 5578 最大约数和 (DP)</h2><h3 id="1-题目大意-26"><a href="#1-题目大意-26" class="headerlink" title="1.题目大意"></a>1.题目大意</h3><p>[问题描述] 选取<code>和</code>不超过S的若干个不同的正整数，使得所有数的约数（不含它本身）之和最大. 输入一个正整数S输出最大的约数之和<br>[样例输入]<br>11<br>[样例输出]<br>9<br>[样例说明]<br>取数字4和6，可以得到最大值(1+2)+(1+2+3) = 9</p><h3 id="2-个人代码-4"><a href="#2-个人代码-4" class="headerlink" title="2.个人代码"></a>2.个人代码</h3><pre><code class="lang-cpp">int a[1005], dp[1005];   // dp[i] 表示 &lt;= i 的最大约数和int main(){    int s;    cin &gt;&gt; s;    for (int i = 1; i &lt;= s; ++i)        for (int j = 1; j &lt;= i/2; ++j)            if(i % j == 0) dp[i] += j; // 如果 j 是 i 的约数之一    for (int i = 1; i &lt;= s; ++i) {        for (int j = 1; j &lt;= i/2; ++j) {            dp[i] = max(dp[i], dp[j] + dp[i-j]);  // 每一对不超过 s 的和都计算过        }    }    cout &lt;&lt; dp[s];    return  0;}</code></pre><hr><p><span id="T5579"></span></p><h2 id="TK-5579-环形石子合并-区间-DP"><a href="#TK-5579-环形石子合并-区间-DP" class="headerlink" title="TK 5579 环形石子合并 (区间 DP)"></a>TK 5579 环形石子合并 (区间 DP)</h2><h3 id="1-题目大意-27"><a href="#1-题目大意-27" class="headerlink" title="1.题目大意"></a>1.题目大意</h3><p>石子合并问题的三种类型: 任意合并/相邻合并/环形合并. <a href="#https://blog.csdn.net/qq_40507857/article/details/81266843">详情链接👆</a><br><strong>环形合并</strong>: 在一个<code>圆形</code>操场的四周摆放着 n 堆石子. 现要将石子有次序地合并成一堆. 规定每次只能把<code>相邻</code>的 2 堆石子合并成新的一堆，并将新的一堆石子数记为该次合并的得分.<br>试设计一个算法，计算出将n堆石子合并成一堆的最小得分和最大得分.</p><h3 id="2-关键思路"><a href="#2-关键思路" class="headerlink" title="2.关键思路"></a>2.关键思路</h3><pre><code class="lang-cpp">const int maxn = 210;  // a1...an a1...an 复制为两个int sum[maxn][maxn], min_dp[maxn][maxn], max_dp[maxn][maxn];//sum[i][j] 表示 i 到 j 的石子总和,  dp[i][j] 表示 i 到 j 的总代价</code></pre><p>从 i 到 j 合并的代价/得分为: i 到 j 的总石子数 + i~k 的得分 + k+1~j 的得分<br>对于区间DP，我们通常是一个循环枚举区间的长度，一个循环枚举区间的左端点.<br>遍历方式: 三层遍历, 区间长度 -&gt; 区间起点 -&gt; 区间分割点</p><h3 id="3-个人代码-11"><a href="#3-个人代码-11" class="headerlink" title="3.个人代码"></a>3.个人代码</h3><pre><code class="lang-cpp">int main(){    int n, x, min_sc=inf, max_sc=0;    cin &gt;&gt; n;    fill(min_dp[0], min_dp[0] + maxn*maxn, inf);  // 默认距离为最大, 方便取 min    for (int i = 1; i &lt;= n; ++i) {        cin &gt;&gt; x;        sum[i][i] = sum[i+n][i+n] = x;        min_dp[i][i] = min_dp[i+n][i+n] = 0;    }    for (int len = 1; len &lt; n; ++len) {  //区间长度        for (int i = 1; i &lt;= 2*n &amp;&amp; i+len &lt;= 2*n; ++i) {    //区间起点            int j = i+len;                  //区间终点            for (int k = i; k &lt; j; ++k) {  //区间分割点                sum[i][j] = sum[i][k] + sum[k+1][j];                min_dp[i][j] = min(min_dp[i][j], min_dp[i][k]+min_dp[k+1][j] + sum[i][j]);   //更新最小得分                max_dp[i][j] = max(max_dp[i][j], max_dp[i][k]+max_dp[k+1][j] + sum[i][j]);   //更新最大得分            }            if(len == n-1){                min_sc = min(min_sc, min_dp[i][j]);   //沉淀出 i~j 的最小值                max_sc = max(max_sc, max_dp[i][j]);   //沉淀出 i~j 的最大值            }        }    }    cout &lt;&lt; min_sc &lt;&lt; endl;    cout &lt;&lt; max_sc &lt;&lt; endl;    return  0;}</code></pre><h3 id="3-类似问题"><a href="#3-类似问题" class="headerlink" title="3.类似问题"></a>3.类似问题</h3><p>区间动归: <a href="https://blog.csdn.net/A_Comme_Amour/article/details/78477488" target="_blank" rel="noopener">邮局问题</a><br>树形区间动归: <a href="https://blog.csdn.net/qq_41205417/article/details/95939090" target="_blank" rel="noopener">加分二叉树</a></p><h4 id="乘积最大"><a href="#乘积最大" class="headerlink" title="乘积最大"></a>乘积最大</h4><p>设有一个长度为N的数字串，要求选手使用K个乘号将它分成K+1个部分，找出一种分法，使得这K+1个部分的乘积最大.<br>同时，为了帮助选手能够正确理解题意，主持人还举了如下的一个例子：<br>有一个数字串：312， 当N=3，K=1时会有以下两种分法：<br>1）3<em>12=36<br>2）31</em>2=62<br>这时，符合题目要求的结果是：31*2=62</p><ul><li><p>区间 DP</p><pre><code class="lang-cpp">int dp[50][50], num[50][50], a[50];//num[i][j]表示 i~j 对应的数值, dp[i][k]表示 k 个乘号, 到 i 为止的最大乘积</code></pre><p>关键: 初始化, <code>num[i][i]</code> 就是原来的数字, <code>dp[i][0] = num[1][i]</code> 乘号个数为 0 时, 到 i 位置的最大乘积就是 1~i 表示的数字本身. <code>dp[i][k] = max(dp[i][k], dp[j][k-1]*num[j+1][i])</code>  乘号之前 $\times$ 乘号之后</p><pre><code class="lang-cpp">int main(){  int n, m, k, maxn=-1; cin &gt;&gt; n &gt;&gt; m;  string str; cin &gt;&gt; str;  for (int i = 1; i &lt;= n; i++){      a[i] = str[i-1]-&#39;0&#39;;      num[i][i] = a[i];  }  for (int i = 1; i &lt; n; ++i)      for (int j = i+1; j &lt;= n; ++j)          num[i][j] = num[i][j-1]*10 + a[j];  fill(dp[0], dp[0]+50*50, 0);  for (int i = 1; i &lt;= n; ++i)      dp[i][0] = num[1][i];   //没有乘号的时候, 用于下面 dp[i][k-1] 初始化  for (int k = 1; k &lt;= m; ++k) {            //乘号个数k      for (int i = 1; i &lt;= n; ++i) {        //终点位置, 到 i 为止的最大乘积          for (int j = 1; j &lt; i; ++j) {     //乘号位置j, j 之前的最大乘积dp[j][k-1], j 以后的数字 num[j+1][i]              dp[i][k] = max(dp[i][k], dp[j][k-1]*num[j+1][i]);  //乘号之前 x 乘号之后          }      }  }  cout &lt;&lt; dp[n][m];  return  0;}</code></pre></li></ul><hr><p><span id="NK-ctgc"></span></p><h2 id="NK-畅通工程-并查集-最小生成树-Kruscal-算法"><a href="#NK-畅通工程-并查集-最小生成树-Kruscal-算法" class="headerlink" title="NK-畅通工程 (并查集, 最小生成树 Kruscal 算法)"></a>NK-畅通工程 (并查集, 最小生成树 Kruscal 算法)</h2><h3 id="1-题目大意-28"><a href="#1-题目大意-28" class="headerlink" title="1.题目大意"></a>1.题目大意</h3><p>给定村庄两两之间的距离, 输出所有村庄连通的最小长度.</p><p>每个测试用例的第1行给出评估的道路条数 N、村庄数目M (N, M &lt; =100 )；随后的 N 行对应村庄间道路的成本，每行给出一对正整数，分别是两个村庄的编号，以及此两村庄间道路的成本（也是正整数）. 为简单起见，村庄从1到M编号. 当N为0时，全部输入结束.</p><p>样例:</p><pre><code>3 31 2 11 3 22 3 41 32 3 20 100------3?</code></pre><h3 id="2-分析-13"><a href="#2-分析-13" class="headerlink" title="2.分析"></a>2.分析</h3><p>此题不建议用简单的 dfs 来做, 会很麻烦. 因为每次 dfs 都要重新设置 visit 数组, 要把每一个村庄依次作为 dfs 的起点, 比较最小的长度, 效率很低. 使用 g[a][b]+邻接表+visit 数组的老三样搭配, 不灵了, 而且数组越界.</p><p>此题我们使用一个经典简单的算法—<code>并查集</code>, 核心思想就是使用一个 Father 数组, <code>Father[i] = x</code> 表示 i 的父亲是 x, <code>Father[i] = x</code> 表示 i 是根节点. </p><p>并查集和最小路径问题是天然搭配, 只需每次 Union 操作, 就能表示加入了一条新的边. 必须要保证没有在同一颗树种, 不是同一个 root, 才能进行 Union, 可以保证访问过的不会再访问.</p><p>一次循环遍历所有边, 如果刚好所有点都 Union 到了同一棵树中, 表示图是连通的. 如果还有别的树单独在外, 表示非连通.</p><p>因为给定所有边的信息, 此题的最小生成树问题采用 Kruscal 算法, 直接按边排序, 每次直接找最短的边.</p><h3 id="3-个人代码-12"><a href="#3-个人代码-12" class="headerlink" title="3.个人代码"></a>3.个人代码</h3><pre><code class="lang-cpp">#include &lt;iostream&gt;#include &lt;algorithm&gt;#define N 101using namespace std;int Father[N];struct Path{    int city1;    int city2;    int cost;}buf[N];bool cmp(Path p1,Path p2) { return p1.cost&lt;p2.cost;int findRoot(int x){    if(Father[x]==-1)return x;    else {        int tmp;        tmp = findRoot(Father[x]);        Father[x] = tmp;        return tmp;    }}int main(){    int n,m;    int a,b;    while(scanf(&quot;%d&quot;,&amp;n)!=EOF){        if(n==0)break;        for(int i=0;i&lt;N;i++){            Father[i]=-1;   //全部初始化为 -1. 看做都是孤立的集合        }        scanf(&quot;%d&quot;,&amp;m);        for(int i=0;i&lt;n;i++){            scanf(&quot;%d%d%d&quot;,&amp;buf[i].city1,&amp;buf[i].city2,&amp;buf[i].cost);        }        sort(buf,buf+n,cmp);  //从最短的路开始找        int TotalCost = 0;        for(int i=0;i&lt;n;i++){            int rootA = findRoot(buf[i].city1);            int rootB = findRoot(buf[i].city2);            if(rootA!=rootB){//若目前不在一个集合内(不是同一个父亲) 可以保证访问过的不会再访问)                Father[rootA] = rootB;//把集合A放入B中 Union 操作                TotalCost+=buf[i].cost;            }        }        int cnt=0;        for(int i=1;i&lt;=m;i++){            if(Father[i]==-1){   //统计根节点的数量                cnt++;            }        }        if(cnt&lt;=1)            printf(&quot;%d\n&quot;,TotalCost);        else//无法构成最小生成树            printf(&quot;?\n&quot;);    }    return 0;}</code></pre><h3 id="4-补充"><a href="#4-补充" class="headerlink" title="4.补充"></a>4.补充</h3><p>本题的并查集方法, 也可以用来统计连通分量的个数, 只需最后 <code>if(tree[i]==-1) ans++;</code> 计算树的个数即可.</p><hr><p><span id="NK-jzzh"></span></p><h2 id="NK-进制转换-10进制-VS-2进制"><a href="#NK-进制转换-10进制-VS-2进制" class="headerlink" title="NK-进制转换 10进制 VS 2进制"></a>NK-进制转换 10进制 VS 2进制</h2><p>(任意长度字符串进制转换: 可取用函数)</p><h3 id="1-题目大意-29"><a href="#1-题目大意-29" class="headerlink" title="1.题目大意"></a>1.题目大意</h3><p>对于一个十进制数A，将A转换为二进制数，然后按位逆序排列，再转换为十进制数B，我们称B为A的二进制逆序数.<br>例如对于十进制数173，它的二进制形式为10101101，逆序排列得到10110101，其十进制数为181，181即为173的二进制逆序数.</p><h3 id="2-个人代码-5"><a href="#2-个人代码-5" class="headerlink" title="2.个人代码"></a>2.个人代码</h3><p>真正无惧溢出的进制转换. 在字符串上直接进行除法, 无需 int 存储.<br>无惧数组越界, 不需要设定字符数组长度.</p><pre><code class="lang-cpp">string conversion(int m, string a, int n){  //数字字符串 a 从 m 进制转换为 n 进制    int len = a.length(), last_rem = 0;    string res;    for (int i = 0; i &lt; len;) {           //本循环功能: 做字符串除法, 每次除以 n        last_rem = 0;   //开始了新一轮计算, 余数只是为了添加到结果中, 每次一定要重新置 0        for (int j = i; j &lt; len; ++j) {   //每次从 不为 0 的最高位开始往后            int cur_rem = (a[j] - &#39;0&#39; + last_rem * m) % n; //求余数, 供下一次使用            a[j] = (a[j] - &#39;0&#39; + last_rem * m) / n + &#39;0&#39;;  //替换掉 a[j]            last_rem = cur_rem;        }        res = char(last_rem + &#39;0&#39;) + res;        while (a[i] == &#39;0&#39;) i++;   //跳过高位的 0, 即 086 -&gt; 86 直接计算    }    return res;}</code></pre><hr><p><span id="dp-set"></span></p><h2 id="动态规划常见问题合集"><a href="#动态规划常见问题合集" class="headerlink" title="动态规划常见问题合集"></a>动态规划常见问题合集</h2><pre><code class="lang-c++">(1)最长递增子序列原理：f[1]=1f[i]=max(1,f[j]+1)(j&lt;i,v[j]&lt;v[i])f[i]表示递增子序列以v[i]结束时它的最长长度#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;algorithm&gt;using namespace std;int main() {    vector&lt;int&gt;v{ 0,2,3,4,4,3,2,5,34,4,6,123 };    vector&lt;int&gt;num;    num.resize(v.size());    num[0] = 1;    for (int i = 2; i &lt; v.size(); i++) {        vector&lt;int&gt;n;        n.push_back(1);        for (int j = 0; j &lt; i; j++)            if (v[j] &lt;= v[i])                n.push_back(num[j] + 1);        num[i] = *max_element(n.begin(), n.end());    }    cout &lt;&lt; *max_element(num.begin(), num.end())&lt;&lt;endl;    return 0;}(2)最长公共子序列原理：dp[i][j]表示s1前i个字符和s2前j个字符组成的两个前缀字符串的最长公共子串长度dp[0][i]=0(0=&lt;i&lt;=m)dp[j][0]=0(0=&lt;j&lt;=n)dp[i][j]=dp[i-1][j-1]+1 (s[i]==s[j])dp[i][j]=max{dp[i-1][j],dp[i][j-1]} (s[i]!=s[j])    #include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;string&gt;#include&lt;algorithm&gt;using namespace std;int main() {    string s1 = &quot;asdfuuu&quot;;    string s2 = &quot;dfummm&quot;;    vector&lt;vector&lt;int&gt;&gt;dp;    int m = s1.length();    int n = s2.length();    dp.resize(m + 1);    for (int i = 0; i &lt;= m; i++)        dp[i].resize(n + 1);    for (int i = 0; i &lt;= m; i++)        dp[i][0] = 0;    for (int j = 0; j &lt;= n; j++)        dp[0][j] = 0;    for (int i = 1; i &lt;= m; i++)        for (int j = 1; j &lt;= n; j++)            if (s1[i] == s2[j])                dp[i][j] = dp[i - 1][j - 1] + 1;            else                dp[i][j] = max(dp[i - 1][j], dp[i][j - 1]);    cout &lt;&lt; dp[m][n] &lt;&lt; endl;    return 0;}(3)最长连续公共子序列原理：dp[i][j]表示以s1[i]，s2[j]为结尾的字符串的公共长度#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;string&gt;#include&lt;algorithm&gt;using namespace std;int main() {    string s1 = &quot;asdfuuuu&quot;;    string s2 = &quot;dfuummm&quot;;    vector&lt;vector&lt;int&gt;&gt;dp;    int m = s1.length();    int n = s2.length();    dp.resize(m + 1);    for (int i = 0; i &lt;= m; i++)        dp[i].resize(n + 1);    for (int i = 0; i &lt;= m; i++)        dp[i][0] = 0;    for (int j = 0; j &lt;= n; j++)        dp[0][j] = 0;    for (int i = 1; i &lt;= m; i++)        for (int j = 1; j &lt;= n; j++)            if (s1[i-1] == s2[j-1])                dp[i][j] = dp[i - 1][j - 1] + 1;    int max = -1;    for (int i = 1; i &lt;= m; i++)        for (int j = 1; j &lt;= n; j++)            if (dp[i][j] &gt; max)                max = dp[i][j];    cout &lt;&lt; max &lt;&lt; endl;    return 0;}(4)最大连续子序列的和原理：dp[i]表示以第i个数字为结尾的连续子序列的和dp[i]=max{dp[i-1]+a[i],a[i]}示例程序:#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;algorithm&gt;using namespace std;int main() {    vector&lt;int&gt;v{ 1,2,3,-5,3,5,2,-7 };    vector&lt;int&gt;sum;    sum.resize(8);    sum[0] = v[0];    for (int i = 1; i &lt; v.size(); i++)        sum[i] = max(sum[i - 1] + v[i], v[i]);    cout &lt;&lt; *max_element(sum.begin(), sum.end());    return 0;}(5)最大子序列和int findMaxSum(vector&lt;int&gt;v){    int sum=0;    int maxSum=0;    for(int i=0;i&lt;v.size();i++){        sum+=v[i];        if(sum&gt;maxSum)            maxSum=sum;        else if(sum&lt;0)            sum=0;    }    cout&lt;&lt;maxSum&lt;&lt;endl;}(7)最大子序列和，但是有一个限定最大值，就转换为动态规划问题中的背包问题对于每个变量来说，它的value同时也是它的costdp[i][j] = max(dp[i - 1][j], dp[i - 1][j - intValue[i]] + intValue[i]);当遇到背包中的值为double类型的时候，可以将其乘以10的次方，转换为整数(6)0-1背包问题要注意vector&lt;vector&lt;int&gt;&gt;dp的时候不一定是0！！！，必须要初始化原理：dp[i][j]表示总体积不超过j的条件下，前i件物品可以达到的最大价值点菜问题：#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;algorithm&gt;using namespace std;class Node {public:    int value;    int cost;};int main() {    vector&lt;Node&gt;v;    vector&lt;vector&lt;int&gt;&gt;dp;    int n;    int cost;    while (cin &gt;&gt;cost&gt;&gt; n) {        v.clear();        dp.clear();        v.resize(0);        dp.resize(0);        v.resize(n + 1);        for (int i = 1; i &lt;= n; i++)            cin &gt;&gt; v[i].cost &gt;&gt; v[i].value;        dp.resize(n + 1);        for (int i = 0; i &lt;= n; i++)            dp[i].resize(cost+1);        //这里当时调试了几次，由于将cost写为n的原因，初始化过程必不可少        for (int i = 0; i &lt;= cost; i++)dp[0][i] = 0;        for (int i = 1; i &lt;= n; i++) {            for (int j = cost; j &gt;= v[i].cost; j--)                //dp[i][j]表示在总体积不超过j的情况下，前i个物体可以达到的最大价值                dp[i][j] = max(dp[i - 1][j - v[i].cost] + v[i].value, dp[i - 1][j]);            for (int j = v[i].cost - 1; j &gt;= 0; j--)                dp[i][j] = dp[i - 1][j];        }        cout &lt;&lt; dp[n][cost] &lt;&lt; endl;    }    return 0;}</code></pre><hr><p><span id="NK-dcth"></span></p><h2 id="NK-单词替换"><a href="#NK-单词替换" class="headerlink" title="NK-单词替换"></a>NK-单词替换</h2><ol><li><p>题目大意</p><pre><code>You want someone to help youYouI</code></pre><pre><code>I want someone to help you</code></pre></li><li><p>核心代码<br>手写 split 函数, 省去字符串匹配的麻烦</p><pre><code class="lang-C++">void split(string s) { v.clear(); v.resize(0); int index = s.find(&#39; &#39;); while(index!=-1){     v.push_back(s.substr(0, index));     s = s.substr(index+1, s.length() - index - 1);     index = s.find(&#39; &#39;); } v.push_back(s);}</code></pre></li></ol><hr><p><span id="NK-graph"></span></p><h2 id="图相关算法"><a href="#图相关算法" class="headerlink" title="图相关算法"></a>图相关算法</h2><pre><code class="lang-c++">（1）计算联通子图的数目#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;algorithm&gt;using namespace std;/*    给出城镇标号，表示这两个城镇是联通的    求出需要再建多少条路才能使得所有城镇联通*/vector&lt;int&gt;tree;    //用来包含每个结点的父结点int findRoot(int x) {   //寻找根节点    if (tree[x] == -1)        return x;    else {        int tmp = findRoot(tree[x]);        tree[x] = tmp;        return tmp;    }}int main() {    int n, e;    while (cin &gt;&gt; n &gt;&gt; e) {        tree.clear();        tree.resize(n + 1);        for (int i = 1; i &lt;= n; i++)            tree[i] = -1;        int a, b;        for (int i = 1; i &lt;= e; i++) {            cin &gt;&gt; a &gt;&gt; b;            a = findRoot(a);            b = findRoot(b);            if (a != b)                tree[a] = b;        }        int ans = 0;        for (int i = 1; i &lt;= n; i++)            if (tree[i] == -1)                ans++;        cout &lt;&lt; ans - 1 &lt;&lt; endl;    }    return 0;}（2）求出具有最多元素的联通子图的结点数量//思路：添加一个sum数组，sum[i]表示以第i个结点为根结点的树的结点数目#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;algorithm&gt;using namespace std;vector&lt;int&gt;tree;    //用来包含每个结点的父结点vector&lt;int&gt;sum;     //用来表示以sum[i]为根节点的树中结点数目int findRoot(int x) {   //寻找根节点    if (tree[x] == -1)        return x;    else {        int tmp = findRoot(tree[x]);        tree[x] = tmp;        return tmp;    }}int main() {    int n;    while (cin &gt;&gt; n ) {        tree.clear();        tree.resize(101);        sum.clear();        sum.resize(101);        for (int i = 1; i &lt;= 100; i++) {            tree[i] = -1;            sum[i] = 1;        }        int a, b;        for (int i = 1; i &lt;= n; i++) {            cin &gt;&gt; a &gt;&gt; b;            a = findRoot(a);            b = findRoot(b);            if (a != b) {                tree[a] = b;                sum[b] += sum[a];            }        }        int max = 0;        for (int i = 1; i &lt;= 100; i++)            if (sum[i] &gt; max)                max = sum[i];        cout &lt;&lt; max &lt;&lt; endl;    }    return 0;}（3）Kruskal算法，最小生成树#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;algorithm&gt;using namespace std;/*    题目：输入结点和边的长度，输出最小生成树的长度    注意：要判断是否能够构成一个联通图*/vector&lt;int&gt;tree;    //用来包含每个结点的父结点int findRoot(int x) {   //寻找根节点    if (tree[x] == -1)        return x;    else {        int tmp = findRoot(tree[x]);        tree[x] = tmp;        return tmp;    }}class Edge {public:    int a, b;    int length;    bool operator&lt;(Edge&amp; e) {        return length &lt; e.length;    }};int main() {    int n;    while (cin &gt;&gt; n) {        if (n == 0)            break;        tree.resize(n + 1);        vector&lt;Edge&gt;e;        e.resize(n*(n - 1) / 2);        for (int i = 1; i &lt;= n; i++)            tree[i] = -1;        for (int i = 0; i &lt; e.size(); i++)            cin &gt;&gt; e[i].a &gt;&gt; e[i].b &gt;&gt; e[i].length;        sort(e.begin(), e.end());        int ans = 0;        for (int i = 0; i &lt; e.size(); i++) {            int a = findRoot(e[i].a);            int b = findRoot(e[i].b);            if (a != b) {                tree[a] = b;                ans += e[i].length;            }        }        int num = 0;    //判断根节点的数目，如果不为1说明这个图是不连通的        for (int i = 1; i &lt; tree.size(); i++)            if (tree[i] == -1)                num++;        if (num != 1)            cout &lt;&lt; &quot;图是不连通的&quot; &lt;&lt; endl;        else            cout &lt;&lt; ans &lt;&lt; endl;    }    return 0;}（4）判断一个图是否存在环/*    原理：一个拓扑排序，统计每个结点的入度，每次找出来一个入度为0的结点，然后它的所有子结点        的入度减1，然后加一个标志位，防止被访问过的结点重复被提取出来*/#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;queue&gt;using namespace std;class Node {public:    int degree;    bool taken;    vector&lt;int&gt;next;    Node() {        taken = false;        degree = 0;    }};int main() {    int n, m;    while (cin &gt;&gt; n &gt;&gt; m) {        vector&lt;Node&gt;v;        v.resize(n);        int s, e;        for (int i = 1; i &lt;= m; i++) {            cin &gt;&gt; s &gt;&gt; e;            v[s].next.push_back(e);            v[e].degree++;        }        queue&lt;int&gt;q;        int ans = 0;        for (int i = 0; i &lt; n; i++)            if (v[i].degree == 0)                q.push(i);        while (!q.empty()) {            int index = q.front();            v[index].taken = true;            q.pop();            ans++;            for (int i : v[index].next)                v[i].degree--;            for (int i = 0; i &lt; n; i++)                if (v[i].degree == 0 &amp;&amp; v[i].taken == false)                    q.push(i);        }        if (ans != n)            cout &lt;&lt; &quot;图存在环&quot; &lt;&lt; endl;        else            cout &lt;&lt; &quot;图不存在环&quot; &lt;&lt; endl;    }    return 0;}</code></pre><hr><p><span id="NK-dzs"></span></p><h2 id="C-大整数-乘法"><a href="#C-大整数-乘法" class="headerlink" title="C++大整数 - 乘法"></a>C++大整数 - 乘法</h2><h3 id="1-题目描述"><a href="#1-题目描述" class="headerlink" title="1.题目描述"></a>1.题目描述</h3><p>输入一个正整数N，输出N的阶乘。<br>输入描述:<br>正整数N(0&lt;=N&lt;=1000)<br>输出描述:<br>输入可能包括多组数据，对于每一组输入数据，输出N的阶乘</p><h3 id="2-题解代码"><a href="#2-题解代码" class="headerlink" title="2.题解代码"></a>2.题解代码</h3><pre><code class="lang-c++">using namespace std;class BigInteger {public:    vector&lt;int&gt;v;    void set(int x) {        v.clear();        v.resize(0);        do {            v.push_back(x%10000);            x /= 10000;        } while (x != 0);    }    void output() {        for (int i = v.size() - 1; i &gt;= 0; i--) {            if (i != v.size() - 1)                cout &lt;&lt; setw(4) &lt;&lt; setfill(&#39;0&#39;) &lt;&lt; v[i];            else                cout &lt;&lt; v[i];        }        cout &lt;&lt; endl;    }    BigInteger operator*(int x) const{        int jin = 0;        BigInteger res;        for (int i = 0; i &lt; v.size(); i++) {            res.v.push_back((x*v[i] + jin) % 10000);            jin = (x*v[i] + jin) / 10000;        }        if (jin &gt; 0)            res.v.push_back(jin);        return res;    }};int main() {    int N;    while (cin &gt;&gt; N) {        BigInteger a;        a.set(1);        for (int i = 1; i &lt;= N; i++)            a =a* i;        a.output();    }    return 0;}</code></pre><h2 id="C-大整数-大整数的因子"><a href="#C-大整数-大整数的因子" class="headerlink" title="C++大整数 - 大整数的因子"></a>C++大整数 - 大整数的因子</h2><h3 id="1-题目描述-1"><a href="#1-题目描述-1" class="headerlink" title="1.题目描述"></a>1.题目描述</h3><p>已知正整数k满足2&lt;=k&lt;=9，现给出长度最大为30位的十进制非负整数c，求所有能整除c的k.</p><h3 id="2-题解代码-1"><a href="#2-题解代码-1" class="headerlink" title="2.题解代码"></a>2.题解代码</h3><pre><code class="lang-c++">while (cin &gt;&gt; s) {    if (s == &quot;-1&quot;)continue;    vector&lt;int&gt;v;    int flag;    for (int k = 2; k &lt;= 9; k++) {   //对每个因子整除        flag = 0;        for (int i = 0; i &lt; s.length(); i++) {   //模拟大整数的除法, 从高位开始            flag *= 10;            if (flag + (s[i] - &#39;0&#39;) &lt; k)                flag += (s[i] - &#39;0&#39;);            else                flag = (flag + (s[i] - &#39;0&#39;)) % k;        }        if (flag == 0)v.push_back(k);   //flag 为 0 才表示除尽    }}</code></pre><hr><p><span id="NK-sxzs"></span></p><h2 id="扇形着色问题"><a href="#扇形着色问题" class="headerlink" title="扇形着色问题"></a>扇形着色问题</h2><p>将一个圆形分成n个扇形，现有m种涂料，现在用这m种涂料对扇形进行着色，要求相邻两个扇形的颜色不能相同</p><p>思路：<br>    第一个m种可能，第二个m-1种可能，第n个m-1种可能，所以一共就是 <code>m(m-1)^(n-1)</code>，但是由于第n个可能跟第一个进行重合，所以就需要减去这种情况，可以视作是第n个与第一个构成一个扇区进行着色，所以就是 <code>A(n-1,m)</code><br>    所以 <code>A(n,m)=m*(m-1)^(n-1)+A(n-1,m)</code></p><p>编程：</p><pre><code class="lang-cpp">    int count(n,m){        if(n==0||m==0)            return 0;        if(n==1)            return m;        vector&lt;int&gt;v;        v.resize(n+1);        v[1]=m;        v[2]=m*(m-1);        for(int i=3;i&lt;=n;i++)            v[i]=int(m*pow(m-1,i-1)-v[i-1]);        cout&lt;&lt;v[n]&lt;&lt;endl;    }</code></pre>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PAT </tag>
            
            <tag> OJ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>欢迎来到 Hexo</title>
      <link href="/2020/02/02/hello-world/"/>
      <url>/2020/02/02/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><p>本博客采用 Hexo 快速搭建，也欢迎大家上手使用 Hexo，选择自己喜欢的主题，省时省力，工具齐全，生态优越，可扩展性好</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre><code class="lang-bash">$ hexo new &quot;My New Post&quot;</code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre><code class="lang-bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre><code class="lang-bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre><code class="lang-bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
