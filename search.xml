<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>动手深度学习(十一):现代 CNN 模型</title>
      <link href="/2020/02/20/dl-notes11-modern-cnn/"/>
      <url>/2020/02/20/dl-notes11-modern-cnn/</url>
      
        <content type="html"><![CDATA[<h1 id="深度卷积神经网络"><a href="#深度卷积神经网络" class="headerlink" title="深度卷积神经网络"></a>深度卷积神经网络</h1><p>LeNet:  在大的真实数据集上的表现并不尽如⼈意。<br>1.神经网络计算复杂。<br>2.还没有⼤量深⼊研究参数初始化和⾮凸优化算法等诸多领域。  </p><p>机器学习的特征提取:手工定义的特征提取函数<br>神经网络的特征提取：通过学习得到数据的多级表征，并逐级表⽰越来越抽象的概念或模式。  </p><p>神经网络发展的限制:数据、硬件</p><h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><p>首次证明了学习到的特征可以超越⼿⼯设计的特征，从而⼀举打破计算机视觉研究的前状。<br><strong>特征：</strong></p><ol><li>8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层。</li><li>将sigmoid激活函数改成了更加简单的ReLU激活函数。<br> ReLu函数的好处：缓解梯度消失，导数计算更快，收敛快，负数的时候梯度为 0，有正则化，稀疏化的作用</li><li>用Dropout来控制全连接层的模型复杂度。<br> Dropout 解决过拟合，不会过度依赖某个神经元</li><li>引入数据增强，如翻转、裁剪和颜色变化，从而进一步扩大数据集来缓解过拟合。</li></ol><p>更多的通道数，具有更多的特征</p><p>计算示例（第一层）：(224-11)/4 + 1 = 54 这一层输出的宽和高就是 54，通道数是 96</p><p>下一层 (54-3)/2 +1 = 26</p><p>下一层 (26+4-5)/1 + 1 =26<br>…</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/d9UtZ2CpjrIbiyV.png" alt="计算公式"></p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/upload/image/q5kv4gpx88.png?imageView2/0/w/640/h/640" alt="LeNet 和 AlexNet 对比"></p><pre><code class="lang-python">#考虑到本代码中的模型过大，CPU 训练较慢，需要 GPU#我们还将代码上传了一份到 https://www.kaggle.com/boyuai/boyu-d2l-modernconvolutionalnetwork#如希望提前使用gpu运行请至kaggle。import timeimport torchfrom torch import nn, optimimport torchvisionimport numpy as npimport syssys.path.append(&quot;/home/kesci/input/&quot;) import d2lzh1981 as d2limport osimport torch.nn.functional as Fdevice = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)class AlexNet(nn.Module):    def __init__(self):        super(AlexNet, self).__init__()        self.conv = nn.Sequential(            nn.Conv2d(1, 96, 11, 4), # in_channels, out_channels, kernel_size, stride, padding            nn.ReLU(),            nn.MaxPool2d(3, 2), # kernel_size, stride            # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数            nn.Conv2d(96, 256, 5, 1, 2),            nn.ReLU(),            nn.MaxPool2d(3, 2),            # 连续3个卷积层，且使用更小的卷积窗口。除了最后的卷积层外，进一步增大了输出通道数。            # 前两个卷积层后不使用池化层来减小输入的高和宽            nn.Conv2d(256, 384, 3, 1, 1),            nn.ReLU(),            nn.Conv2d(384, 384, 3, 1, 1),            nn.ReLU(),            nn.Conv2d(384, 256, 3, 1, 1),            nn.ReLU(),            nn.MaxPool2d(3, 2)        )         # 这里全连接层的输出个数比LeNet中的大数倍。使用丢弃层来缓解过拟合        self.fc = nn.Sequential(            nn.Linear(256*5*5, 4096),            nn.ReLU(),            nn.Dropout(0.5),            #由于使用CPU镜像，精简网络，若为GPU镜像可添加该层            #nn.Linear(4096, 4096),            #nn.ReLU(),            #nn.Dropout(0.5),            # 输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000            nn.Linear(4096, 10),        )    def forward(self, img):  # 把 self.conv 和 self.fc 连接起来        feature = self.conv(img)  # 输出 4 维：batch_size * channels * h * w        output = self.fc(feature.view(img.shape[0], -1))  # 变成 2 维：batch_size * hiddens        return output</code></pre><pre><code class="lang-python">net = AlexNet()print(net)</code></pre><pre><code>AlexNet(  (conv): Sequential(    (0): Conv2d(1, 96, kernel_size=(11, 11), stride=(4, 4))    (1): ReLU()    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)    (3): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))    (4): ReLU()    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)    (6): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (7): ReLU()    (8): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (9): ReLU()    (10): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (11): ReLU()    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)  )  (fc): Sequential(    (0): Linear(in_features=6400, out_features=4096, bias=True)    (1): ReLU()    (2): Dropout(p=0.5, inplace=False)    (3): Linear(in_features=4096, out_features=10, bias=True)  ))</code></pre><h3 id="载入数据集"><a href="#载入数据集" class="headerlink" title="载入数据集"></a>载入数据集</h3><pre><code class="lang-python"># 本函数已保存在d2lzh_pytorch包中方便以后使用def load_data_fashion_mnist(batch_size, resize=None, root=&#39;/home/kesci/input/FashionMNIST2065&#39;):    &quot;&quot;&quot;Download the fashion mnist dataset and then load into memory.&quot;&quot;&quot;    trans = []    if resize:        trans.append(torchvision.transforms.Resize(size=resize))    trans.append(torchvision.transforms.ToTensor())  # 变成 tensor 才能训练    transform = torchvision.transforms.Compose(trans)    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=True, download=True, transform=transform)    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=False, download=True, transform=transform)    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=2)    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=2)    return train_iter, test_iter#batchsize=128batch_size = 16# 如出现“out of memory”的报错信息，可减小batch_size或resizetrain_iter, test_iter = load_data_fashion_mnist(batch_size,224)for X, Y in train_iter:    print(&#39;X =&#39;, X.shape,        &#39;\nY =&#39;, Y.type(torch.int32)) # Y 就是标签    break</code></pre><pre><code>X = torch.Size([16, 1, 224, 224]) Y = tensor([2, 4, 5, 2, 3, 2, 7, 7, 7, 3, 5, 7, 8, 2, 6, 3], dtype=torch.int32)</code></pre><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><pre><code class="lang-python">lr, num_epochs = 0.001, 3optimizer = torch.optim.Adam(net.parameters(), lr=lr)d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</code></pre><pre><code>training on  cpuepoch 1, loss 0.4997, train acc 0.812, test acc 0.865, time 3995.5 sec</code></pre><p>cpu 特别慢，明显不如 cuda(gpu)</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/U1pLjfKmEd8IAnu.png" alt="AlexNet 结果"></p><h2 id="使用重复元素的网络（VGG）"><a href="#使用重复元素的网络（VGG）" class="headerlink" title="使用重复元素的网络（VGG）"></a>使用重复元素的网络（VGG）</h2><p>AlexNet：比较死板，很难扩展<br>VGG：通过重复使⽤简单的基础块来构建深度模型。（不同的 VGG 结构是一样的，但参数可能不一样）<br>Block:数个相同的填充为1、窗口形状为$3\times 3$的卷积层,接上一个步幅为2、窗口形状为$2\times 2$的最大池化层（图中写错）<br>卷积层保持输入的高和宽不变，而池化层则对其减半。</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/upload/image/q5l6vut7h1.png?imageView2/0/w/640/h/640" alt=""></p><h3 id="VGG11的简单实现"><a href="#VGG11的简单实现" class="headerlink" title="VGG11的简单实现"></a>VGG11的简单实现</h3><pre><code class="lang-python">def vgg_block(num_convs, in_channels, out_channels): #卷积层个数，输入通道数，输出通道数    blk = []    for i in range(num_convs):        if i == 0:            blk.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))        else:            blk.append(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)) # 之后通道数都不变        blk.append(nn.ReLU())    blk.append(nn.MaxPool2d(kernel_size=2, stride=2)) # 这里会使宽高减半    return nn.Sequential(*blk)  # 对 block 内进行组合</code></pre><pre><code class="lang-python">conv_arch = ((1, 1, 64), (1, 64, 128), (2, 128, 256), (2, 256, 512), (2, 512, 512)) # 每一层的 channel h w# 经过5个vgg_block, 宽高会减半5次, 变成 224/32 = 7，最后变成 7 * 7 * 512fc_features = 512 * 7 * 7 # c * w * hfc_hidden_units = 4096 # 任意</code></pre><pre><code class="lang-python">def vgg(conv_arch, fc_features, fc_hidden_units=4096): # 把多个 block 组合起来    net = nn.Sequential()    # 卷积层部分    for i, (num_convs, in_channels, out_channels) in enumerate(conv_arch):        # 每经过一个vgg_block都会使宽高减半        net.add_module(&quot;vgg_block_&quot; + str(i+1), vgg_block(num_convs, in_channels, out_channels))    # 全连接层部分    net.add_module(&quot;fc&quot;, nn.Sequential(d2l.FlattenLayer(),                                 nn.Linear(fc_features, fc_hidden_units),                                 nn.ReLU(),                                 nn.Dropout(0.5),                                 nn.Linear(fc_hidden_units, fc_hidden_units),                                 nn.ReLU(),                                 nn.Dropout(0.5),                                 nn.Linear(fc_hidden_units, 10)  # 两个隐藏层，一个输出层                                ))    return net</code></pre><pre><code class="lang-python">net = vgg(conv_arch, fc_features, fc_hidden_units)X = torch.rand(1, 1, 224, 224)# named_children获取一级子模块及其名字(named_modules会返回所有子模块,包括子模块的子模块)for name, blk in net.named_children():     X = blk(X)    print(name, &#39;output shape: &#39;, X.shape)</code></pre><pre><code>vgg_block_1 output shape:  torch.Size([1, 64, 112, 112])vgg_block_2 output shape:  torch.Size([1, 128, 56, 56])vgg_block_3 output shape:  torch.Size([1, 256, 28, 28])vgg_block_4 output shape:  torch.Size([1, 512, 14, 14])vgg_block_5 output shape:  torch.Size([1, 512, 7, 7])fc output shape:  torch.Size([1, 10])</code></pre><pre><code class="lang-python">ratio = 8  # Fashion数据集不用太多参数，采用小一点的模型，也防止过拟合small_conv_arch = [(1, 1, 64//ratio), (1, 64//ratio, 128//ratio), (2, 128//ratio, 256//ratio),                    (2, 256//ratio, 512//ratio), (2, 512//ratio, 512//ratio)]net = vgg(small_conv_arch, fc_features // ratio, fc_hidden_units // ratio)print(net)</code></pre><pre><code>Sequential(  (vgg_block_1): Sequential(    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (1): ReLU()    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)  )  (vgg_block_2): Sequential(    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (1): ReLU()    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)  )  (vgg_block_3): Sequential(    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (1): ReLU()    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (3): ReLU()    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)  )  (vgg_block_4): Sequential(    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (1): ReLU()    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (3): ReLU()    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)  )  (vgg_block_5): Sequential(    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (1): ReLU()    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (3): ReLU()    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)  )  (fc): Sequential(    (0): FlattenLayer()    (1): Linear(in_features=3136, out_features=512, bias=True)    (2): ReLU()    (3): Dropout(p=0.5, inplace=False)    (4): Linear(in_features=512, out_features=512, bias=True)    (5): ReLU()    (6): Dropout(p=0.5, inplace=False)    (7): Linear(in_features=512, out_features=10, bias=True)  ))</code></pre><pre><code class="lang-python">batchsize=16#batch_size = 64# 如出现“out of memory”的报错信息，可减小batch_size或resize# train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)lr, num_epochs = 0.001, 5optimizer = torch.optim.Adam(net.parameters(), lr=lr)d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</code></pre><h2 id="⽹络中的⽹络（NiN）"><a href="#⽹络中的⽹络（NiN）" class="headerlink" title="⽹络中的⽹络（NiN）"></a>⽹络中的⽹络（NiN）</h2><p>LeNet、AlexNet和VGG：先以由卷积层构成的模块充分抽取空间特征，再以由<code>全连接层</code>构成的模块来输出分类结果。 </p><p>NiN：</p><ul><li><code>串联</code>多个由卷积层和“全连接”层构成的小⽹络来构建⼀个深层⽹络。（小模块组装起来）</li><li>⽤了<code>输出通道数</code>等于<code>标签类别数</code>的NiN块，通过调节最后一个模块的通道数，来使最后的输出等于类别数</li><li>然后使⽤<code>全局平均池化层</code> GlobalAvgPool 对每个通道中所有元素求平均并直接⽤于分类，可以减少模型参数，缓解过拟合，但也会造成模型训练时间增加</li></ul><h2 id="卷积层与全连接层-对比分析"><a href="#卷积层与全连接层-对比分析" class="headerlink" title="卷积层与全连接层:对比分析"></a>卷积层与全连接层:对比分析</h2><p>多换一个数据集的话（主要是size的变化），NiN block仅需要改变第一个Convolution的大小，而后续的1x1的Convolution保持不变</p><p>卷积和全连接的一个重要的差别，卷积其实<code>不关心特征图的H和W</code>.<br>如果你从10分类shape224<em>224</em>3的彩色数据集，切换到一个2分类shape160<em>160</em>1的灰度图数据集.<br>只需要替换第一个卷积的input_channel 和最后一个卷积或者全连接层的output_channel.<br>而如果你是从彩色数据集换成了彩色数据集，第一个卷积甚至都不需要发生变化.</p><h2 id="理解卷积工作原理"><a href="#理解卷积工作原理" class="headerlink" title="理解卷积工作原理"></a>理解卷积工作原理</h2><ol><li><p>卷积神经网络通过使用<code>滑动窗口</code>在输入的不同位置处重复计算，减小参数数量。连接卷积层块和全连接层块时，需要做一次展平操作。</p></li><li><p>在通过卷积层或池化层后，输出的高和宽可能减小，为了尽可能<code>保留输入的特征</code>，我们可以在减小高宽的同时增加<code>通道数</code>。</p></li><li><p>卷积的工作流程是，以一定的卷积核大小，一定的步长，在特征图的不同位置，用<code>同一个卷积核</code>来进行互相关运算。</p></li><li><p>这就好像是，一个卷积核是用来提取某一种<code>局部特征</code>的，它在图像中的不同位置来寻找是否有符合它所关心的特征的局部区域。</p></li><li><p>这种工作机制导致了图像的<code>尺寸（宽和高）并不影响</code>卷积运算，只有通道数的变化才会影响。可以和权值共享的概念联系一下理解。</p></li><li><p>1x1 conv除了改变channel数，还有增加非线性的作用，一般都会跟着一个<code>激活层</code>，其他感觉比较玄学设计，CV里可能跟ReLU可以做到单侧抑制增加稀疏有关吧。</p></li></ol><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/upload/image/q5l6u1p5vy.png?imageView2/0/w/960/h/960" alt=""></p><ul><li><p><code>1×1卷积核作用</code></p><p>  1.放缩通道数：通过控制卷积核的数量达到通道数的放缩，放大 or 缩小<br>  2.增加非线性。1×1卷积核的卷积过程<code>相当于全连接层的计算过程</code>，并且还加入了非线性激活函数，从而可以增加网络的非线性。<br>  3.计算参数少   </p></li></ul><pre><code class="lang-python">def nin_block(in_channels, out_channels, kernel_size, stride, padding):  # 可复用的 block    blk = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),                        nn.ReLU(),                        nn.Conv2d(out_channels, out_channels, kernel_size=1), # 后面两个 channel 不变                        nn.ReLU(),                        nn.Conv2d(out_channels, out_channels, kernel_size=1), # kernel_size=1 不改变形状                         nn.ReLU())    return blk</code></pre><pre><code class="lang-python"># 已保存在d2lzh_pytorchclass GlobalAvgPool2d(nn.Module):    # 全局平均池化层可通过将池化窗口形状设置成输入的高和宽实现    def __init__(self):        super(GlobalAvgPool2d, self).__init__()    def forward(self, x):        return F.avg_pool2d(x, kernel_size=x.size()[2:]) # 对每个通道的整个 feature_map 取平均，等于 x.size()即可net = nn.Sequential(    nin_block(1, 96, kernel_size=11, stride=4, padding=0),    nn.MaxPool2d(kernel_size=3, stride=2),    nin_block(96, 256, kernel_size=5, stride=1, padding=2),    nn.MaxPool2d(kernel_size=3, stride=2),    nin_block(256, 384, kernel_size=3, stride=1, padding=1),    nn.MaxPool2d(kernel_size=3, stride=2),     nn.Dropout(0.5),    # 标签类别数是10    nin_block(384, 10, kernel_size=3, stride=1, padding=1),  # 输出为 10，=类别数    GlobalAvgPool2d(),  # 形状是（批量大小，10，1，1）    # 将四维的输出转成二维的输出，其形状为(批量大小, 10)    d2l.FlattenLayer())</code></pre><pre><code class="lang-python">X = torch.rand(1, 1, 224, 224)for name, blk in net.named_children():     X = blk(X)    print(name, &#39;output shape: &#39;, X.shape)</code></pre><pre><code>0 output shape:  torch.Size([1, 96, 54, 54])1 output shape:  torch.Size([1, 96, 26, 26])2 output shape:  torch.Size([1, 256, 26, 26])3 output shape:  torch.Size([1, 256, 12, 12])4 output shape:  torch.Size([1, 384, 12, 12])5 output shape:  torch.Size([1, 384, 5, 5])6 output shape:  torch.Size([1, 384, 5, 5])7 output shape:  torch.Size([1, 10, 5, 5])8 output shape:  torch.Size([1, 10, 1, 1])9 output shape:  torch.Size([1, 10])</code></pre><pre><code class="lang-python">batch_size = 128# 如出现“out of memory”的报错信息，可减小batch_size或resize#train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)lr, num_epochs = 0.002, 5optimizer = torch.optim.Adam(net.parameters(), lr=lr)d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</code></pre><p>NiN重复使⽤由卷积层和代替全连接层的1×1卷积层构成的NiN块来构建深层⽹络。<br>NiN去除了容易造成过拟合的全连接输出层，而是将其替换成输出通道数等于标签类别数 的NiN块和全局平均池化层。<br>NiN的以上设计思想影响了后⾯⼀系列卷积神经⽹络的设计。  </p><h2 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h2><ol><li>由Inception基础块组成。  </li><li>Inception块相当于⼀个有4条线路的⼦⽹络（并行线路）。它通过不同窗口形状的卷积层和最⼤池化层来并⾏抽取信息，并使⽤<strong>「1×1卷积层」</strong> <strong>「减少通道数从而降低模型复杂度」</strong>。   </li><li>可以⾃定义的超参数是每个层的输出通道数，我们以此来控制模型复杂度。 </li><li>都用了 padding 保证高和宽不变</li></ol><p>概括 4 个模型：</p><ul><li>AlexNet 难以灵活地改变模型结构。</li><li>VGG 通过重复使⽤简单的基础块来构建深度模型, 由VGG block组成</li><li>NiN 使⽤全局平均池化层对每个通道中所有元素求平均并直接⽤于分类, 由NiN block组成</li><li>GooLeNet 通过不同窗口形状的卷积层和最⼤池化层来并⾏抽取信息, 由Inception组成</li></ul><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/upload/image/q5l6uortw.png?imageView2/0/w/640/h/640" alt="Inception"></p><pre><code class="lang-python">class Inception(nn.Module):    # c1 - c4为每条线路里的层的输出通道数    def __init__(self, in_c, c1, c2, c3, c4):        super(Inception, self).__init__()        # 线路1，单1 x 1卷积层        self.p1_1 = nn.Conv2d(in_c, c1, kernel_size=1)        # 线路2，1 x 1卷积层后接3 x 3卷积层        self.p2_1 = nn.Conv2d(in_c, c2[0], kernel_size=1)        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)        # 线路3，1 x 1卷积层后接5 x 5卷积层        self.p3_1 = nn.Conv2d(in_c, c3[0], kernel_size=1)        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)        # 线路4，3 x 3最大池化层后接1 x 1卷积层        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)        self.p4_2 = nn.Conv2d(in_c, c4, kernel_size=1)    def forward(self, x):        p1 = F.relu(self.p1_1(x))        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))        p4 = F.relu(self.p4_2(self.p4_1(x)))        return torch.cat((p1, p2, p3, p4), dim=1)  # 在通道维上连结输出（各个线路通道数直接相加）</code></pre><h3 id="GoogLeNet模型"><a href="#GoogLeNet模型" class="headerlink" title="GoogLeNet模型"></a>GoogLeNet模型</h3><p>完整模型结构 （中间的并联是若干个 Inception）</p><p>最后一层，它可以确保经过 Global AvgPool 后输出的形状为 [batch_size, num_of_channels, 1, 1]。<br>当输入图像的尺寸发生变化时，可以发挥作用</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/upload/image/q5l6x0fyyn.png?imageView2/0/w/640/h/640" alt="完整GoogLeNet模型"></p><pre><code class="lang-python">b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),  # 抽取特征，减小宽高                   nn.ReLU(),                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),  # 1x1 增加非线性的作用                   nn.Conv2d(64, 192, kernel_size=3, padding=1), # padding=1 在 kernel_size=3 时可保持宽高不变                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))b3 = nn.Sequential(Inception(192, 64, (96, 128), (16, 32), 32),  # 对应 inception 内的各个线路                   Inception(256, 128, (128, 192), (32, 96), 64),                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))b4 = nn.Sequential(Inception(480, 192, (96, 208), (16, 48), 64),  # 480 就是上一层各个线路的叠加                   Inception(512, 160, (112, 224), (24, 64), 64),                   Inception(512, 128, (128, 256), (24, 64), 64),                   Inception(512, 112, (144, 288), (32, 64), 64),                   Inception(528, 256, (160, 320), (32, 128), 128),                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))b5 = nn.Sequential(Inception(832, 256, (160, 320), (32, 128), 128),                   Inception(832, 384, (192, 384), (48, 128), 128),                   d2l.GlobalAvgPool2d())net = nn.Sequential(b1, b2, b3, b4, b5,                     d2l.FlattenLayer(), nn.Linear(1024, 10))net = nn.Sequential(b1, b2, b3, b4, b5, d2l.FlattenLayer(), nn.Linear(1024, 10))X = torch.rand(1, 1, 96, 96)for blk in net.children():     X = blk(X)    print(&#39;output shape: &#39;, X.shape)#batchsize=128batch_size = 16# 如出现“out of memory”的报错信息，可减小batch_size或resize#train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)lr, num_epochs = 0.001, 5optimizer = torch.optim.Adam(net.parameters(), lr=lr)d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</code></pre><p>每一层训练，feature_map 图像的宽高都会减半，特征数都会翻倍，表示提取的特征越来越高级，越来越抽象</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/8xDuCcnMRre1LbU.png" alt="各层的实际参数-训练结果"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> CV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动手深度学习(十):LeNet 模型 -- CNN 升级版</title>
      <link href="/2020/02/20/dl-notes10-LeNet/"/>
      <url>/2020/02/20/dl-notes10-LeNet/</url>
      
        <content type="html"><![CDATA[<h2 id="LeNet-模型（CNN-升级版）"><a href="#LeNet-模型（CNN-升级版）" class="headerlink" title="LeNet 模型（CNN 升级版）"></a>LeNet 模型（CNN 升级版）</h2><ol><li>lenet 模型介绍</li><li>lenet 网络搭建</li><li>运用lenet进行图像识别-fashion-mnist数据集</li></ol><h1 id="Convolutional-Neural-Networks"><a href="#Convolutional-Neural-Networks" class="headerlink" title="Convolutional Neural Networks"></a>Convolutional Neural Networks</h1><p>使用全连接层的局限性：</p><ul><li>图像在同一列邻近的像素在这个向量中可能相距较远。它们构成的模式可能难以被模型识别。</li><li>对于大尺寸的输入图像，使用全连接层容易导致模型过大。</li></ul><p>使用卷积层的优势：</p><ul><li>卷积层保留输入形状。</li><li>卷积层通过滑动窗口将同一卷积核与不同位置的输入重复计算，从而避免参数尺寸过大。</li></ul><h2 id="LeNet-模型"><a href="#LeNet-模型" class="headerlink" title="LeNet 模型"></a>LeNet 模型</h2><p>LeNet分为卷积层块和全连接层块两个部分。下面我们分别介绍这两个模块。</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://s2.ax1x.com/2020/02/19/3VrrQ0.png" alt="LeNet 模型图"></p><p>卷积层块里的基本单位是卷积层后接平均池化层：卷积层用来识别图像里的空间模式，如线条和物体局部，之后的平均池化层则用来降低卷积层对位置的敏感性。</p><p>卷积层块由两个这样的基本单位重复堆叠构成。在卷积层块中，每个卷积层都使用$5 \times 5$的窗口，并在输出上使用sigmoid激活函数。第一个卷积层输出通道数为6，第二个卷积层输出通道数则增加到16。</p><p>全连接层块含3个全连接层。它们的输出个数分别是120、84和10，其中10为输出的类别个数。</p><p>下面我们通过Sequential类来实现LeNet模型。</p><pre><code class="lang-python">#importimport syssys.path.append(&quot;/home/kesci/input&quot;)import d2lzh1981 as d2limport torchimport torch.nn as nnimport torch.optim as optimimport time</code></pre><pre><code class="lang-python">#netclass Flatten(torch.nn.Module):  #展平操作    def forward(self, x):        return x.view(x.shape[0], -1)class Reshape(torch.nn.Module): #将图像大小重定型    def forward(self, x):        return x.view(-1,1,28,28)      #(B x C x H x W)net = torch.nn.Sequential(     #Lelet                                                      Reshape(),    nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2), #卷积 b*1*28*28  =&gt;b*6*28*28 （b:batch_size)    nn.Sigmoid(),                                                           nn.AvgPool2d(kernel_size=2, stride=2),    # 平均池化                #b*6*28*28  =&gt;b*6*14*14    nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),           #b*6*14*14  =&gt;b*16*10*10    nn.Sigmoid(),    nn.AvgPool2d(kernel_size=2, stride=2),                              #b*16*10*10  =&gt; b*16*5*5    Flatten(),          # 纯粹的展平操作                                #b*16*5*5   =&gt; b*400    nn.Linear(in_features=16*5*5, out_features=120),    nn.Sigmoid(),    nn.Linear(120, 84),    nn.Sigmoid(),    nn.Linear(84, 10))</code></pre><p>接下来我们构造一个高和宽均为28的单通道数据样本，并逐层进行前向计算来查看每个层的输出形状。</p><pre><code class="lang-python">#printX = torch.randn(size=(1,1,28,28), dtype = torch.float32)   # batch_size = 1, channel = 1, h = w =28for layer in net:    X = layer(X)    print(layer.__class__.__name__,&#39;output shape: \t&#39;,X.shape)</code></pre><pre><code>Reshape output shape:      torch.Size([1, 1, 28, 28])Conv2d output shape:      torch.Size([1, 6, 28, 28])Sigmoid output shape:      torch.Size([1, 6, 28, 28])AvgPool2d output shape:      torch.Size([1, 6, 14, 14])Conv2d output shape:      torch.Size([1, 16, 10, 10])Sigmoid output shape:      torch.Size([1, 16, 10, 10])AvgPool2d output shape:      torch.Size([1, 16, 5, 5])Flatten output shape:      torch.Size([1, 400])Linear output shape:      torch.Size([1, 120])Sigmoid output shape:      torch.Size([1, 120])Linear output shape:      torch.Size([1, 84])Sigmoid output shape:      torch.Size([1, 84])Linear output shape:      torch.Size([1, 10])</code></pre><p>可以看到，在卷积层块中输入的高和宽在逐层减小。卷积层由于使用高和宽均为5的卷积核，从而将高和宽分别减小4，而池化层则将高和宽减半，但通道数则从1增加到16。全连接层则逐层减少输出个数，直到变成图像的类别数10。</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://s2.ax1x.com/2020/02/19/3VrHeO.png" alt="逐层变化情况"></p><h2 id="获取数据和训练模型"><a href="#获取数据和训练模型" class="headerlink" title="获取数据和训练模型"></a>获取数据和训练模型</h2><p>下面我们来实现LeNet模型。我们仍然使用Fashion-MNIST作为训练数据集。</p><pre><code class="lang-python"># 数据batch_size = 256train_iter, test_iter = d2l.load_data_fashion_mnist(  # 训练集、测试集    batch_size=batch_size, root=&#39;/home/kesci/input/FashionMNIST2065&#39;)print(len(train_iter))  # 235 个 batch，每个 batch 256张图</code></pre><pre><code>235</code></pre><p>为了使读者更加形象的看到数据，添加额外的部分来展示数据的图像</p><pre><code class="lang-python">#数据展示import matplotlib.pyplot as pltdef show_fashion_mnist(images, labels):    d2l.use_svg_display()    # 这里的_表示我们忽略（不使用）的变量    _, figs = plt.subplots(1, len(images), figsize=(12, 12))    for f, img, lbl in zip(figs, images, labels):        f.imshow(img.view((28, 28)).numpy())        f.set_title(lbl)        f.axes.get_xaxis().set_visible(False)        f.axes.get_yaxis().set_visible(False)    plt.show()for Xdata,ylabel in train_iter:    breakX, y = [], []for i in range(10):    print(Xdata[i].shape,ylabel[i].numpy())    X.append(Xdata[i]) # 将第i个feature加到X中    y.append(ylabel[i].numpy()) # 将第i个label加到y中show_fashion_mnist(X, y)</code></pre><pre><code>torch.Size([1, 28, 28]) 3torch.Size([1, 28, 28]) 8torch.Size([1, 28, 28]) 1torch.Size([1, 28, 28]) 4torch.Size([1, 28, 28]) 0torch.Size([1, 28, 28]) 0torch.Size([1, 28, 28]) 4torch.Size([1, 28, 28]) 9torch.Size([1, 28, 28]) 4torch.Size([1, 28, 28]) 7</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/4FE5CE6E20494BFE898E9D8EAAF30C7B/q5ndypu2sq.svg"></p><p>因为卷积神经网络计算比多层感知机要复杂，建议使用GPU来加速计算。我们查看看是否可以用GPU，如果成功则使用<code>cuda:0</code>，否则仍然使用<code>cpu</code>。</p><pre><code class="lang-python"># This function has been saved in the d2l package for future use#use GPUdef try_gpu():    &quot;&quot;&quot;If GPU is available, return torch.device as cuda:0; else return torch.device as cpu.&quot;&quot;&quot;    if torch.cuda.is_available():        device = torch.device(&#39;cuda:0&#39;)    else:        device = torch.device(&#39;cpu&#39;)    return devicedevice = try_gpu()device</code></pre><pre><code>device(type=&#39;cpu&#39;)</code></pre><p>我们实现<code>evaluate_accuracy</code>函数，该函数用于计算模型<code>net</code>在数据集<code>data_iter</code>上的准确率。</p><pre><code class="lang-python">#计算准确率&#39;&#39;&#39;(1). net.train()  启用 BatchNormalization 和 Dropout，将BatchNormalization和Dropout置为True(2). net.eval()不启用 BatchNormalization 和 Dropout，将BatchNormalization和Dropout置为False&#39;&#39;&#39;def evaluate_accuracy(data_iter, net,device=torch.device(&#39;cpu&#39;)):    &quot;&quot;&quot;Evaluate accuracy of a model on the given data set.&quot;&quot;&quot;    acc_sum,n = torch.tensor([0],dtype=torch.float32,device=device),0    for X,y in data_iter:        # If device is the GPU, copy the data to the GPU.        X,y = X.to(device),y.to(device)        net.eval()        with torch.no_grad():   # 不需要梯度，也不会反向传播            y = y.long()            acc_sum += torch.sum((torch.argmax(net(X), dim=1) == y))              # dim=1 dim 0 -&gt; 1 -&gt; ... 维度从大到小[[0.2 ,0.4 ,0.5 ,0.6 ,0.8] ,[ 0.1,0.2 ,0.4 ,0.3 ,0.1]] =&gt; [ 4 , 2 ]            n += y.shape[0]    return acc_sum.item()/n</code></pre><p>我们定义函数<code>train_ch5</code>，用于训练模型。</p><pre><code class="lang-python">#训练函数def train_ch5(net, train_iter, test_iter,criterion, num_epochs, batch_size, device,lr=None):    &quot;&quot;&quot;Train and evaluate a model with CPU or GPU.&quot;&quot;&quot;    print(&#39;training on&#39;, device)    net.to(device)    optimizer = optim.SGD(net.parameters(), lr=lr)    for epoch in range(num_epochs):        train_l_sum = torch.tensor([0.0],dtype=torch.float32,device=device)     # loss 总和        train_acc_sum = torch.tensor([0.0],dtype=torch.float32,device=device)   # 预测正确的总数        n, start = 0, time.time()        for X, y in train_iter:  # X 训练数据，y 训练标签            net.train()  # 与 net.eval 相反，启用 批标准化和 dropout            optimizer.zero_grad()  # 不同批的梯度不相关，要清零            X,y = X.to(device),y.to(device)             y_hat = net(X)            loss = criterion(y_hat, y)            loss.backward()            optimizer.step()            with torch.no_grad():                y = y.long()                train_l_sum += loss.float()                train_acc_sum += (torch.sum((torch.argmax(y_hat, dim=1) == y))).float()                n += y.shape[0]        test_acc = evaluate_accuracy(test_iter, net,device)        print(&#39;epoch %d, loss %.4f, train acc %.3f, test acc %.3f, &#39;              &#39;time %.1f sec&#39;              % (epoch + 1, train_l_sum/n, train_acc_sum/n, test_acc,                 time.time() - start))</code></pre><p>我们重新将模型参数初始化到对应的设备<code>device</code>(<code>cpu</code> or <code>cuda:0</code>)之上，并使用Xavier随机初始化。损失函数和训练算法则依然使用交叉熵损失函数和小批量随机梯度下降。</p><pre><code class="lang-python"># 训练lr, num_epochs = 0.9, 10def init_weights(m):    if type(m) == nn.Linear or type(m) == nn.Conv2d:        torch.nn.init.xavier_uniform_(m.weight)net.apply(init_weights)net = net.to(device)criterion = nn.CrossEntropyLoss()   #交叉熵描述了两个概率分布之间的距离，交叉熵越小说明两者之间越接近train_ch5(net, train_iter, test_iter, criterion,num_epochs, batch_size,device, lr)</code></pre><pre><code>training on cpuepoch 1, loss 0.0091, train acc 0.100, test acc 0.168, time 21.6 secepoch 2, loss 0.0065, train acc 0.355, test acc 0.599, time 21.5 secepoch 3, loss 0.0035, train acc 0.651, test acc 0.665, time 21.8 secepoch 4, loss 0.0028, train acc 0.717, test acc 0.723, time 21.7 secepoch 5, loss 0.0025, train acc 0.746, test acc 0.753, time 21.4 secepoch 6, loss 0.0023, train acc 0.767, test acc 0.754, time 21.5 secepoch 7, loss 0.0022, train acc 0.782, test acc 0.785, time 21.3 secepoch 8, loss 0.0021, train acc 0.798, test acc 0.791, time 21.8 secepoch 9, loss 0.0019, train acc 0.811, test acc 0.790, time 22.0 secepoch 10, loss 0.0019, train acc 0.821, test acc 0.804, time 22.1 sec</code></pre><pre><code class="lang-python"># testfor testdata,testlabe in test_iter:    testdata,testlabe = testdata.to(device),testlabe.to(device)    breakprint(testdata.shape,testlabe.shape)net.eval()y_pre = net(testdata)print(torch.argmax(y_pre,dim=1)[:10])print(testlabe[:10])</code></pre><pre><code>torch.Size([256, 1, 28, 28]) torch.Size([256])tensor([9, 2, 1, 1, 6, 1, 2, 6, 5, 7])tensor([9, 2, 1, 1, 6, 1, 4, 6, 5, 7])</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>卷积神经网络就是含卷积层的网络。<br>LeNet交替使用卷积层和最大池化层后接全连接层来进行图像分类。</p><h2 id="练习题"><a href="#练习题" class="headerlink" title="练习题"></a>练习题</h2><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://s2.ax1x.com/2020/02/19/3VkvBd.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> CV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动手深度学习(九):卷积神经网络 CNN 基础</title>
      <link href="/2020/02/20/dl-notes9-cnn-basic/"/>
      <url>/2020/02/20/dl-notes9-cnn-basic/</url>
      
        <content type="html"><![CDATA[<h1 id="卷积神经网络基础"><a href="#卷积神经网络基础" class="headerlink" title="卷积神经网络基础"></a>卷积神经网络基础</h1><p>卷积本质上体现了<code>两种信号的时移累加</code>。</p><p>本节我们介绍卷积神经网络的基础概念，主要是卷积层和池化层，并解释填充、步幅、输入通道和输出通道的含义。</p><h2 id="二维卷积层"><a href="#二维卷积层" class="headerlink" title="二维卷积层"></a>二维卷积层</h2><p>本节介绍的是最常见的二维卷积层，常用于处理图像数据。</p><h3 id="二维互相关运算"><a href="#二维互相关运算" class="headerlink" title="二维互相关运算"></a>二维互相关运算</h3><p>二维互相关（cross-correlation）运算的输入是一个二维输入数组和一个二维<code>核（kernel）数组</code>，输出也是一个二维数组，其中核数组通常称为<code>卷积核</code>或<code>过滤器</code>（filter）。卷积核的尺寸通常小于输入数组，卷积核在输入数组上滑动，在每个位置上，卷积核与该位置处的输入子数组按元素相乘并求和，得到输出数组中相应位置的元素。图1展示了一个互相关运算的例子，阴影部分分别是输入的第一个计算区域、核数组以及对应的输出。</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/xTyKEpR7j6gVaHb.png" alt="二维互相关运算"></p><p>下面我们用<code>corr2d</code>函数实现二维互相关运算，它接受输入数组<code>X</code>与核数组<code>K</code>，并输出数组<code>Y</code>。</p><pre><code class="lang-python">import torch import torch.nn as nndef corr2d(X, K):    H, W = X.shape    h, w = K.shape    Y = torch.zeros(H - h + 1, W - w + 1)    for i in range(Y.shape[0]):        for j in range(Y.shape[1]):            Y[i, j] = (X[i: i + h, j: j + w] * K).sum()    return Y</code></pre><p>构造上图中的输入数组<code>X</code>、核数组<code>K</code>来验证二维互相关运算的输出。</p><pre><code class="lang-python">X = torch.tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]])K = torch.tensor([[0, 1], [2, 3]])Y = corr2d(X, K)print(Y)</code></pre><pre><code>tensor([[19., 25.],        [37., 43.]])</code></pre><h3 id="二维卷积层-1"><a href="#二维卷积层-1" class="headerlink" title="二维卷积层"></a>二维卷积层</h3><p>二维卷积层将输入和卷积核做互相关运算，并加上一个标量偏置来得到输出。卷积层的<code>模型参数</code>包括「卷积核」<code>self.weight</code>和「标量偏置」<code>self.bias</code>。</p><ul><li><p><code>nn.Parameter()</code>是 tensor 的子类，会为参数自动附上梯度，是可学习的</p></li><li><p><code>nn.Module</code>表示的模型中会维护一个参数集合，定义nn.Parameter()会自动把参数注册到 model 的参数集合中</p></li></ul><pre><code class="lang-python">class Conv2D(nn.Module):    def __init__(self, kernel_size):  # kernel_size 卷积层的高和宽        super(Conv2D, self).__init__()        self.weight = nn.Parameter(torch.randn(kernel_size))        self.bias = nn.Parameter(torch.randn(1))    def forward(self, x):        return corr2d(x, self.weight) + self.bias  # 广播机制，每个位置加了同样一个偏置</code></pre><p>下面我们看一个例子，我们构造一张$6 \times 8$的图像，中间4列为黑（0），其余为白（1），希望检测到颜色边缘。我们的标签是一个$6 \times 7$的二维数组，第2列是1（从1到0的边缘），第6列是-1（从0到1的边缘）。</p><pre><code class="lang-python">X = torch.ones(6, 8)Y = torch.zeros(6, 7)X[:, 2: 6] = 0Y[:, 1] = 1Y[:, 5] = -1print(X)print(Y)</code></pre><pre><code>tensor([[1., 1., 0., 0., 0., 0., 1., 1.],        [1., 1., 0., 0., 0., 0., 1., 1.],        [1., 1., 0., 0., 0., 0., 1., 1.],        [1., 1., 0., 0., 0., 0., 1., 1.],        [1., 1., 0., 0., 0., 0., 1., 1.],        [1., 1., 0., 0., 0., 0., 1., 1.]])tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])</code></pre><p>我们希望学习一个$1 \times 2$卷积层，通过卷积层来检测颜色边缘。（因为计算边缘考虑的是同一行的相邻两个元素，1 行 2 列）</p><pre><code class="lang-python">conv2d = Conv2D(kernel_size=(1, 2))step = 30lr = 0.01for i in range(step):    Y_hat = conv2d(X)    l = ((Y_hat - Y) ** 2).sum()    l.backward()    # 梯度下降    conv2d.weight.data -= lr * conv2d.weight.grad    conv2d.bias.data -= lr * conv2d.bias.grad    # 梯度清零    conv2d.weight.grad.zero_()    conv2d.bias.grad.zero_()    if (i + 1) % 5 == 0:        print(&#39;Step %d, loss %.3f&#39; % (i + 1, l.item()))print(conv2d.weight.data)print(conv2d.bias.data)</code></pre><pre><code>Step 5, loss 4.569Step 10, loss 0.949Step 15, loss 0.228Step 20, loss 0.060Step 25, loss 0.016Step 30, loss 0.004tensor([[ 1.0161, -1.0177]])tensor([0.0009])</code></pre><h3 id="互相关运算与卷积运算"><a href="#互相关运算与卷积运算" class="headerlink" title="互相关运算与卷积运算"></a>互相关运算与卷积运算</h3><p>卷积层得名于卷积运算，但卷积层中用到的并非卷积运算而是互相关运算。我们将核数组上下翻转、左右翻转，再与输入数组做互相关运算，这一过程就是卷积运算。由于卷积层的核数组是可学习的，所以使用互相关运算与使用卷积运算并无本质区别。</p><h3 id="特征图与感受野"><a href="#特征图与感受野" class="headerlink" title="特征图与感受野"></a>特征图与感受野</h3><p>二维卷积层<strong>输出</strong>的二维数组可以看作是输入在空间维度（宽和高）上某一级的表征，也叫<strong>「特征图」（feature map）</strong>。影响元素$x$的前向计算的所有可能<strong>输入</strong>区域（可能大于输入的实际尺寸）叫做$x$的 <strong>「感受野」（receptive field）</strong>。</p><p>以图1为例，输入中阴影部分的四个元素是输出中阴影部分元素的感受野。我们将图中形状为$2 \times 2$的输出记为$Y$，将$Y$与另一个形状为$2 \times 2$的核数组做互相关运算，输出单个元素$z$。那么，$z$在$Y$上的感受野包括$Y$的全部四个元素，在输入上的感受野包括其中全部9个元素。可见，我们可以通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征。</p><p>堆叠加深，不同层的感受野不同，<code>越深越大</code></p><h2 id="填充和步幅"><a href="#填充和步幅" class="headerlink" title="填充和步幅"></a>填充和步幅</h2><p>我们介绍卷积层的两个超参数，即填充和步幅，它们可以对给定形状的输入和卷积核改变输出形状。</p><h3 id="填充"><a href="#填充" class="headerlink" title="填充"></a>填充</h3><p>填充（padding）是指在输入高和宽的两侧填充元素（通常是0元素），图2里我们在原输入高和宽的两侧分别添加了值为0的元素。</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/qzCBiYL8nONrIds.png" alt="填充"></p><p>图2 在输入的高和宽两侧分别填充了0元素的二维互相关计算</p><p>如果原输入的高和宽是$n_h$和$n_w$，卷积核的高和宽是$k_h$和$k_w$，在高的两侧一共填充$p_h$行，在宽的两侧一共填充$p_w$列，则输出形状为：</p><script type="math/tex; mode=display">(n_h+p_h-k_h+1)\times(n_w+p_w-k_w+1)</script><p>我们在卷积神经网络中使用奇数高宽的核，比如$3 \times 3$，$5 \times 5$的卷积核，对于高度（或宽度）为大小为$2 k + 1$的核，令步幅为1，在高（或宽）两侧选择大小为$k$的填充，便可保持输入与输出尺寸相同。</p><h3 id="步幅"><a href="#步幅" class="headerlink" title="步幅"></a>步幅</h3><p>在互相关运算中，卷积核在输入数组上滑动，每次滑动的行数与列数即是步幅（stride）。此前我们使用的步幅都是1，图3展示了在高上步幅为3、在宽上步幅为2的二维互相关运算。</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/r9cVRdxynqzvjbt.png" alt="步幅"></p><p>图3 高和宽上步幅分别为3和2的二维互相关运算</p><p>一般来说，当高上步幅为$s_h$，宽上步幅为$s_w$时，输出形状为：</p><script type="math/tex; mode=display">\lfloor(n_h+p_h-k_h+s_h)/s_h\rfloor \times \lfloor(n_w+p_w-k_w+s_w)/s_w\rfloor</script><p>如果$p_h=k_h-1$，$p_w=k_w-1$，那么输出形状将简化为$\lfloor(n_h+s_h-1)/s_h\rfloor \times \lfloor(n_w+s_w-1)/s_w\rfloor$。更进一步，如果输入的高和宽能分别被高和宽上的步幅整除，那么输出形状将是$(n_h / s_h) \times (n_w/s_w)$。</p><p>当$p_h = p_w = p$时，我们称填充为$p$；当$s_h = s_w = s$时，我们称步幅为$s$。</p><h2 id="多输入通道和多输出通道"><a href="#多输入通道和多输出通道" class="headerlink" title="多输入通道和多输出通道"></a>多输入通道和多输出通道</h2><p>之前的输入和输出都是二维数组，但真实数据的维度经常更高。例如，彩色图像在高和宽2个维度外还有RGB（红、绿、蓝）3个颜色通道。假设彩色图像的高和宽分别是$h$和$w$（像素），那么它可以表示为一个$3 \times h \times w$的多维数组，我们将大小为3的这一维称为通道（channel）维。</p><h3 id="多输入通道"><a href="#多输入通道" class="headerlink" title="多输入通道"></a>多输入通道</h3><p>卷积层的输入可以包含多个通道，图4展示了一个含2个输入通道的二维互相关计算的例子。（每层的输入和该层的核相乘，最后叠加成单维度输出，输出只有一个通道）</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/NCjg1iDhPY8ItOF.png" alt="2 输入通道 1 输出通道"></p><p>图4 含2个输入通道的互相关计算</p><p>假设输入数据的通道数为$c_i$，卷积核形状为$k_h\times k_w$，我们为每个输入通道各分配一个形状为$k_h\times k_w$的核数组，将$c_i$个互相关运算的二维输出按通道相加，得到一个二维数组作为输出。我们把$c_i$个核数组在通道维上连结，即得到一个形状为$c_i\times k_h\times k_w$的卷积核。</p><h3 id="多输出通道"><a href="#多输出通道" class="headerlink" title="多输出通道"></a>多输出通道</h3><p>卷积层的输出也可以包含多个通道，设卷积核输入通道数和输出通道数分别为$c_i$和$c_o$，高和宽分别为$k_h$和$k_w$。如果希望得到含多个通道的输出，我们可以为每个输出通道分别创建形状为$c_i\times k_h\times k_w$的核数组，将它们在输出通道维上连结（有加法计算），卷积核的形状即$c_o\times c_i\times k_h\times k_w$。<br>（如下图，3 个输入通道，2 个输出通道，中间有 6 个 $1 \times 1$ 的核数组。)</p><p>对于输出通道的卷积核，我们提供这样一种理解，一个$c_i \times k_h \times k_w$的核数组可以提取某种局部特征，但是输入可能具有相当丰富的特征，我们需要有多个这样的$c_i \times k_h \times k_w$的核数组，不同的核数组提取的是不同的特征。</p><h3 id="1x1卷积层"><a href="#1x1卷积层" class="headerlink" title="1x1卷积层"></a>1x1卷积层</h3><p>最后讨论形状为$1 \times 1$的卷积核，我们通常称这样的卷积运算为$1 \times 1$卷积，称包含这种卷积核的卷积层为$1 \times 1$卷积层。图5展示了使用输入通道数为3、输出通道数为2的$1\times 1$卷积核的互相关计算。</p><p>$1\times 1$卷积层的作用与<code>全连接层</code>等价，相当于<code>矩阵乘法</code>。</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/I8MX65oJvur3b7f.png" alt="2x3=6 个 1x1 卷积层"></p><p>图5 1x1卷积核的互相关计算。输入和输出具有<code>相同的高和宽</code></p><p>$1 \times 1$卷积核可在不改变高宽的情况下，<code>调整通道数</code>。$1 \times 1$卷积核不识别高和宽维度上相邻元素构成的模式，其主要计算发生在通道维上。假设我们将通道维当作特征维，将高和宽维度上的元素当成数据样本，那么$1\times 1$卷积层的作用与全连接层等价。</p><h2 id="卷积层与全连接层的对比"><a href="#卷积层与全连接层的对比" class="headerlink" title="卷积层与全连接层的对比"></a>卷积层与全连接层的对比</h2><p>二维卷积层经常用于处理图像，与此前的全连接层相比，它主要有两个优势：</p><ul><li><p>一是全连接层把图像<code>展平</code>成一个向量，在输入图像上相邻的元素可能因为展平操作不再相邻，网络难以捕捉局部信息。而卷积层的设计，天然地具有提取局部信息的能力。</p></li><li><p>二是卷积层的参数量更少。不考虑偏置的情况下，一个形状为$(c_i, c_o, h, w)$的卷积核的参数量是$c_i \times c_o \times h \times w$，<code>与输入图像的宽高无关</code>。假如一个卷积层的输入和输出形状分别是$(c_1, h_1, w_1)$和$(c_2, h_2, w_2)$，如果要用全连接层进行连接，参数数量就是$c_1 \times c_2 \times h_1 \times w_1 \times h_2 \times w_2$。使用卷积层可以以较少的参数数量来处理更大的图像。</p></li></ul><h2 id="卷积层的简洁实现"><a href="#卷积层的简洁实现" class="headerlink" title="卷积层的简洁实现"></a>卷积层的简洁实现</h2><p>我们使用Pytorch中的<code>nn.Conv2d</code>类来实现二维卷积层，主要关注以下几个构造函数参数：</p><ul><li><code>in_channels</code> (python:int) – Number of channels in the input imag  输入通道数</li><li><code>out_channels</code> (python:int) – Number of channels produced by the convolution 输出通道数</li><li><code>kernel_size</code> (python:int or tuple) – Size of the convolving kernel 卷积核大小，int 表示高和宽相等，元组表示不相等</li><li><code>stride</code> (python:int or tuple, optional) – Stride of the convolution. Default: 1</li><li><code>padding</code> (python:int or tuple, optional) – Zero-padding added to both sides of the input. Default: 0</li><li><code>bias</code> (bool, optional) – If True, adds a learnable bias to the output. Default: True</li></ul><p><code>forward</code>函数的参数为一个四维张量，形状为$(N批量大小, C_{in}通道数， H_{in}高度, W_{in}宽度)$，返回值也是一个四维张量，形状为$(N, C_{out}, H_{out}, W_{out})$</p><p>kernel_size（即卷积核大小）这是需要人为设定的参数，该参数是不需要学习的，当然大小不同，卷积结果也是不同的。经过大量实验表明，大多选用<code>1x1、3x3、5x5</code>等尺寸较小且长宽为奇数的卷积核。</p><p>代码讲解</p><pre><code class="lang-python">X = torch.rand(4, 2, 3, 5)print(X.shape)conv2d = nn.Conv2d(in_channels=2, out_channels=3, kernel_size=(3, 5), stride=1, padding=(1, 2))Y = conv2d(X)print(&#39;Y.shape: &#39;, Y.shape)print(&#39;weight.shape: &#39;, conv2d.weight.shape)  # padding 和 size 刚好是 2k+1 的关系，所以 in 和 out 宽高不变print(&#39;bias.shape: &#39;, conv2d.bias.shape) # Y.shape:  torch.Size([4, 2, 3, 5])# Y.shape:  torch.Size([4, 3, 3, 5])# weight.shape:  torch.Size([3, 2, 3, 5])# bias.shape:  torch.Size([3])</code></pre><h2 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h2><h3 id="二维池化层"><a href="#二维池化层" class="headerlink" title="二维池化层"></a>二维池化层</h3><p>池化层主要用于缓解卷积层对位置的过度敏感性。同卷积层一样，池化层每次对输入数据的一个固定形状窗口（又称池化窗口）中的元素计算输出，池化层直接计算池化窗口内元素的最大值或者平均值，该运算也分别叫做<code>最大池化</code>或<code>平均池化</code>。图6展示了池化窗口形状为$2\times 2$的最大池化。</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/CpF4XrYOE3Z2qKc.png" alt="2x2 的最大池化"></p><p>二维平均池化的工作原理与二维最大池化类似，但将最大运算符替换成平均运算符。池化窗口形状为$p \times q$的池化层称为$p \times q$池化层，其中的池化运算叫作$p \times q$池化。</p><p>池化层也可以在输入的高和宽两侧填充并调整窗口的移动步幅来改变输出形状。池化层填充和步幅与卷积层填充和步幅的工作机制一样。</p><p>在处理多通道输入数据时，池化层对每个输入通道分别池化，但不会像卷积层那样将各通道的结果按通道相加。这意味着池化层的<code>输出通道数与输入通道数相等</code>。</p><ul><li><code>卷积与池化的区别</code>：<br>  池化层没有需要学习的特征<br>  卷积层用来学习识别某一种特征<br>  池化层的主要作用是降维<br>  卷积层当设置步长 &gt;1 时也可以用来完成降维的工作</li></ul><h3 id="池化层的简洁实现"><a href="#池化层的简洁实现" class="headerlink" title="池化层的简洁实现"></a>池化层的简洁实现</h3><p>我们使用Pytorch中的<code>nn.MaxPool2d</code>实现最大池化层，关注以下构造函数参数：</p><ul><li><code>kernel_size</code> – the size of the window to take a max over</li><li><code>stride</code> – the stride of the window. Default value is kernel_size</li><li><code>padding</code> – implicit zero padding to be added on both sides</li></ul><p><code>forward</code>函数的参数为一个四维张量，形状为$(N, C, H_{in}, W_{in})$，返回值也是一个四维张量，形状为$(N, C, H_{out}, W_{out})$，其中$N$是批量大小，$C, H, W$分别表示通道数、高度、宽度。</p><p>代码讲解</p><pre><code class="lang-python">X = torch.arange(32, dtype=torch.float32).view(1, 2, 4, 4)pool2d = nn.MaxPool2d(kernel_size=3, padding=1, stride=(2, 1))Y = pool2d(X)print(X)print(Y)</code></pre><pre><code>tensor([[[[ 0.,  1.,  2.,  3.],          [ 4.,  5.,  6.,  7.],          [ 8.,  9., 10., 11.],          [12., 13., 14., 15.]],         [[16., 17., 18., 19.],          [20., 21., 22., 23.],          [24., 25., 26., 27.],          [28., 29., 30., 31.]]]])tensor([[[[ 5.,  6.,  7.,  7.],          [13., 14., 15., 15.]],         [[21., 22., 23., 23.],          [29., 30., 31., 31.]]]])</code></pre><p>平均池化层使用的是<code>nn.AvgPool2d</code>，使用方法与<code>nn.MaxPool2d</code>相同。</p><h2 id="练习题"><a href="#练习题" class="headerlink" title="练习题"></a>练习题</h2><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/ktF7bxGVugQzO5U.png" alt=""></p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/FYjpTINr4ViwZ39.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> CV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动手深度学习(八):机器翻译简介</title>
      <link href="/2020/02/20/dl-notes8-mach-trans/"/>
      <url>/2020/02/20/dl-notes8-mach-trans/</url>
      
        <content type="html"><![CDATA[<h1 id="机器翻译和数据集"><a href="#机器翻译和数据集" class="headerlink" title="机器翻译和数据集"></a>机器翻译和数据集</h1><p>机器翻译（MT）：将一段文本从一种语言自动翻译为另一种语言，用神经网络解决这个问题通常称为神经机器翻译（NMT）。<br>主要特征：输出是单词<code>序列</code>而不是单个单词。 输出序列的长度可能与源序列的长度<code>不同</code>。</p><h2 id="机器翻译任务代码总结如下"><a href="#机器翻译任务代码总结如下" class="headerlink" title="机器翻译任务代码总结如下"></a>机器翻译任务代码总结如下</h2><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><ol><li><code>读取数据</code>，处理数据中的编码问题，并将无效的字符串删除</li><li><code>分词</code>，分词的目的就是将字符串转换成单词组成的列表。目前有很多现成的分词工具可以直接使用，也可以直接按照空格进行分词(不推荐，因为分词不是很准确)</li><li><code>建立词典</code>，将单词组成的列表编程单词id组成的列表，这里会得到如下几样东西<ol><li>去重后词典，及其中单词对应的索引列表</li><li>还可以得到给定索引找到其对应的单词的列表，以及给定单词得到对应索引的字典</li><li>原始语料所有词对应的词典索引的列表</li></ol></li><li>对数据进行<code>padding操作</code>。基于rnn的Seq2Seq模型，可以处理任意长度，“变长数据的读入”基于rnn的机器翻译也并不需要固定长度。要<code>padding</code>，是因为像tf、pytorch这些框架要求一个batch的数据必须长度相等，不然会报错；要<code>截断</code>，设置最大的数据长度是因为decode的时候达到这个长度我们就停止；再一个原因就是为了加快计算，不然的话为了单个特别长的数据，batch中的其他数据都补成这么长，很慢。</li><li><code>制作数据生成器</code>，但是需要注意的是对于翻译任务的数据格式，机器翻译的输入是一段文本序列，输出也是一段文本序列。</li></ol><h3 id="Seq2Seq模型的构建"><a href="#Seq2Seq模型的构建" class="headerlink" title="Seq2Seq模型的构建"></a>Seq2Seq模型的构建</h3><ol><li>先编码后解码的框架: 先对输入序列使用循环神经网络对他进行编码，编码成一个<code>向量</code>之后，再将编码得到的向量作为一个新的<code>解码循环神经网络</code>的隐藏状态的输入，进行解码，一次输出一个序列的元素，再将模型训练输出的序列元素与真实标签计算损失进行学习。</li><li>词嵌入: 一般情况下输入到编码网络中的数据不是一个<code>onehot</code>向量而是经过了编码之后的向量，比如由<code>word2vec</code>技术，让编码后的向量由更加丰富的含义。</li><li>在进行编码和解码的过程中数据都是以<code>时间步</code>展开，也就是(Seq_len,)这种形式的数据进行处理的</li><li>对于编码与解码的循环神经网络，可以通过控制隐藏层的<code>层数</code>及每一层隐藏层神经元的<code>数量</code>来控制模型的复杂度</li><li>编码部分，RNN的用0初始化隐含状态，最后的输出主要是隐藏状态, 编码RNN输出的隐含状态认为是其对应的编码向量</li><li>解码器的整体形状与编码器是一样的，只不过解码器的模型的隐藏状态是<code>由编码器的输出的隐藏状态初始化</code>的。</li></ol><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><ol><li>解码器的输出是一个和词典维度相同的向量，其每个值对应与向量索引位置对应词的分数，一般是选择<code>分数最大</code>的那个词作为最终的输出。</li><li>在计算损失函数之前，要把<code>padding去掉</code>，因为padding的部分不参与计算</li></ol><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><ol><li>解码器在测试的时候需要将模型的输出作为下一个时间步的输入</li><li>Beam Search搜索算法。<ol><li>假设预测的时候词典的大小为3，内容为a,b,c. <code>beam size</code>为2，解码的时候过程如下</li><li>生成第一个词的时候，选择概率最大的两个词，假设为a,c.那么当前的两个序列就是a和c。</li><li>生成第二个词的时候，将当前序列a和c，分别与此表中的所有词进行组合，得到新的6个序列aa ab ac ca cb cc,计算每个序列的得分，并选择得分最高的2个序列，作为新的当前序列，假如为aa cb </li><li>后面不断重复这个过程，直到遇到结束符或者达到最大长度为止，最终输出得分最高的2个序列。</li></ol></li></ol><pre><code class="lang-python">import osos.listdir(&#39;/home/kesci/input/&#39;)</code></pre><pre><code>[&#39;fraeng6506&#39;, &#39;d2l9528&#39;, &#39;d2l6239&#39;]</code></pre><pre><code class="lang-python">import syssys.path.append(&#39;/home/kesci/input/d2l9528/&#39;)import collectionsimport d2limport zipfilefrom d2l.data.base import Vocabimport timeimport torchimport torch.nn as nnimport torch.nn.functional as Ffrom torch.utils import datafrom torch import optim</code></pre><h3 id="数据预处理-1"><a href="#数据预处理-1" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>将数据集清洗、转化为神经网络的输入minbatch</p><pre><code class="lang-python">with open(&#39;/home/kesci/input/fraeng6506/fra.txt&#39;, &#39;r&#39;) as f:      raw_text = f.read()print(raw_text[0:1000])</code></pre><pre><code>Go.    Va !    CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) &amp; #1158250 (Wittydev)Hi.    Salut !    CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) &amp; #509819 (Aiji)Hi.    Salut.    CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) &amp; #4320462 (gillux)Run!    Cours !    CC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) &amp; #906331 (sacredceltic)Run!    Courez !    CC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) &amp; #906332 (sacredceltic)Who?    Qui ?    CC-BY 2.0 (France) Attribution: tatoeba.org #2083030 (CK) &amp; #4366796 (gillux)Wow!    Ça alors !    CC-BY 2.0 (France) Attribution: tatoeba.org #52027 (Zifre) &amp; #374631 (zmoo)Fire!    Au feu !    CC-BY 2.0 (France) Attribution: tatoeba.org #1829639 (Spamster) &amp; #4627939 (sacredceltic)Help!    À l&#39;aide !    CC-BY 2.0 (France) Attribution: tatoeba.org #435084 (lukaszpp) &amp; #128430 (sysko)Jump.    Saute.    CC-BY 2.0 (France) Attribution: tatoeba.org #631038 (Shishir) &amp; #2416938 (Phoenix)Stop!    Ça suffit !    CC-BY 2.0 (France) Attribution: tato</code></pre><pre><code class="lang-python">def preprocess_raw(text):    text = text.replace(&#39;\u202f&#39;, &#39; &#39;).replace(&#39;\xa0&#39;, &#39; &#39;)    out = &#39;&#39;    for i, char in enumerate(text.lower()):        if char in (&#39;,&#39;, &#39;!&#39;, &#39;.&#39;) and i &gt; 0 and text[i-1] != &#39; &#39;:            out += &#39; &#39;        out += char    return outtext = preprocess_raw(raw_text)print(text[0:1000])</code></pre><pre><code>go .    va !    cc-by 2 .0 (france) attribution: tatoeba .org #2877272 (cm) &amp; #1158250 (wittydev)hi .    salut !    cc-by 2 .0 (france) attribution: tatoeba .org #538123 (cm) &amp; #509819 (aiji)hi .    salut .    cc-by 2 .0 (france) attribution: tatoeba .org #538123 (cm) &amp; #4320462 (gillux)run !    cours !    cc-by 2 .0 (france) attribution: tatoeba .org #906328 (papabear) &amp; #906331 (sacredceltic)run !    courez !    cc-by 2 .0 (france) attribution: tatoeba .org #906328 (papabear) &amp; #906332 (sacredceltic)who?    qui ?    cc-by 2 .0 (france) attribution: tatoeba .org #2083030 (ck) &amp; #4366796 (gillux)wow !    ça alors !    cc-by 2 .0 (france) attribution: tatoeba .org #52027 (zifre) &amp; #374631 (zmoo)fire !    au feu !    cc-by 2 .0 (france) attribution: tatoeba .org #1829639 (spamster) &amp; #4627939 (sacredceltic)help !    à l&#39;aide !    cc-by 2 .0 (france) attribution: tatoeba .org #435084 (lukaszpp) &amp; #128430 (sysko)jump .    saute .    cc-by 2 .0 (france) attribution: tatoeba .org #631038 (shishir) &amp; #2416938 (phoenix)stop !    ça suffit !    cc-b</code></pre><p>字符在计算机里是以编码的形式存在，我们通常所用的空格是 \x20 ，是在标准ASCII可见字符 0x20~0x7e 范围内。<br>而 \xa0 属于 latin1 （ISO/IEC_8859-1）中的扩展字符集字符，代表不间断空白符nbsp(non-breaking space)，超出gbk编码范围，是需要去除的特殊字符。再数据预处理的过程中，我们首先需要对数据进行清洗。</p><h3 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h3><p>字符串—-单词组成的列表</p><pre><code class="lang-python">num_examples = 50000source, target = [], []for i, line in enumerate(text.split(&#39;\n&#39;)):    if i &gt; num_examples:        break    parts = line.split(&#39;\t&#39;)    if len(parts) &gt;= 2:        source.append(parts[0].split(&#39; &#39;))        target.append(parts[1].split(&#39; &#39;))source[0:3], target[0:3]</code></pre><pre><code>([[&#39;go&#39;, &#39;.&#39;], [&#39;hi&#39;, &#39;.&#39;], [&#39;hi&#39;, &#39;.&#39;]], [[&#39;va&#39;, &#39;!&#39;], [&#39;salut&#39;, &#39;!&#39;], [&#39;salut&#39;, &#39;.&#39;]])</code></pre><pre><code class="lang-python">d2l.set_figsize()d2l.plt.hist([[len(l) for l in source], [len(l) for l in target]],label=[&#39;source&#39;, &#39;target&#39;])d2l.plt.legend(loc=&#39;upper right&#39;);</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/t19DZi4dB2XOPEn.png" alt="分词结果"></p><h3 id="建立词典"><a href="#建立词典" class="headerlink" title="建立词典"></a>建立词典</h3><p>单词组成的列表—-单词id组成的列表</p><pre><code class="lang-python">def build_vocab(tokens):    tokens = [token for line in tokens for token in line]    return d2l.data.base.Vocab(tokens, min_freq=3, use_special_tokens=True)src_vocab = build_vocab(source)  # 4 个特殊字符，pad 补充 bos句开始 eos句结束 unk未知字符len(src_vocab)</code></pre><pre><code>3789</code></pre><p><code>Vocab</code> 会进行词频统计, 从大到小排序<br><code>__len__</code> 返回长度<br><code>__getitem__</code> 返回单词对应的 id<br><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/3oBvEZdHYKXzwfg.png" alt="Vocab"></p><h3 id="载入数据集"><a href="#载入数据集" class="headerlink" title="载入数据集"></a>载入数据集</h3><p>进行 pad 保证句子长度是一样的, 一个 batch 使用的同一个长度</p><pre><code class="lang-python">def pad(line, max_len, padding_token):    if len(line) &gt; max_len:        return line[:max_len]    return line + [padding_token] * (max_len - len(line))pad(src_vocab[source[0]], 10, src_vocab.pad)</code></pre><pre><code>[38, 4, 0, 0, 0, 0, 0, 0, 0, 0]</code></pre><p>首尾标注 <code>bos</code> 和 <code>eos</code></p><pre><code class="lang-python">def build_array(lines, vocab, max_len, is_source):    lines = [vocab[line] for line in lines]    if not is_source:  # 如果是目标数据集，就要加一对首尾标注        lines = [[vocab.bos] + line + [vocab.eos] for line in lines]    array = torch.tensor([pad(line, max_len, vocab.pad) for line in lines])    valid_len = (array != vocab.pad).sum(1) #第一个维度    return array, valid_len</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/LW4YMRvUpCA9ul6.png" alt="TensorDataset"></p><pre><code class="lang-python">def load_data_nmt(batch_size, max_len): # This function is saved in d2l.    src_vocab, tgt_vocab = build_vocab(source), build_vocab(target)  # 先进行分词    src_array, src_valid_len = build_array(source, src_vocab, max_len, True)  # 构建源语言数组    tgt_array, tgt_valid_len = build_array(target, tgt_vocab, max_len, False) # 构建目标语言数组    train_data = data.TensorDataset(src_array, src_valid_len, tgt_array, tgt_valid_len)     # TensorDataset 利用 assert 判断 4 个参数是不是一一对应的    train_iter = data.DataLoader(train_data, batch_size, shuffle=True)    return src_vocab, tgt_vocab, train_iter</code></pre><p>X 是英语句子, Y 是法语句子</p><pre><code class="lang-python">src_vocab, tgt_vocab, train_iter = load_data_nmt(batch_size=2, max_len=8)for X, X_valid_len, Y, Y_valid_len, in train_iter:    print(&#39;X =&#39;, X.type(torch.int32), &#39;\nValid lengths for X =&#39;, X_valid_len,        &#39;\nY =&#39;, Y.type(torch.int32), &#39;\nValid lengths for Y =&#39;, Y_valid_len)    break</code></pre><pre><code>X = tensor([[   5,   24,    3,    4,    0,    0,    0,    0],        [  12, 1388,    7,    3,    4,    0,    0,    0]], dtype=torch.int32) Valid lengths for X = tensor([4, 5]) Y = tensor([[   1,   23,   46,    3,    3,    4,    2,    0],        [   1,   15,  137,   27, 4736,    4,    2,    0]], dtype=torch.int32) Valid lengths for Y = tensor([7, 7])</code></pre><h1 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a>Encoder-Decoder</h1><p> encoder：输入到隐藏状态<br> decoder：隐藏状态到输出</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/7aEyZcGDVS1zXPU.png" alt="Encoder-Decoder"></p><pre><code class="lang-python">class Encoder(nn.Module):    def __init__(self, **kwargs):        super(Encoder, self).__init__(**kwargs)    def forward(self, X, *args):        raise NotImplementedError</code></pre><pre><code class="lang-python">class Decoder(nn.Module):    def __init__(self, **kwargs):        super(Decoder, self).__init__(**kwargs)    def init_state(self, enc_outputs, *args):        raise NotImplementedError    def forward(self, X, state):        raise NotImplementedError</code></pre><pre><code class="lang-python">class EncoderDecoder(nn.Module):    def __init__(self, encoder, decoder, **kwargs):        super(EncoderDecoder, self).__init__(**kwargs)        self.encoder = encoder        self.decoder = decoder    def forward(self, enc_X, dec_X, *args):        enc_outputs = self.encoder(enc_X, *args)        dec_state = self.decoder.init_state(enc_outputs, *args)        return self.decoder(dec_X, dec_state)</code></pre><p>可以应用在对话系统、生成式任务中。</p><h1 id="Sequence-to-Sequence模型"><a href="#Sequence-to-Sequence模型" class="headerlink" title="Sequence to Sequence模型"></a>Sequence to Sequence模型</h1><h3 id="模型："><a href="#模型：" class="headerlink" title="模型："></a>模型：</h3><ul><li>训练（已知英语所对应的法语）</li><li>Encoder 是一个 RNN，可以是 LSTM 可以是 GRU</li><li>hidden state 就是 Encoder 得到的 ht，就是语义编码，也就是 Decoder 用来初始化的 $H_{-1}$</li><li>Decoder 就是一个生成语言模型，有了前几个生成后几个，类似于歌词生成<br><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/JL3VW7gznsGIStw.png" alt="生成"></li><li>预测（不知道对应的法语）</li></ul><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/YhcpTl6nQERLosu.png" alt="预测"></p><h3 id="具体结构："><a href="#具体结构：" class="headerlink" title="具体结构："></a>具体结构：</h3><ul><li>比如 apple 对应的 id，会把它翻译成<code>词向量</code>，代表 apple 的含义</li><li><code>dense()</code>多用于CNN网络搭建时搭建全连接层使用，在RNN网络搭建，特别是涉及到用RNN做分类时，dense()可以用作RNN<code>分类预测输出</code>, 每个输出也是一个分类问题</li><li><code>embedding层</code>的作用就是给每个单词，赋一个特定的词向量<br><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/5otbRSFCVqhZDUK.png" alt="具体结构"><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3></li></ul><pre><code class="lang-python">class Seq2SeqEncoder(d2l.Encoder):    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,                 dropout=0, **kwargs):        super(Seq2SeqEncoder, self).__init__(**kwargs)        self.num_hiddens=num_hiddens        self.num_layers=num_layers        self.embedding = nn.Embedding(vocab_size, embed_size) # embed_size 词向量维度，输入单词的形状        self.rnn = nn.LSTM(embed_size,num_hiddens, num_layers, dropout=dropout)    def begin_state(self, batch_size, device):  # 初始化        return [torch.zeros(size=(self.num_layers, batch_size, self.num_hiddens),  device=device),                torch.zeros(size=(self.num_layers, batch_size, self.num_hiddens),  device=device)]    def forward(self, X, *args):        X = self.embedding(X) # X shape: (batch_size, seq_len, embed_size)         # batch_size 有几句话，seq_len 一句话有几个单词，每个单词都变成了 embed_size 维的词向量        X = X.transpose(0, 1)  # RNN 第一个维度应该是时间（句子的顺序是第一个维度），所以把第 1、2 维度调换一下        # state = self.begin_state(X.shape[1], device=X.device)        out, state = self.rnn(X) # out 是每个单元的输出，state 是最后一个单元的输出        # out 的 shape 是 (seq_len, batch_size, num_hiddens).        # state 包含最后一个记忆细胞的状态，和隐层状态        # 在最后一步， state shape 是 (num_layers, batch_size, num_hiddens)        return out, state</code></pre><pre><code class="lang-python">encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8,num_hiddens=16, num_layers=2) #每个词用 8 维向量表示X = torch.zeros((4, 7),dtype=torch.long)  # 输入 4 句话，每句话 7 个单词output, state = encoder(X)output.shape, len(state), state[0].shape, state[1].shape  # len(state)=2 表示包含了记忆细胞和隐层状态两个部分</code></pre><pre><code>(torch.Size([7, 4, 16]), 2, torch.Size([2, 4, 16]), torch.Size([2, 4, 16]))</code></pre><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><pre><code class="lang-python">class Seq2SeqDecoder(d2l.Decoder):    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,                 dropout=0, **kwargs):        super(Seq2SeqDecoder, self).__init__(**kwargs)        self.embedding = nn.Embedding(vocab_size, embed_size)        self.rnn = nn.LSTM(embed_size,num_hiddens, num_layers, dropout=dropout)        self.dense = nn.Linear(num_hiddens,vocab_size) # dense 用于输出结果        # 通过全连接层把16个隐藏单元映射到100个单词，在100 维度的向量里，选出评分最高的，作为结果输出    def init_state(self, enc_outputs, *args):        return enc_outputs[1]  # 用 Encoder 的输出作初始化    def forward(self, X, state):        X = self.embedding(X).transpose(0, 1)        out, state = self.rnn(X, state)        # Make the batch to be the first dimension to simplify loss computation.        out = self.dense(out).transpose(0, 1)        return out, state</code></pre><pre><code class="lang-python">decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8,num_hiddens=16, num_layers=2)state = decoder.init_state(encoder(X))out, state = decoder(X, state)out.shape, len(state), state[0].shape, state[1].shape  # out 4*7*10 10 代表单词表大小，在 10 个得分里选最高的</code></pre><pre><code>(torch.Size([4, 7, 10]), 2, torch.Size([2, 4, 16]), torch.Size([2, 4, 16]))</code></pre><h3 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h3><p>计算有效长度, 忽略 padding</p><pre><code class="lang-python">def SequenceMask(X, X_len,value=0):      # X 一个 batch 的输入，X_len 输入原本的有效长度（由于有 pad）    maxlen = X.size(1)    mask = torch.arange(maxlen)[None, :].to(X_len.device) &lt; X_len[:, None] # len 以后的部分都用 value 填充    X[~mask]=value    return X</code></pre><pre><code class="lang-python">X = torch.tensor([[1,2,3], [4,5,6]])SequenceMask(X,torch.tensor([1,2]))  # 第一句有效长度为 1，第二句有效长度为 2</code></pre><pre><code>tensor([[1, 0, 0],        [4, 5, 0]])</code></pre><pre><code class="lang-python">X = torch.ones((2,3, 4))SequenceMask(X, torch.tensor([1,2]),value=-1)</code></pre><pre><code>tensor([[[ 1.,  1.,  1.,  1.],         [-1., -1., -1., -1.],         [-1., -1., -1., -1.]],        [[ 1.,  1.,  1.,  1.],         [ 1.,  1.,  1.,  1.],         [-1., -1., -1., -1.]]])</code></pre><p>定义交叉熵损失函数, 继承<code>CrossEntropyLoss</code></p><pre><code class="lang-python">class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):    # pred shape: (batch_size, seq_len, vocab_size)    # label shape: (batch_size, seq_len)    # valid_length shape: (batch_size, )    def forward(self, pred, label, valid_length):        # the sample weights shape should be (batch_size, seq_len)        weights = torch.ones_like(label)        weights = SequenceMask(weights, valid_length).float()  # padding 部分的损失变成 0，保留有效长度部分的存世        self.reduction=&#39;none&#39;        output=super(MaskedSoftmaxCELoss, self).forward(pred.transpose(1,2), label)        return (output*weights).mean(dim=1)</code></pre><pre><code class="lang-python">loss = MaskedSoftmaxCELoss()loss(torch.ones((3, 4, 10)), torch.ones((3,4),dtype=torch.long), torch.tensor([4,3,0])) # label 3个句子，4 个正确单词； 有效长度[4, 3, 0]</code></pre><pre><code>tensor([2.3026, 1.7269, 0.0000])</code></pre><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><pre><code class="lang-python">def train_ch7(model, data_iter, lr, num_epochs, device):  # Saved in d2l 如果device是GPU，下面的device都要设定    model.to(device)    optimizer = optim.Adam(model.parameters(), lr=lr)    loss = MaskedSoftmaxCELoss()    tic = time.time()    for epoch in range(1, num_epochs+1):        l_sum, num_tokens_sum = 0.0, 0.0  # loss 的总和，单词数量的总和        for batch in data_iter:            optimizer.zero_grad()            X, X_vlen, Y, Y_vlen = [x.to(device) for x in batch]            Y_input, Y_label, Y_vlen = Y[:,:-1], Y[:,1:], Y_vlen-1 # Y_input decoder的输入            Y_hat, _ = model(X, Y_input, X_vlen, Y_vlen)            l = loss(Y_hat, Y_label, Y_vlen).sum()            l.backward()            with torch.no_grad():                d2l.grad_clipping_nn(model, 5, device)  #梯度裁剪            num_tokens = Y_vlen.sum().item()            optimizer.step()            l_sum += l.sum().item()            num_tokens_sum += num_tokens        if epoch % 50 == 0:            print(&quot;epoch {0:4d},loss {1:.3f}, time {2:.1f} sec&quot;.format(                   epoch, (l_sum/num_tokens_sum), time.time()-tic))            tic = time.time()</code></pre><pre><code class="lang-python">embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.0batch_size, num_examples, max_len = 64, 1e3, 10lr, num_epochs, ctx = 0.005, 300, d2l.try_gpu()src_vocab, tgt_vocab, train_iter = d2l.load_data_nmt(    batch_size, max_len,num_examples)encoder = Seq2SeqEncoder(    len(src_vocab), embed_size, num_hiddens, num_layers, dropout)decoder = Seq2SeqDecoder(    len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)model = d2l.EncoderDecoder(encoder, decoder)train_ch7(model, train_iter, lr, num_epochs, ctx)</code></pre><pre><code>epoch   50,loss 0.093, time 38.2 secepoch  100,loss 0.046, time 37.9 secepoch  150,loss 0.032, time 36.8 secepoch  200,loss 0.027, time 37.5 secepoch  250,loss 0.026, time 37.8 secepoch  300,loss 0.025, time 37.3 sec</code></pre><h3 id="测试-1"><a href="#测试-1" class="headerlink" title="测试"></a>测试</h3><pre><code class="lang-python">def translate_ch7(model, src_sentence, src_vocab, tgt_vocab, max_len, device):    src_tokens = src_vocab[src_sentence.lower().split(&#39; &#39;)]    src_len = len(src_tokens)    if src_len &lt; max_len:        src_tokens += [src_vocab.pad] * (max_len - src_len)  # 进行 padding    enc_X = torch.tensor(src_tokens, device=device)    enc_valid_length = torch.tensor([src_len], device=device)    # use expand_dim to add the batch_size dimension.    enc_outputs = model.encoder(enc_X.unsqueeze(dim=0), enc_valid_length) # unsqueeze 增加一个维度（因为有多个 batch_size)    dec_state = model.decoder.init_state(enc_outputs, enc_valid_length) # encoder 的输出作为 dec 的输入初始化    dec_X = torch.tensor([tgt_vocab.bos], device=device).unsqueeze(dim=0) # 设为 bos 表示开始    predict_tokens = []    for _ in range(max_len):        Y, dec_state = model.decoder(dec_X, dec_state) # 当前 RNN 单元的 output Y，给下一个单元的 dec_state        dec_X = Y.argmax(dim=2)  # 得分最高的单词，作为下一个单元的输入        py = dec_X.squeeze(dim=0).int().item()        if py == tgt_vocab.eos:  # 结束则跳出循环            break        predict_tokens.append(py)    return &#39; &#39;.join(tgt_vocab.to_tokens(predict_tokens))</code></pre><pre><code class="lang-python">for sentence in [&#39;Go .&#39;, &#39;Wow !&#39;, &quot;I&#39;m OK .&quot;, &#39;I won !&#39;]:    print(sentence + &#39; =&gt; &#39; + translate_ch7(        model, sentence, src_vocab, tgt_vocab, max_len, ctx))</code></pre><pre><code>Go . =&gt; va !Wow ! =&gt; &lt;unk&gt; !I&#39;m OK . =&gt; ça va .I won ! =&gt; j&#39;ai gagné !</code></pre><h2 id="Beam-Search-集束搜索"><a href="#Beam-Search-集束搜索" class="headerlink" title="Beam Search 集束搜索"></a>Beam Search 集束搜索</h2><p>简单greedy search：（贪心）只考虑了局部最优解</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/emJlxcIbrCVTS1o.png" alt="简单贪心"></p><p>维特比算法：选择整体分数最高的句子（搜索空间太大）<br>集束搜索：每次不只选一个，选择 top n，增加备选项但又不全局搜索</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/L5aI1irfAKm2znq.png" alt="集束搜索"></p><h2 id="练习题"><a href="#练习题" class="headerlink" title="练习题"></a>练习题</h2><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/BAHTqRNlPsiMdyr.png" alt="机器翻译-练习题"></p><ul><li>训练时下一个单词是已知的, 预测时需要作为下一个单词生成的输入</li><li>每个 batch 之内, size 是给定的, 句子长度都是一样的</li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动手深度学习(七):循环神经网络 RNN 基础</title>
      <link href="/2020/02/19/dl-notes7-rnn-basic/"/>
      <url>/2020/02/19/dl-notes7-rnn-basic/</url>
      
        <content type="html"><![CDATA[<h1 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h1><p>本节介绍循环神经网络，下图展示了如何基于循环神经网络实现语言模型。我们的目的是基于当前的输入与过去的输入序列，预测序列的下一个字符。循环神经网络引入一个隐藏变量$H$，用$H_{t}$表示$H$在时间步$t$的值。$H_{t}$的计算基于$X_{t}$和$H_{t-1}$，可以认为$H_{t}$记录了到当前字符为止的序列信息，利用$H_{t}$对序列的下一个字符进行预测。</p><h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><p>W_xh: 状态-输入权重<br>W_hh: 状态-状态权重<br>W_hq: 状态-输出权重<br>b_h: 隐藏层的偏置<br>b_q: 输出层的偏置</p><h3 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h3><ul><li>循环神经网络 <code>RNN</code> 的参数就是上述的三个权重和两个偏置，并且在沿着时间训练（参数的更新），参数的数量没有发生变化，仅仅是上述的参数的值在更新。循环神经网络可以看作是沿着时间维度上的权值共享</li><li>卷积神经网络 <code>CNN</code> 中，一个卷积核通过在特征图上滑动进行卷积，是空间维度的权值共享。在卷积神经网络中通过控制特征图的数量来控制每一层模型的复杂度，而循环神经网络是通过控制W_xh和W_hh中h的维度来控制模型的复杂度。</li></ul><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://s2.ax1x.com/2020/02/19/3VcEfH.png" alt="RNN 模型"></p><h2 id="一个batch的数据的表示"><a href="#一个batch的数据的表示" class="headerlink" title="一个batch的数据的表示"></a>一个batch的数据的表示</h2><ul><li>如何将一个batch的数据转换成时间步数个（批量大小，词典大小）的矩阵？</li><li>每个字符都是一个词典大小的向量(<code>one-hot 表示法</code>)，每个样本是时间步数个序列，每个batch是批量大小个样本</li><li>第一个{批量大小，词典大小}的矩阵：取出一个批量样本中<code>每个序列的第一个字符</code>，并将每个字符展开成词典大小的向量，就形成了第一个时间步所表示的矩阵</li><li>第二个{批量大小，词典大小}的矩阵：取出一个批量样本中<code>每个序列的第二个字符</code>，并将每个字符展开成词典大小的向量，就形成了第二个时间步所表示的矩阵</li><li>最后就形成了时间步个{批量大小，词典大小}的矩阵. 1个 batch = <code>step * batch_size * vocab_size</code> ，这也就是每个batch最后的形式</li></ul><h2 id="隐藏状态的初始化"><a href="#隐藏状态的初始化" class="headerlink" title="隐藏状态的初始化"></a>隐藏状态的初始化</h2><ul><li>随机采样时：每次迭代都需要重新初始化隐藏状态（每个epoch有很多词迭代，每次迭代都需要进行初始化，因为对于随机采样的样本中只有一个批量内的数据是连续的）</li><li>相邻采样时：如果是相邻采样，则说明前后两个batch的数据是连续的，所以在训练每个batch的时候只需要更新一次（也就是说模型在一个epoch中的迭代不需要重新初始化隐藏状态）</li></ul><h2 id="循环神经网络的构造"><a href="#循环神经网络的构造" class="headerlink" title="循环神经网络的构造"></a>循环神经网络的构造</h2><p>我们先看循环神经网络的具体构造。假设$\boldsymbol{X}_t \in \mathbb{R}^{n \times d}$是时间步$t$的小批量输入（其批量大小为 𝑛 ，输入个数为 𝑑），$\boldsymbol{H}_t  \in \mathbb{R}^{n \times h}$是该时间步的隐藏变量，则：</p><script type="math/tex; mode=display">\boldsymbol{H}_t = \phi(\boldsymbol{X}_t \boldsymbol{W}_{xh} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{hh}  + \boldsymbol{b}_h).</script><p>其中，$\boldsymbol{W}_{xh} \in \mathbb{R}^{d \times h}$，$\boldsymbol{W}_{hh} \in \mathbb{R}^{h \times h}$，$\boldsymbol{b}_{h} \in \mathbb{R}^{1 \times h}$，$\phi$函数是<code>非线性激活函数</code>。由于引入了$\boldsymbol{H}_{t-1} \boldsymbol{W}_{hh}$，$H_{t}$能够捕捉截至当前时间步的序列的历史信息，就像是神经网络当前时间步的状态或记忆一样。由于$H_{t}$的计算基于$H_{t-1}$，上式的计算是循环的，使用循环计算的网络即<code>循环神经网络</code>（recurrent neural network）。</p><p>在时间步$t$，输出层的输出为：</p><script type="math/tex; mode=display">\boldsymbol{O}_t = \boldsymbol{H}_t \boldsymbol{W}_{hq} + \boldsymbol{b}_q.</script><p>其中$\boldsymbol{W}_{hq} \in \mathbb{R}^{h \times q}$，$\boldsymbol{b}_q \in \mathbb{R}^{1 \times q}$。<br>（h 是由上一层隐藏层的神经元个数决定的，q 是自己的输出个数）</p><h2 id="从零开始实现循环神经网络"><a href="#从零开始实现循环神经网络" class="headerlink" title="从零开始实现循环神经网络"></a>从零开始实现循环神经网络</h2><p>我们先尝试从零开始实现一个基于字符级循环神经网络的语言模型，这里我们使用周杰伦的歌词作为语料，首先我们读入数据：</p><pre><code class="lang-python">import torchimport torch.nn as nnimport timeimport mathimport syssys.path.append(&quot;/home/kesci/input&quot;)import d2l_jay9460 as d2l(corpus_indices, char_to_idx, idx_to_char, vocab_size) = d2l.load_data_jay_lyrics()device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)</code></pre><h3 id="one-hot向量"><a href="#one-hot向量" class="headerlink" title="one-hot向量"></a>one-hot向量</h3><p>我们需要将字符表示成向量，这里采用one-hot向量。假设词典大小是$N$，每次字符对应一个从$0$到$N-1$的唯一的索引，则该字符的向量是一个长度为$N$的向量，若字符的索引是$i$，则该向量的第$i$个位置为$1$，其他位置为$0$。下面分别展示了索引为0和2的one-hot向量，向量长度等于词典大小 vocab_size。</p><pre><code class="lang-python">def one_hot(x, n_class, dtype=torch.float32):    result = torch.zeros(x.shape[0], n_class, dtype=dtype, device=x.device)  # shape: (n, n_class)    result.scatter_(1, x.long().view(-1, 1), 1)  # result[i, x[i, 0]] = 1    return result# one-hot 每个人携带整个序列，但是只标记自己print(vocab_size)x = torch.tensor([0, 2])x_one_hot = one_hot(x, vocab_size)print(x_one_hot)print(x_one_hot.shape)print(x_one_hot.sum(axis=1))</code></pre><pre><code>1027tensor([[1., 0., 0.,  ..., 0., 0., 0.],        [0., 0., 1.,  ..., 0., 0., 0.]])torch.Size([2, 1027])tensor([1., 1.])</code></pre><p>我们每次采样的小批量的形状是（批量大小, 时间步数）。下面的函数将这样的小批量变换成数个形状为（批量大小, 词典大小）的矩阵，矩阵个数等于时间步数。也就是说，时间步$t$的输入为$\boldsymbol{X}_t \in \mathbb{R}^{n \times d}$，其中$n$为批量大小，$d$为词向量大小，即one-hot向量长度（词典大小）。</p><pre><code class="lang-python">def to_onehot(X, n_class):      # X 相当于小批量，批量大小即行数n，时间步数即列数d, n_class 相当于字典大小，即单词个数 vocab_size    # 每次传给 one_hot 参数，X[:, i]表示第 i 列所有行，每次两行，0/5, 1/6... n_class 即 vocab_size    # 对每一列进行 one_hot, 最后返回的一个三维 tensor    # one_hot(X[:, i] 为 [tensor([0, 5]), tensor([1, 6]), tensor([2, 7]), tensor([3, 8]), tensor([4, 9])]    return [one_hot(X[:, i], n_class) for i in range(X.shape[1])]X = torch.arange(10).view(2, 5) #此处批量大小为 2，时间步数为 5inputs = to_onehot(X, vocab_size)print(X)print(inputs)# 上面是一行一组x = torch.tensor([0, 2]), x只有一行，这里是两个一组，因为 X 有两行</code></pre><h3 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><pre><code class="lang-python">num_inputs, num_hiddens, num_outputs = vocab_size, 256, vocab_size# num_inputs: d# num_hiddens: h, 隐藏单元的个数256, 是超参数# num_outputs: qdef get_params():    def _one(shape): # 对给定的 shape 进行随机初始化        param = torch.zeros(shape, device=device, dtype=torch.float32)        nn.init.normal_(param, 0, 0.01)        return torch.nn.Parameter(param)    # 隐藏层参数    W_xh = _one((num_inputs, num_hiddens))    W_hh = _one((num_hiddens, num_hiddens))    b_h = torch.nn.Parameter(torch.zeros(num_hiddens, device=device))    # 输出层参数    W_hq = _one((num_hiddens, num_outputs))    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device))    return (W_xh, W_hh, b_h, W_hq, b_q)</code></pre><h3 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h3><p>函数<code>rnn</code>用循环的方式依次完成循环神经网络每个时间步的计算。</p><pre><code class="lang-python">def rnn(inputs, state, params):    # inputs和outputs皆为num_steps个形状为(batch_size, vocab_size)的矩阵    W_xh, W_hh, b_h, W_hq, b_q = params    H, = state   # 状态可能有多个，先定义成一个元组    outputs = []    for X in inputs:        H = torch.tanh(torch.matmul(X, W_xh) + torch.matmul(H, W_hh) + b_h)  # 根据公式        Y = torch.matmul(H, W_hq) + b_q        outputs.append(Y)    return outputs, (H,)   # 可能会使用相邻采样，当前的状态(H,)就可以作为下一个状态的初始值</code></pre><p>函数init_rnn_state初始化隐藏变量，这里的返回值是一个元组。</p><pre><code class="lang-python">def init_rnn_state(batch_size, num_hiddens, device):    return (torch.zeros((batch_size, num_hiddens), device=device), )</code></pre><p>做个简单的测试来观察输出结果的个数（时间步数），以及第一个时间步的输出层输出的形状和隐藏状态的形状。</p><pre><code class="lang-python"># print(X.shape)  # 输入批量，批量大小是 2，时间步数是 5# print(num_hiddens) # 隐藏单元个数 256# print(vocab_size)  # 字典大小 1027state = init_rnn_state(X.shape[0], num_hiddens, device)  # 初始化状态inputs = to_onehot(X.to(device), vocab_size)params = get_params()outputs, state_new = rnn(inputs, state, params)  # 把 one_hot 形式的 X 作为 inputs 输入进去# print(len(inputs), inputs[0].shape)   # 5 torch.Size([2, 1027])# print(len(outputs), outputs[0].shape) # 5 torch.Size([2, 1027])# print(len(state), state[0].shape)     # 1 torch.Size([2, 256])# print(len(state_new), state_new[0].shape) # 1 torch.Size([2, 256])  都是 2 行 256 列</code></pre><h3 id="裁剪梯度"><a href="#裁剪梯度" class="headerlink" title="裁剪梯度"></a>裁剪梯度</h3><p>循环神经网络中较容易出现梯度衰减或梯度爆炸，这会导致网络几乎无法训练。<code>裁剪梯度</code>（clip gradient）是一种应对<code>梯度爆炸</code>的方法。假设我们把所有模型参数的梯度拼接成一个向量 $\boldsymbol{g}$，并设裁剪的阈值是$\theta$。裁剪后的梯度</p><script type="math/tex; mode=display"> \min\left(\frac{\theta}{\|\boldsymbol{g}\|}, 1\right)\boldsymbol{g}</script><p>的$L_2$范数不超过$\theta$。</p><pre><code class="lang-python">def grad_clipping(params, theta, device):    norm = torch.tensor([0.0], device=device)    for param in params:        norm += (param.grad.data ** 2).sum()    norm = norm.sqrt().item()    if norm &gt; theta:  # 如果不够 1，就乘上一个数        for param in params:            param.grad.data *= (theta / norm)</code></pre><h3 id="定义预测函数"><a href="#定义预测函数" class="headerlink" title="定义预测函数"></a>定义预测函数</h3><p>以下函数基于前缀<code>prefix</code>（含有数个字符的字符串）来预测接下来的<code>num_chars</code>个字符。这个函数稍显复杂，其中我们将循环神经单元<code>rnn</code>设置成了函数参数，这样在后面小节介绍其他循环神经网络时能重复使用这个函数。</p><pre><code class="lang-python">def predict_rnn(prefix, num_chars, rnn, params, init_rnn_state,  # 给定 prefix，预测接下来的 num_chars                num_hiddens, vocab_size, device, idx_to_char, char_to_idx):    state = init_rnn_state(1, num_hiddens, device)    output = [char_to_idx[prefix[0]]]   # output记录prefix加上预测的num_chars个字符    for t in range(num_chars + len(prefix) - 1):        # 将上一时间步的输出作为当前时间步的输入        X = to_onehot(torch.tensor([[output[-1]]], device=device), vocab_size)        # 计算输出和更新隐藏状态        (Y, state) = rnn(X, state, params)        # 下一个时间步的输入是prefix里的字符(还在初始态）, 或者当前的最佳预测字符（计算出了 Y）        if t &lt; len(prefix) - 1:            output.append(char_to_idx[prefix[t + 1]])        else:            output.append(Y[0].argmax(dim=1).item())    return &#39;&#39;.join([idx_to_char[i] for i in output])</code></pre><p>我们先测试一下<code>predict_rnn</code>函数。我们将根据前缀“分开”创作长度为10个字符（不考虑前缀长度）的一段歌词。因为模型参数为随机值，所以预测结果也是随机的。</p><pre><code class="lang-python">predict_rnn(&#39;分开&#39;, 10, rnn, params, init_rnn_state, num_hiddens, vocab_size,            device, idx_to_char, char_to_idx)</code></pre><pre><code>&#39;分开潮养每霜敌刺母张暴欢&#39;</code></pre><h3 id="困惑度"><a href="#困惑度" class="headerlink" title="困惑度"></a>困惑度</h3><p>我们通常使用<code>困惑度</code>（perplexity）来评价语言模型的好坏。回忆一下“softmax回归”一节中交叉熵损失函数的定义。困惑度是对交叉熵损失函数做指数运算后得到的值。特别地，</p><ul><li><code>最佳</code>情况下，模型总是把标签类别的概率预测为1，此时困惑度为1；  （困惑度就是预测概率的倒数）</li><li><code>最坏</code>情况下，模型总是把标签类别的概率预测为0，此时困惑度为正无穷；</li><li><code>基线</code>情况下，模型总是预测所有类别的概率都相同，此时困惑度为类别个数。</li></ul><p>显然，任何一个有效模型的困惑度必须小于类别个数。在本例中，困惑度必须<code>小于</code>词典大小<code>vocab_size</code>。</p><h3 id="定义模型训练函数"><a href="#定义模型训练函数" class="headerlink" title="定义模型训练函数"></a>定义模型训练函数</h3><p>跟之前章节的模型训练函数相比，这里的模型训练函数有以下几点不同：</p><ol><li>使用困惑度评价模型。</li><li>在迭代模型参数前裁剪梯度。</li><li>对时序数据采用不同采样方法将导致隐藏状态初始化的不同。</li></ol><pre><code class="lang-python">def train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,                          vocab_size, device, corpus_indices, idx_to_char,                          char_to_idx, is_random_iter, num_epochs, num_steps,                          lr, clipping_theta, batch_size, pred_period,                          pred_len, prefixes):    if is_random_iter:        data_iter_fn = d2l.data_iter_random  # 随机采样    else:        data_iter_fn = d2l.data_iter_consecutive # 相邻采样    params = get_params()    loss = nn.CrossEntropyLoss()    for epoch in range(num_epochs):        if not is_random_iter:  # 如使用相邻采样，在epoch开始时初始化隐藏状态            state = init_rnn_state(batch_size, num_hiddens, device)        l_sum, n, start = 0.0, 0, time.time()        data_iter = data_iter_fn(corpus_indices, batch_size, num_steps, device)        for X, Y in data_iter:            if is_random_iter:  # 如使用随机采样，在每个小批量更新前初始化隐藏状态                state = init_rnn_state(batch_size, num_hiddens, device)            else:  # 否则需要使用detach函数从计算图分离隐藏状态                for s in state:                    s.detach_()  # detach_()将返回的向量从创建它的计算图中分离出来，后面就不再对它求梯度了            # inputs是num_steps个形状为(batch_size, vocab_size)的矩阵            inputs = to_onehot(X, vocab_size)            # outputs有num_steps个形状为(batch_size, vocab_size)的矩阵            (outputs, state) = rnn(inputs, state, params)  # 前向计算            # 拼接之后形状为(num_steps * batch_size, vocab_size)            outputs = torch.cat(outputs, dim=0)            # Y的形状是(batch_size, num_steps)，转置后再变成形状为            # (num_steps * batch_size,)的向量，这样跟输出的行一一对应            y = torch.flatten(Y.T)            # 使用交叉熵损失计算平均分类误差            l = loss(outputs, y.long())            # 梯度清0            if params[0].grad is not None:                for param in params:                    param.grad.data.zero_()            l.backward()            grad_clipping(params, clipping_theta, device)  # 裁剪梯度            d2l.sgd(params, lr, 1)  # 因为误差已经取过均值，梯度不用再做平均            l_sum += l.item() * y.shape[0]            n += y.shape[0]        if (epoch + 1) % pred_period == 0:            print(&#39;epoch %d, perplexity %f, time %.2f sec&#39; % (                epoch + 1, math.exp(l_sum / n), time.time() - start))            for prefix in prefixes:                print(&#39; -&#39;, predict_rnn(prefix, pred_len, rnn, params, init_rnn_state,                    num_hiddens, vocab_size, device, idx_to_char, char_to_idx))</code></pre><h3 id="训练模型并创作歌词"><a href="#训练模型并创作歌词" class="headerlink" title="训练模型并创作歌词"></a>训练模型并创作歌词</h3><p>现在我们可以训练模型了。首先，设置模型超参数。我们将根据前缀“分开”和“不分开”分别创作长度为50个字符（不考虑前缀长度）的一段歌词。我们每过50个迭代周期便根据当前训练的模型创作一段歌词。</p><pre><code class="lang-python">num_epochs, num_steps, batch_size, lr, clipping_theta = 250, 35, 32, 1e2, 1e-2pred_period, pred_len, prefixes = 50, 50, [&#39;分开&#39;, &#39;不分开&#39;]</code></pre><p>下面采用随机采样训练模型并创作歌词。</p><pre><code class="lang-python">train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,                      vocab_size, device, corpus_indices, idx_to_char,                      char_to_idx, True, num_epochs, num_steps, lr,                      clipping_theta, batch_size, pred_period, pred_len,                      prefixes)</code></pre><pre><code>epoch 50, perplexity 71.093786, time 0.76 sec - 分开 我不要再想你 我知要这 我的让我心能 不要我 别子我 别子我 别子我 别子我 别子我 别子我 别子 - 不分开 我不要这生你 我知想这 我的让我心能 不要我 别子我 别子我 别子我 别子我 别子我 别子我 别子epoch 100, perplexity 9.897621, time 0.71 sec - 分开 我想想好生活 不知 是你 却不 痛箭  穿什么 如果我遇起头 有话去对医药 说哼 我不 我不 我不 - 不分开永 我不想再想 我不 我不 我不要 爱情走的太快就像龙卷风 不能不让我疯狂的可爱女人 坏坏的让我疯狂epoch 150, perplexity 2.847206, time 0.77 sec - 分开 还默法 一步两步三步四步望著天 看星星 一颗两颗三颗四颗 连成线背著背默默许下心愿 看远方的星是否 - 不分开吗 我不能再想 我不 我不 我不要 爱情走的太快就像龙卷风 不能承受我已无处可躲 我不想再想 我不能epoch 200, perplexity 1.564534, time 0.72 sec - 分开 有想都 教拳脚武术的老板 练铁沙掌 耍杨家枪 在伤子中夫我妈愿知错搞错  爱 用着星的对笑下想通  - 不分开想 我不能再想 我不 我不 我不能 爱情走的太快就像龙卷风 不能承受我已无处可躲 我不要再想 我不要epoch 250, perplexity 1.311809, time 0.72 sec - 分开 那切都上一步四步望透的 为吸走 一颗两颗三颗四颗 连成线背著背 游荡在蓝安排的雨 随时准备来袭 我 - 不分开简简的胖女巫 用拉丁文念咒语啦啦呜 她养的黑猫笑起来像哭 啦啦啦呜 静满村枪 在人笑功 我一定带我妈</code></pre><p>接下来采用相邻采样训练模型并创作歌词。</p><pre><code class="lang-python">train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,                      vocab_size, device, corpus_indices, idx_to_char,                      char_to_idx, False, num_epochs, num_steps, lr,                      clipping_theta, batch_size, pred_period, pred_len,                      prefixes)</code></pre><pre><code>epoch 50, perplexity 60.294393, time 0.74 sec - 分开 我想要你想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我 - 不分开 我想要你 你有了 别不我的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我epoch 100, perplexity 7.141162, time 0.72 sec - 分开 我已要再爱 我不要再想 我不 我不 我不要再想 我不 我不 我不要 爱情我的见快就像龙卷风 离能开 - 不分开柳 你天黄一个棍 后知哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 epoch 150, perplexity 2.090277, time 0.73 sec - 分开 我已要这是你在著 不想我都做得到 但那个人已经不是我 没有你在 我却多难熬  没有你在我有多难熬多 - 不分开觉 你已经离 我想再好 这样心中 我一定带我 我的完空 不你是风 一一彩纵 在人心中 我一定带我妈走epoch 200, perplexity 1.305391, time 0.77 sec - 分开 我已要这样牵看你的手 它一定实现它一定像现 载著你 彷彿载著阳光 不管到你留都是晴天 蝴蝶自在飞力 - 不分开觉 你已经离开我 不知不觉 我跟了这节奏 后知后觉 又过了一个秋 后知后觉 我该好好生活 我该好好生epoch 250, perplexity 1.230800, time 0.79 sec - 分开 我不要 是你看的太快了悲慢 担心今手身会大早 其么我也睡不着  昨晚梦里你来找 我才  原来我只想 - 不分开觉 你在经离开我 不知不觉 你知了有节奏 后知后觉 后知了一个秋 后知后觉 我该好好生活 我该好好生</code></pre><h2 id="循环神经网络的-pytorch-简洁实现"><a href="#循环神经网络的-pytorch-简洁实现" class="headerlink" title="循环神经网络的 pytorch 简洁实现"></a>循环神经网络的 pytorch 简洁实现</h2><h3 id="定义模型-1"><a href="#定义模型-1" class="headerlink" title="定义模型"></a>定义模型</h3><p>我们使用Pytorch中的<code>nn.RNN</code>来构造循环神经网络。在本节中，我们主要关注<code>nn.RNN</code>的以下几个构造函数参数：</p><ul><li><code>input_size</code> - The number of expected features in the input x</li><li><code>hidden_size</code> – The number of features in the hidden state h</li><li><code>nonlinearity</code> – The non-linearity to use. Can be either ‘tanh’ or ‘relu’. Default: ‘tanh’</li><li><code>batch_first</code> – If True, then the input and output tensors are provided as (batch_size, num_steps, input_size). Default: False</li></ul><p>这里的<code>batch_first</code>决定了输入的形状，我们使用默认的参数<code>False</code>，对应的输入形状是 (num_steps, batch_size, input_size)。</p><p><code>forward</code>函数的参数为：</p><ul><li><code>input</code> of shape (num_steps, batch_size, input_size): tensor containing the features of the input sequence. （三维）</li><li><code>h_0</code> of shape (num_layers * num_directions, batch_size, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1. （<code>h_0</code>对应的是 RNN 中的初始 state）<br>（num_layers主要和深度 RNN 有关，num_directions主要和双向 RNN 有关，此处 <code>num_layers 乘以 num_directions</code> = 1）</li></ul><p><code>forward</code>函数的返回值是：</p><ul><li><code>output</code> of shape (num_steps, batch_size, num_directions * hidden_size): tensor containing the output features (h_t) from the last layer of the RNN, for each t.</li><li><code>h_n</code> of shape (num_layers * num_directions, batch_size, hidden_size): tensor containing the hidden state for t = num_steps. （最后一个时间步，隐藏状态的值）</li></ul><p>现在我们构造一个<code>nn.RNN</code>实例，并用一个简单的例子来看一下输出的形状。</p><pre><code class="lang-python">rnn_layer = nn.RNN(input_size=vocab_size, hidden_size=num_hiddens)num_steps, batch_size = 35, 2X = torch.rand(num_steps, batch_size, vocab_size)state = NoneY, state_new = rnn_layer(X, state)print(Y.shape, state_new.shape)</code></pre><pre><code>torch.Size([35, 2, 256]) torch.Size([1, 2, 256])</code></pre><p>我们定义一个完整的基于循环神经网络的语言模型。</p><pre><code class="lang-python">class RNNModel(nn.Module):    def __init__(self, rnn_layer, vocab_size):        super(RNNModel, self).__init__()        self.rnn = rnn_layer  # 一个 rnn 实例，也可以是一个 LSTM 实例        self.hidden_size = rnn_layer.hidden_size * (2 if rnn_layer.bidirectional else 1)         self.vocab_size = vocab_size        self.dense = nn.Linear(self.hidden_size, vocab_size)  #线性层作为输出层    def forward(self, inputs, state):        # inputs.shape: (batch_size, num_steps)        X = to_onehot(inputs, vocab_size)        X = torch.stack(X)  # X.shape: (num_steps, batch_size, vocab_size)        hiddens, state = self.rnn(X, state)        hiddens = hiddens.view(-1, hiddens.shape[-1])  # hiddens.shape: (num_steps * batch_size, hidden_size)        output = self.dense(hiddens)  # 用Linear线性函数作输出        return output, state</code></pre><p>类似的，我们需要实现一个<code>预测函数</code>，与前面的区别在于前向计算和初始化隐藏状态。</p><pre><code class="lang-python">def predict_rnn_pytorch(prefix, num_chars, model, vocab_size, device, idx_to_char,                      char_to_idx):    state = None    output = [char_to_idx[prefix[0]]]  # output记录prefix加上预测的num_chars个字符    for t in range(num_chars + len(prefix) - 1):        X = torch.tensor([output[-1]], device=device).view(1, 1)        (Y, state) = model(X, state)  # 前向计算不需要传入模型参数        if t &lt; len(prefix) - 1:            output.append(char_to_idx[prefix[t + 1]])        else:            output.append(Y.argmax(dim=1).item())    return &#39;&#39;.join([idx_to_char[i] for i in output])</code></pre><p>使用权重为随机值的模型来预测一次。</p><pre><code class="lang-python">model = RNNModel(rnn_layer, vocab_size).to(device)predict_rnn_pytorch(&#39;分开&#39;, 10, model, vocab_size, device, idx_to_char, char_to_idx)</code></pre><pre><code>&#39;分开老忧忧老钩忧老忧忧老&#39;</code></pre><p>接下来实现<code>训练函数</code>，这里只使用了相邻采样。</p><pre><code class="lang-python">def train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,                                corpus_indices, idx_to_char, char_to_idx,                                num_epochs, num_steps, lr, clipping_theta,                                batch_size, pred_period, pred_len, prefixes):    loss = nn.CrossEntropyLoss()    optimizer = torch.optim.Adam(model.parameters(), lr=lr)    model.to(device)    for epoch in range(num_epochs):        l_sum, n, start = 0.0, 0, time.time()        data_iter = d2l.data_iter_consecutive(corpus_indices, batch_size, num_steps, device) # 相邻采样        state = None        for X, Y in data_iter:            if state is not None:                # 使用detach函数从计算图分离隐藏状态                if isinstance (state, tuple): # LSTM, state:(h, c)                      state[0].detach_()                    state[1].detach_()                else:                     state.detach_()            (output, state) = model(X, state) # output.shape: (num_steps * batch_size, vocab_size)            y = torch.flatten(Y.T)            l = loss(output, y.long())            optimizer.zero_grad()            l.backward()            grad_clipping(model.parameters(), clipping_theta, device)            optimizer.step()            l_sum += l.item() * y.shape[0]            n += y.shape[0]        if (epoch + 1) % pred_period == 0:            print(&#39;epoch %d, perplexity %f, time %.2f sec&#39; % (                epoch + 1, math.exp(l_sum / n), time.time() - start))            for prefix in prefixes:                print(&#39; -&#39;, predict_rnn_pytorch(                    prefix, pred_len, model, vocab_size, device, idx_to_char,                    char_to_idx))</code></pre><p>训练模型。</p><pre><code class="lang-python">num_epochs, batch_size, lr, clipping_theta = 250, 32, 1e-3, 1e-2pred_period, pred_len, prefixes = 50, 50, [&#39;分开&#39;, &#39;不分开&#39;]train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,                            corpus_indices, idx_to_char, char_to_idx,                            num_epochs, num_steps, lr, clipping_theta,                            batch_size, pred_period, pred_len, prefixes)</code></pre><pre><code>epoch 50, perplexity 10.107632, time 0.58 sec - 分开 我不了口  一定 我的走  有一颗四颗 连成线背著背默默许下心愿 在小村外的溪边河口 默默默默 我 - 不分开 我想要你的手 我有多烦恼  说你你你不我  想要你的让我面红的可爱女人 坏坏的让我疯狂的可爱女人 epoch 100, perplexity 1.234571, time 0.56 sec - 分开 我面心事 爱来一 蓝色眼睛的凯萨琳公主 专吃 有AB血型的公老鼠 恍恍惚惚 是谁的脚步 银制茶壶  - 不分开 我没有话不想就多烦恼多难熬  说你再爸不会不 多想就这样没担你 我着你的想活 说想要你的微笑每天都epoch 150, perplexity 1.060467, time 0.57 sec - 分开 我面心事 从来开 我妈妈没有你 我有多烦恼  没有你烦我有多烦恼多难熬  穿过云层 我试著努力向你 - 不分开 了没有多难过 是什么 干什么 我想想和你说你听的让我的听你就到 我想你说你 我不了口让她知道 就是epoch 200, perplexity 1.030212, time 0.64 sec - 分开 我面心外 相来开  家庭你 你打我不要 想  穿什么 你就想一我 你这样打我妈 你跟你 没有 我不 - 不分开 了我不知不要 这样的甜蜜 让我开始想相 命和你当许在这里在听飞就一片 说有一个风慢的老斑鸠 印地安epoch 250, perplexity 1.020210, time 0.60 sec - 分开 我面心仪 相来方 不知道这 杵水伊 坦堡 却只想你和汉堡 我想要你的微笑每天都能看到  我知道这里 - 不分开 了我水也睡活 我想多受就能再这样的节奏 谁都无可奈何 没有你以后 我灵魂失控 黑云在降落 我被它拖</code></pre><h2 id="练习题"><a href="#练习题" class="headerlink" title="练习题"></a>练习题</h2><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://s2.ax1x.com/2020/02/19/3VkgpT.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动手深度学习(六):语言模型</title>
      <link href="/2020/02/19/dl-notes6-lang-model/"/>
      <url>/2020/02/19/dl-notes6-lang-model/</url>
      
        <content type="html"><![CDATA[<h1 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h1><p>一段自然语言文本可以看作是一个离散时间序列，给定一个长度为$T$的词的序列$w_1, w_2, \ldots, w_T$，语言模型的目标就是评估该序列是否合理，即计算该序列的概率：</p><script type="math/tex; mode=display">P(w_1, w_2, \ldots, w_T).</script><p>本节我们介绍基于统计的语言模型，主要是$n$元语法（$n$-gram）。在后续内容中，我们将会介绍基于神经网络的语言模型。</p><h2 id="语言模型-1"><a href="#语言模型-1" class="headerlink" title="语言模型"></a>语言模型</h2><p>假设序列$w_1, w_2, \ldots, w_T$中的每个词是依次生成的，我们有</p><script type="math/tex; mode=display">\begin{aligned}P\left(w_{1}, w_{2}, \ldots, w_{T}\right) &=\prod_{t=1}^{T} P\left(w_{t} | w_{1}, \ldots, w_{t-1}\right) \\&=P\left(w_{1}\right) P\left(w_{2} | w_{1}\right) \cdots P\left(w_{T} | w_{1} w_{2} \cdots w_{T-1}\right)\end{aligned}</script><p>例如，一段含有4个词的文本序列的概率</p><script type="math/tex; mode=display">P(w_1, w_2, w_3, w_4) =  P(w_1) P(w_2 \mid w_1) P(w_3 \mid w_1, w_2) P(w_4 \mid w_1, w_2, w_3).</script><p>语言模型的参数就是词的概率以及给定前几个词情况下的条件概率。设训练数据集为一个大型文本语料库，如维基百科的所有条目，词的概率可以通过该词在训练数据集中的相对词频来计算，例如，$w_1$的概率可以计算为：</p><script type="math/tex; mode=display">\hat P(w_1) = \frac{n(w_1)}{n}</script><p>其中$n(w_1)$为语料库中以$w_1$作为第一个词的文本的数量，$n$为语料库中文本的总数量。</p><p>类似的，给定$w_1$情况下，$w_2$的条件概率可以计算为：</p><script type="math/tex; mode=display">\hat P(w_2 \mid w_1) = \frac{n(w_1, w_2)}{n(w_1)}</script><p>其中$n(w_1, w_2)$为语料库中以$w_1$作为第一个词，$w_2$作为第二个词的文本的数量。</p><h2 id="n元语法"><a href="#n元语法" class="headerlink" title="n元语法"></a>n元语法</h2><p>序列长度增加，计算和存储多个词共同出现的概率的复杂度会呈指数级增加。$n$元语法通过马尔可夫假设简化模型，<code>马尔科夫假设</code>是指一个词的出现<code>只与前面n个词相关</code>，即$n$阶马尔可夫链（Markov chain of order $n$），如果$n=1$，那么有$P(w_3 \mid w_1, w_2) = P(w_3 \mid w_2)$。基于$n-1$阶马尔可夫链，我们可以将语言模型改写为</p><script type="math/tex; mode=display">P(w_1, w_2, \ldots, w_T) = \prod_{t=1}^T P(w_t \mid w_{t-(n-1)}, \ldots, w_{t-1}) .</script><p>以上也叫<code>n元语法</code>（$n$-grams），它是基于<code>n - 1阶马尔可夫链</code>的概率语言模型。例如，当$n=2$时，含有4个词的文本序列的概率就可以改写为：</p><script type="math/tex; mode=display">\begin{aligned}P\left(w_{1}, w_{2}, w_{3}, w_{4}\right) &=P\left(w_{1}\right) P\left(w_{2} | w_{1}\right) P\left(w_{3} | w_{1}, w_{2}\right) P\left(w_{4} | w_{1}, w_{2}, w_{3}\right) \\&=P\left(w_{1}\right) P\left(w_{2} | w_{1}\right) P\left(w_{3} | w_{2}\right) P\left(w_{4} | w_{3}\right)\end{aligned}</script><p>当$n$分别为1、2和3时，我们将其分别称作一元语法（unigram）、二元语法（bigram）和三元语法（trigram）。例如，长度为4的序列$w_1, w_2, w_3, w_4$在一元语法、二元语法和三元语法中的概率分别为</p><script type="math/tex; mode=display">\begin{aligned}P(w_1, w_2, w_3, w_4) &=  P(w_1) P(w_2) P(w_3) P(w_4) ,\\P(w_1, w_2, w_3, w_4) &=  P(w_1) P(w_2 \mid w_1) P(w_3 \mid w_2) P(w_4 \mid w_3) ,\\P(w_1, w_2, w_3, w_4) &=  P(w_1) P(w_2 \mid w_1) P(w_3 \mid w_1, w_2) P(w_4 \mid w_2, w_3) .\end{aligned}</script><p>当$n$较小时，$n$元语法往往并不准确。例如，在一元语法中，由三个词组成的句子“你走先”和“你先走”的概率是一样的。然而，当$n$较大时，$n$元语法需要计算并存储大量的词频和多词相邻频率。</p><p>思考：$n$元语法可能有哪些缺陷？</p><ol><li>参数空间过大</li><li>数据稀疏</li></ol><h1 id="语言模型数据集"><a href="#语言模型数据集" class="headerlink" title="语言模型数据集"></a>语言模型数据集</h1><h2 id="读取数据集"><a href="#读取数据集" class="headerlink" title="读取数据集"></a>读取数据集</h2><pre><code class="lang-python">with open(&#39;/home/kesci/input/jaychou_lyrics4703/jaychou_lyrics.txt&#39;) as f:    corpus_chars = f.read()print(len(corpus_chars))print(corpus_chars[: 40])corpus_chars = corpus_chars.replace(&#39;\n&#39;, &#39; &#39;).replace(&#39;\r&#39;, &#39; &#39;)corpus_chars = corpus_chars[: 10000]</code></pre><pre><code>63282想要有直升机想要和你飞到宇宙去想要和你融化在一起融化在宇宙里我每天每天每</code></pre><h2 id="建立字符索引"><a href="#建立字符索引" class="headerlink" title="建立字符索引"></a>建立字符索引</h2><pre><code class="lang-python">idx_to_char = list(set(corpus_chars)) # 去重，得到索引到字符的映射char_to_idx = {char: i for i, char in enumerate(idx_to_char)} # 字符到索引的映射vocab_size = len(char_to_idx)print(vocab_size)corpus_indices = [char_to_idx[char] for char in corpus_chars]  # 将每个字符转化为索引，得到一个索引的序列sample = corpus_indices[: 20]print(&#39;chars:&#39;, &#39;&#39;.join([idx_to_char[idx] for idx in sample]))print(&#39;indices:&#39;, sample)</code></pre><pre><code>1027chars: 想要有直升机 想要和你飞到宇宙去 想要和indices: [153, 983, 692, 753, 789, 806, 502, 153, 983, 312, 582, 908, 664, 965, 211, 857, 502, 153, 983, 312]</code></pre><p>定义函数<code>load_data_jay_lyrics</code>，在后续章节中直接调用。</p><pre><code class="lang-python">def load_data_jay_lyrics():    with open(&#39;/home/kesci/input/jaychou_lyrics4703/jaychou_lyrics.txt&#39;) as f:        corpus_chars = f.read()    corpus_chars = corpus_chars.replace(&#39;\n&#39;, &#39; &#39;).replace(&#39;\r&#39;, &#39; &#39;)    corpus_chars = corpus_chars[0:10000]    idx_to_char = list(set(corpus_chars))    char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])    vocab_size = len(char_to_idx)    corpus_indices = [char_to_idx[char] for char in corpus_chars]    return corpus_indices, char_to_idx, idx_to_char, vocab_size</code></pre><h2 id="时序数据的采样"><a href="#时序数据的采样" class="headerlink" title="时序数据的采样"></a>时序数据的采样</h2><p>在训练中我们需要每次随机读取小批量样本和标签。与之前章节的实验数据不同的是，时序数据的一个样本通常包含连续的字符。假设时间步数为5，样本序列为5个字符，即“想”“要”“有”“直”“升”。该样本的标签序列为这些字符分别在训练集中的下一个字符，即“要”“有”“直”“升”“机”，即$X$=“想要有直升”，$Y$=“要有直升机”。</p><p>现在我们考虑序列“想要有直升机，想要和你飞到宇宙去”，如果时间步数为5，有以下可能的样本和标签：</p><ul><li>$X$：“想要有直升”，$Y$：“要有直升机”</li><li>$X$：“要有直升机”，$Y$：“有直升机，”</li><li>$X$：“有直升机，”，$Y$：“直升机，想”</li><li>…</li><li>$X$：“要和你飞到”，$Y$：“和你飞到宇”</li><li>$X$：“和你飞到宇”，$Y$：“你飞到宇宙”</li><li>$X$：“你飞到宇宙”，$Y$：“飞到宇宙去”</li></ul><p>可以看到，如果序列的长度为$T$，时间步数为$n$，那么一共有$T-n$个合法的样本，但是这些样本有大量的重合，我们通常采用更加高效的采样方式。我们有两种方式对时序数据进行采样，分别是随机采样和相邻采样。</p><h3 id="随机采样"><a href="#随机采样" class="headerlink" title="随机采样"></a>随机采样</h3><p>下面的代码每次从数据里随机采样一个小批量。其中批量大小<code>batch_size</code>是每个小批量的样本数，<code>num_steps</code>是每个样本所包含的时间步数。<br>在随机采样中，每个样本是原始序列上任意截取的一段序列，相邻的两个随机小批量在原始序列上的位置不一定相毗邻。</p><pre><code class="lang-python">import torchimport randomdef data_iter_random(corpus_indices, batch_size, num_steps, device=None):    # 减1是因为对于长度为n的序列，X最多只有包含其中的前n - 1个字符    num_examples = (len(corpus_indices) - 1) // num_steps  # 下取整，得到不重叠情况下的样本个数    example_indices = [i * num_steps for i in range(num_examples)]  # 每个样本的第一个字符在corpus_indices中的下标    random.shuffle(example_indices)    def _data(i):        # 返回从i开始的长为num_steps的序列        return corpus_indices[i: i + num_steps]    if device is None:        device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)    for i in range(0, num_examples, batch_size):        # 每次选出batch_size个随机样本        batch_indices = example_indices[i: i + batch_size]  # 当前batch的各个样本的首字符的下标        X = [_data(j) for j in batch_indices]        Y = [_data(j + 1) for j in batch_indices]        yield torch.tensor(X, device=device), torch.tensor(Y, device=device)</code></pre><p>测试一下这个函数，我们输入从0到29的连续整数作为一个人工序列，设批量大小和时间步数分别为2和6，打印随机采样每次读取的小批量样本的输入<code>X</code>和标签<code>Y</code>。</p><pre><code class="lang-python">my_seq = list(range(30))for X, Y in data_iter_random(my_seq, batch_size=2, num_steps=6):    #batch_size 为 2（每个 batch 有两行), num_steps 时间步数为 6 (每行有 5 个元素) batch_num 为 2，一共两个 batch    print(&#39;X: &#39;, X, &#39;\nY:&#39;, Y, &#39;\n&#39;)</code></pre><pre><code>X:  tensor([[18, 19, 20, 21, 22, 23],        [12, 13, 14, 15, 16, 17]]) Y: tensor([[19, 20, 21, 22, 23, 24],        [13, 14, 15, 16, 17, 18]]) X:  tensor([[ 6,  7,  8,  9, 10, 11],        [ 0,  1,  2,  3,  4,  5]]) Y: tensor([[ 7,  8,  9, 10, 11, 12],        [ 1,  2,  3,  4,  5,  6]]) </code></pre><h3 id="相邻采样"><a href="#相邻采样" class="headerlink" title="相邻采样"></a>相邻采样</h3><p>在相邻采样中，相邻的两个随机小批量在原始序列上的位置相毗邻.（交替出现）<br><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://s2.ax1x.com/2020/02/19/3VSeKI.png" alt=""></p><pre><code class="lang-python">def data_iter_consecutive(corpus_indices, batch_size, num_steps, device=None):    # bach_size 为 n, 数据就会被拆成 n 块    if device is None:        device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)    corpus_len = len(corpus_indices) // batch_size * batch_size  # 保留下来的序列的长度    corpus_indices = corpus_indices[: corpus_len]  # 仅保留前corpus_len个字符 保证可以整除批量大小    indices = torch.tensor(corpus_indices, device=device)    indices = indices.view(batch_size, -1)  # resize成(batch_size, )    batch_num = (indices.shape[1] - 1) // num_steps   # -1 是因为不能包含最后一个字符，因为最后一个就没有标签了    for i in range(batch_num):  # 每次构建一个批量        i = i * num_steps        X = indices[:, i: i + num_steps]        Y = indices[:, i + 1: i + num_steps + 1]        yield X, Y</code></pre><p>同样的设置下，打印相邻采样每次读取的小批量样本的输入<code>X</code>和标签<code>Y</code>。相邻的两个随机小批量在原始序列上的位置相毗邻。<br>X 的标签 Y 就是它的下一个位置。</p><pre><code class="lang-python">for X, Y in data_iter_consecutive(my_seq, batch_size=2, num_steps=6):    print(&#39;X: &#39;, X, &#39;\nY:&#39;, Y, &#39;\n&#39;)my_seq = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]for X, Y in data_iter_consecutive(my_seq, batch_size=2, num_steps=2):    print(&#39;X: &#39;, X, &#39;\nY:&#39;, Y, &#39;\n&#39;)</code></pre><pre><code>X:  tensor([[ 0,  1,  2,  3,  4,  5],        [15, 16, 17, 18, 19, 20]]) Y: tensor([[ 1,  2,  3,  4,  5,  6],        [16, 17, 18, 19, 20, 21]]) X:  tensor([[ 6,  7,  8,  9, 10, 11],        [21, 22, 23, 24, 25, 26]]) Y: tensor([[ 7,  8,  9, 10, 11, 12],        [22, 23, 24, 25, 26, 27]]) X:  tensor([[0, 1],        [5, 6]]) Y: tensor([[1, 2],        [6, 7]]) X:  tensor([[2, 3],        [7, 8]]) Y: tensor([[3, 4],        [8, 9]]) </code></pre><h2 id="练习题"><a href="#练习题" class="headerlink" title="练习题"></a>练习题</h2><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/RlvJN7BnbPGfp2Y.png" alt="语言模型"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动手深度学习(五):梯度消失、梯度爆炸</title>
      <link href="/2020/02/19/dl-notes5-grad/"/>
      <url>/2020/02/19/dl-notes5-grad/</url>
      
        <content type="html"><![CDATA[<h1 id="梯度消失、梯度爆炸以及Kaggle房价预测"><a href="#梯度消失、梯度爆炸以及Kaggle房价预测" class="headerlink" title="梯度消失、梯度爆炸以及Kaggle房价预测"></a>梯度消失、梯度爆炸以及Kaggle房价预测</h1><ol><li>梯度消失和梯度爆炸</li><li>考虑到环境因素的其他问题</li><li>Kaggle房价预测</li></ol><h1 id="梯度消失和梯度爆炸"><a href="#梯度消失和梯度爆炸" class="headerlink" title="梯度消失和梯度爆炸"></a>梯度消失和梯度爆炸</h1><p>深度模型有关数值稳定性的典型问题是消失（vanishing）和爆炸（explosion）。</p><p><strong>当神经网络的层数较多时，模型的数值稳定性容易变差。</strong></p><p>假设一个层数为$L$的多层感知机的第$l$层$\boldsymbol{H}^{(l)}$的权重参数为$\boldsymbol{W}^{(l)}$，输出层$\boldsymbol{H}^{(L)}$的权重参数为$\boldsymbol{W}^{(L)}$。为了便于讨论，不考虑偏差参数，且设所有隐藏层的激活函数为恒等映射（identity mapping）$\phi(x) = x$。给定输入$\boldsymbol{X}$，多层感知机的第$l$层的输出$\boldsymbol{H}^{(l)} = \boldsymbol{X} \boldsymbol{W}^{(1)} \boldsymbol{W}^{(2)} \ldots \boldsymbol{W}^{(l)}$。此时，如果层数$l$较大，$\boldsymbol{H}^{(l)}$的计算可能会出现衰减或爆炸。举个例子，假设输入和所有层的权重参数都是标量，如权重参数为0.2和5，多层感知机的第30层输出为输入$\boldsymbol{X}$分别与$0.2^{30} \approx 1 \times 10^{-21}$（消失）和$5^{30} \approx 9 \times 10^{20}$（爆炸）的乘积。当层数较多时，梯度的计算也容易出现消失或爆炸。</p><h1 id="随机初始化模型参数"><a href="#随机初始化模型参数" class="headerlink" title="随机初始化模型参数"></a>随机初始化模型参数</h1><p>在神经网络中，通常需要随机初始化模型参数。下面我们来解释这样做的原因。</p><p>回顾多层感知机一节描述的多层感知机。为了方便解释，假设输出层只保留一个输出单元$o_1$（删去$o_2$和$o_3$以及指向它们的箭头），且隐藏层使用相同的激活函数。如果将每个隐藏单元的参数都初始化为相等的值，那么在正向传播时每个隐藏单元将根据相同的输入计算出相同的值，并传递至输出层。在反向传播中，每个隐藏单元的参数梯度值相等。因此，这些参数在使用基于梯度的优化算法迭代后值依然相等。之后的迭代也是如此。在这种情况下，无论隐藏单元有多少，隐藏层本质上只有1个隐藏单元在发挥作用。因此，正如在前面的实验中所做的那样，我们通常将神经网络的模型参数，特别是权重参数，进行随机初始化。</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/Yvn1aRXFSdqm5ED.png" alt=""></p><h3 id="PyTorch的默认随机初始化"><a href="#PyTorch的默认随机初始化" class="headerlink" title="PyTorch的默认随机初始化"></a>PyTorch的默认随机初始化</h3><p>随机初始化模型参数的方法有很多。在线性回归的简洁实现中，我们使用<code>torch.nn.init.normal_()</code>使模型<code>net</code>的权重参数采用正态分布的随机初始化方式。不过，PyTorch中<code>nn.Module</code>的模块参数都采取了较为合理的初始化策略（不同类型的layer具体采样的哪一种初始化方法的可参考<a href="https://github.com/pytorch/pytorch/tree/master/torch/nn/modules" target="_blank" rel="noopener">源代码</a>），因此一般不用我们考虑。</p><h3 id="Xavier随机初始化"><a href="#Xavier随机初始化" class="headerlink" title="Xavier随机初始化"></a>Xavier随机初始化</h3><p>还有一种比较常用的随机初始化方法叫作Xavier随机初始化。<br>假设某全连接层的输入个数为$a$，输出个数为$b$，Xavier随机初始化将使该层中权重参数的每个元素都随机采样于均匀分布</p><script type="math/tex; mode=display">U\left(-\sqrt{\frac{6}{a+b}}, \sqrt{\frac{6}{a+b}}\right).</script><p>它的设计主要考虑到，模型参数初始化后，每层输出的方差不该受该层输入个数影响，且每层梯度的方差也不该受该层输出个数影响。</p><h1 id="考虑环境因素"><a href="#考虑环境因素" class="headerlink" title="考虑环境因素"></a>考虑环境因素</h1><h2 id="协变量偏移"><a href="#协变量偏移" class="headerlink" title="协变量偏移"></a>协变量偏移</h2><p>这里我们假设，虽然输入的分布可能随时间而改变，但是标记函数，即条件分布P（y∣x）不会改变。虽然这个问题容易理解，但在实践中也容易忽视。</p><p>想想区分猫和狗的一个例子。我们的训练数据使用的是猫和狗的真实的照片，但是在测试时，我们被要求对猫和狗的卡通图片进行分类。</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://s2.ax1x.com/2020/02/19/3VWGV0.png" alt="协变量偏移"></p><p>显然，这不太可能奏效。训练集由照片组成，而测试集只包含卡通。在一个看起来与测试集有着本质不同的数据集上进行训练，而不考虑如何适应新的情况，这是不是一个好主意。不幸的是，这是一个非常常见的陷阱。</p><p>统计学家称这种协变量变化是因为问题的根源在于特征分布的变化（即协变量的变化）。数学上，我们可以说P（x）改变了，但P（y∣x）保持不变。尽管它的有用性并不局限于此，当我们认为x导致y时，协变量移位通常是正确的假设。</p><h2 id="标签偏移"><a href="#标签偏移" class="headerlink" title="标签偏移"></a>标签偏移</h2><p>当我们认为导致偏移的是标签P（y）上的边缘分布的变化，但类条件分布是不变的P（x∣y）时，就会出现相反的问题。当我们认为y导致x时，标签偏移是一个合理的假设。例如，通常我们希望根据其表现来预测诊断结果。在这种情况下，我们认为诊断引起的表现，即疾病引起的症状。有时标签偏移和协变量移位假设可以同时成立。例如，当真正的标签函数是确定的和不变的，那么协变量偏移将始终保持，包括如果标签偏移也保持。有趣的是，当我们期望标签偏移和协变量偏移保持时，使用来自标签偏移假设的方法通常是有利的。这是因为这些方法倾向于操作看起来像标签的对象，这（在深度学习中）与处理看起来像输入的对象（在深度学习中）相比相对容易一些。</p><p>病因（要预测的诊断结果）导致 症状（观察到的结果）。  </p><p>训练数据集，数据很少只包含流感p(y)的样本。  </p><p>而测试数据集有流感p(y)和流感q(y)，其中不变的是流感症状p(x|y)。</p><h2 id="概念偏移"><a href="#概念偏移" class="headerlink" title="概念偏移"></a>概念偏移</h2><p>另一个相关的问题出现在概念转换中，即标签本身的定义发生变化的情况。这听起来很奇怪，毕竟猫就是猫。的确，猫的定义可能不会改变，但我们能不能对软饮料也这么说呢？事实证明，如果我们周游美国，按地理位置转移数据来源，我们会发现，即使是如图所示的这个简单术语的定义也会发生相当大的概念转变。</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/XZM92uq4EvNQmYT.png" alt="美国软饮料名称的概念转变"></p><p>如果我们要建立一个机器翻译系统，分布P（y∣x）可能因我们的位置而异。这个问题很难发现。另一个可取之处是P（y∣x）通常只是逐渐变化。</p><h1 id="Kaggle-房价预测实战"><a href="#Kaggle-房价预测实战" class="headerlink" title="Kaggle 房价预测实战"></a>Kaggle 房价预测实战</h1><p>作为深度学习基础篇章的总结，我们将对本章内容学以致用。下面，让我们动手实战一个Kaggle比赛：房价预测。本节将提供未经调优的数据的预处理、模型的设计和超参数的选择。我们希望读者通过动手操作、仔细观察实验现象、认真分析实验结果并不断调整方法，得到令自己满意的结果。</p><pre><code class="lang-python">%matplotlib inlineimport torchimport torch.nn as nnimport numpy as npimport pandas as pdimport syssys.path.append(&quot;/home/kesci/input&quot;)import d2lzh1981 as d2lprint(torch.__version__)torch.set_default_tensor_type(torch.FloatTensor)</code></pre><pre><code>1.3.0</code></pre><h2 id="获取和读取数据集"><a href="#获取和读取数据集" class="headerlink" title="获取和读取数据集"></a>获取和读取数据集</h2><p>比赛数据分为训练数据集和测试数据集。两个数据集都包括每栋房子的特征，如街道类型、建造年份、房顶类型、地下室状况等特征值。这些特征值有连续的数字、离散的标签甚至是缺失值“na”。只有训练数据集包括了每栋房子的价格，也就是标签。我们可以访问比赛网页，点击“Data”标签，并下载这些数据集。</p><p>我们将通过<code>pandas</code>库读入并处理数据。在导入本节需要的包前请确保已安装<code>pandas</code>库。<br>假设解压后的数据位于<code>/home/kesci/input/houseprices2807/</code>目录，它包括两个csv文件。下面使用<code>pandas</code>读取这两个文件。</p><pre><code class="lang-python">test_data = pd.read_csv(&quot;/home/kesci/input/houseprices2807/house-prices-advanced-regression-techniques/test.csv&quot;)train_data = pd.read_csv(&quot;/home/kesci/input/houseprices2807/house-prices-advanced-regression-techniques/train.csv&quot;)</code></pre><p>训练数据集包括1460个样本、80个特征和1个标签。</p><pre><code class="lang-python">train_data.shape</code></pre><pre><code>(1460, 81)</code></pre><p>测试数据集包括1459个样本和80个特征。我们需要将测试数据集中每个样本的标签预测出来。</p><pre><code class="lang-python">test_data.shape</code></pre><pre><code>(1459, 80)</code></pre><p>让我们来查看前4个样本的前4个特征、后2个特征和标签（SalePrice）：</p><pre><code class="lang-python">train_data.iloc[0:4, [0, 1, 2, 3, -3, -2, -1]]</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>Id</th>      <th>MSSubClass</th>      <th>MSZoning</th>      <th>LotFrontage</th>      <th>SaleType</th>      <th>SaleCondition</th>      <th>SalePrice</th>    </tr>  </thead>  <tbody>    <tr>      <td>0</td>      <td>1</td>      <td>60</td>      <td>RL</td>      <td>65.0</td>      <td>WD</td>      <td>Normal</td>      <td>208500</td>    </tr>    <tr>      <td>1</td>      <td>2</td>      <td>20</td>      <td>RL</td>      <td>80.0</td>      <td>WD</td>      <td>Normal</td>      <td>181500</td>    </tr>    <tr>      <td>2</td>      <td>3</td>      <td>60</td>      <td>RL</td>      <td>68.0</td>      <td>WD</td>      <td>Normal</td>      <td>223500</td>    </tr>    <tr>      <td>3</td>      <td>4</td>      <td>70</td>      <td>RL</td>      <td>60.0</td>      <td>WD</td>      <td>Abnorml</td>      <td>140000</td>    </tr>  </tbody></table></div><p>可以看到第一个特征是Id，它能帮助模型记住每个训练样本，但难以推广到测试样本，所以我们不使用它来训练。我们将所有的训练数据和测试数据的79个特征按样本连结。</p><pre><code class="lang-python">all_features = pd.concat((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:]))</code></pre><h2 id="预处理数据"><a href="#预处理数据" class="headerlink" title="预处理数据"></a>预处理数据</h2><p>我们对连续数值的特征做标准化（standardization）：设该特征在整个数据集上的均值为$\mu$，标准差为$\sigma$。那么，我们可以将该特征的每个值先减去$\mu$再除以$\sigma$得到标准化后的每个特征值。对于缺失的特征值，我们将其替换成该特征的均值。</p><pre><code class="lang-python">numeric_features = all_features.dtypes[all_features.dtypes != &#39;object&#39;].indexall_features[numeric_features] = all_features[numeric_features].apply(    lambda x: (x - x.mean()) / (x.std()))# 标准化后，每个数值特征的均值变为0，所以可以直接用0来替换缺失值all_features[numeric_features] = all_features[numeric_features].fillna(0)</code></pre><p>接下来将离散数值转成指示特征。举个例子，假设特征MSZoning里面有两个不同的离散值RL和RM，那么这一步转换将去掉MSZoning特征，并新加两个特征MSZoning_RL和MSZoning_RM，其值为0或1。如果一个样本原来在MSZoning里的值为RL，那么有MSZoning_RL=1且MSZoning_RM=0。</p><pre><code class="lang-python"># dummy_na=True将缺失值也当作合法的特征值并为其创建指示特征all_features = pd.get_dummies(all_features, dummy_na=True)all_features.shape</code></pre><pre><code>(2919, 331)</code></pre><p>可以看到这一步转换将特征数从79增加到了331。</p><p>最后，通过<code>values</code>属性得到NumPy格式的数据，并转成<code>Tensor</code>方便后面的训练。</p><pre><code class="lang-python">n_train = train_data.shape[0]train_features = torch.tensor(all_features[:n_train].values, dtype=torch.float)test_features = torch.tensor(all_features[n_train:].values, dtype=torch.float)train_labels = torch.tensor(train_data.SalePrice.values, dtype=torch.float).view(-1, 1)</code></pre><h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><pre><code class="lang-python">loss = torch.nn.MSELoss()def get_net(feature_num):    net = nn.Linear(feature_num, 1)    for param in net.parameters():        nn.init.normal_(param, mean=0, std=0.01)    return net</code></pre><p>下面定义比赛用来评价模型的对数均方根误差。给定预测值$\hat y_1, \ldots, \hat y_n$和对应的真实标签$y_1,\ldots, y_n$，它的定义为</p><script type="math/tex; mode=display">\sqrt{\frac{1}{n}\sum_{i=1}^n\left(\log(y_i)-\log(\hat y_i)\right)^2}.</script><p>对数均方根误差的实现如下。</p><pre><code class="lang-python">def log_rmse(net, features, labels):    with torch.no_grad():        # 将小于1的值设成1，使得取对数时数值更稳定        clipped_preds = torch.max(net(features), torch.tensor(1.0))        rmse = torch.sqrt(2 * loss(clipped_preds.log(), labels.log()).mean())    return rmse.item()</code></pre><p>下面的训练函数跟本章中前几节的不同在于使用了Adam优化算法。相对之前使用的小批量随机梯度下降，它对学习率相对不那么敏感。我们将在之后的“优化算法”一章里详细介绍它。</p><pre><code class="lang-python">def train(net, train_features, train_labels, test_features, test_labels,          num_epochs, learning_rate, weight_decay, batch_size):    train_ls, test_ls = [], []    dataset = torch.utils.data.TensorDataset(train_features, train_labels)    train_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True)    # 这里使用了Adam优化算法    optimizer = torch.optim.Adam(params=net.parameters(), lr=learning_rate, weight_decay=weight_decay)     net = net.float()    for epoch in range(num_epochs):        for X, y in train_iter:            l = loss(net(X.float()), y.float())            optimizer.zero_grad()            l.backward()            optimizer.step()        train_ls.append(log_rmse(net, train_features, train_labels))        if test_labels is not None:            test_ls.append(log_rmse(net, test_features, test_labels))    return train_ls, test_ls</code></pre><h2 id="K折交叉验证"><a href="#K折交叉验证" class="headerlink" title="K折交叉验证"></a>K折交叉验证</h2><p>我们在模型选择、欠拟合和过拟合中介绍了$K$折交叉验证。它将被用来选择模型设计并调节超参数。下面实现了一个函数，它返回第<code>i</code>折交叉验证时所需要的训练和验证数据。</p><pre><code class="lang-python">def get_k_fold_data(k, i, X, y):    # 返回第i折交叉验证时所需要的训练和验证数据    assert k &gt; 1    fold_size = X.shape[0] // k    X_train, y_train = None, None    for j in range(k):        idx = slice(j * fold_size, (j + 1) * fold_size)        X_part, y_part = X[idx, :], y[idx]        if j == i:            X_valid, y_valid = X_part, y_part        elif X_train is None:            X_train, y_train = X_part, y_part        else:            X_train = torch.cat((X_train, X_part), dim=0)            y_train = torch.cat((y_train, y_part), dim=0)    return X_train, y_train, X_valid, y_valid</code></pre><p>在$K$折交叉验证中我们训练$K$次并返回训练和验证的平均误差</p><pre><code class="lang-python">def k_fold(k, X_train, y_train, num_epochs,           learning_rate, weight_decay, batch_size):    train_l_sum, valid_l_sum = 0, 0    for i in range(k):        data = get_k_fold_data(k, i, X_train, y_train)        net = get_net(X_train.shape[1])        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate,                                   weight_decay, batch_size)        train_l_sum += train_ls[-1]        valid_l_sum += valid_ls[-1]        if i == 0:            d2l.semilogy(range(1, num_epochs + 1), train_ls, &#39;epochs&#39;, &#39;rmse&#39;,                         range(1, num_epochs + 1), valid_ls,                         [&#39;train&#39;, &#39;valid&#39;])        print(&#39;fold %d, train rmse %f, valid rmse %f&#39; % (i, train_ls[-1], valid_ls[-1]))    return train_l_sum / k, valid_l_sum / k</code></pre><h2 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h2><p>我们使用一组未经调优的超参数并计算交叉验证误差。可以改动这些超参数来尽可能减小平均测试误差。<br>有时候你会发现一组参数的训练误差可以达到很低，但是在$K$折交叉验证上的误差可能反而较高。这种现象很可能是由过拟合造成的。因此，当训练误差降低时，我们要观察$K$折交叉验证上的误差是否也相应降低。</p><pre><code class="lang-python">k, num_epochs, lr, weight_decay, batch_size = 5, 100, 5, 0, 64train_l, valid_l = k_fold(k, train_features, train_labels, num_epochs, lr, weight_decay, batch_size)print(&#39;%d-fold validation: avg train rmse %f, avg valid rmse %f&#39; % (k, train_l, valid_l))</code></pre><pre><code>fold 0, train rmse 0.241365, valid rmse 0.223083fold 1, train rmse 0.229118, valid rmse 0.267488fold 2, train rmse 0.232072, valid rmse 0.237995fold 3, train rmse 0.238050, valid rmse 0.218671fold 4, train rmse 0.231004, valid rmse 0.2591855-fold validation: avg train rmse 0.234322, avg valid rmse 0.241284</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://s2.ax1x.com/2020/02/19/3VfyOs.png" alt="损失不断下降"></p><h1 id="预测并在Kaggle中提交结果"><a href="#预测并在Kaggle中提交结果" class="headerlink" title="预测并在Kaggle中提交结果"></a>预测并在Kaggle中提交结果</h1><p>下面定义预测函数。在预测之前，我们会使用完整的训练数据集来重新训练模型，并将预测结果存成提交所需要的格式。</p><pre><code class="lang-python">def train_and_pred(train_features, test_features, train_labels, test_data,                   num_epochs, lr, weight_decay, batch_size):    net = get_net(train_features.shape[1])    train_ls, _ = train(net, train_features, train_labels, None, None,                        num_epochs, lr, weight_decay, batch_size)    d2l.semilogy(range(1, num_epochs + 1), train_ls, &#39;epochs&#39;, &#39;rmse&#39;)    print(&#39;train rmse %f&#39; % train_ls[-1])    preds = net(test_features).detach().numpy()    test_data[&#39;SalePrice&#39;] = pd.Series(preds.reshape(1, -1)[0])    submission = pd.concat([test_data[&#39;Id&#39;], test_data[&#39;SalePrice&#39;]], axis=1)    submission.to_csv(&#39;./submission.csv&#39;, index=False)    # sample_submission_data = pd.read_csv(&quot;../input/house-prices-advanced-regression-techniques/sample_submission.csv&quot;)</code></pre><p>设计好模型并调好超参数之后，下一步就是对测试数据集上的房屋样本做价格预测。如果我们得到与交叉验证时差不多的训练误差，那么这个结果很可能是理想的，可以在Kaggle上提交结果。</p><pre><code class="lang-python">train_and_pred(train_features, test_features, train_labels, test_data, num_epochs, lr, weight_decay, batch_size)</code></pre><p>希望大家自己动手完成房价预测的实现，多参与讨论。</p><h1 id="练习题"><a href="#练习题" class="headerlink" title="练习题"></a>练习题</h1><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/GJjuZ5mLcKX7Hsd.png" alt="(1)"></p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/qAFfVdXbyMgrHkj.png" alt="(2)"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动手深度学习(四):过拟合与欠拟合</title>
      <link href="/2020/02/14/dl-notes4-ov-und-fit/"/>
      <url>/2020/02/14/dl-notes4-ov-und-fit/</url>
      
        <content type="html"><![CDATA[<h1 id="过拟合、欠拟合及其解决方案"><a href="#过拟合、欠拟合及其解决方案" class="headerlink" title="过拟合、欠拟合及其解决方案"></a>过拟合、欠拟合及其解决方案</h1><ol><li>过拟合、欠拟合的概念</li><li>权重衰减</li><li>丢弃法</li></ol><h1 id="模型选择、过拟合和欠拟合"><a href="#模型选择、过拟合和欠拟合" class="headerlink" title="模型选择、过拟合和欠拟合"></a>模型选择、过拟合和欠拟合</h1><h2 id="训练误差和泛化误差"><a href="#训练误差和泛化误差" class="headerlink" title="训练误差和泛化误差"></a>训练误差和泛化误差</h2><p>在解释上述现象之前，我们需要区分训练误差（training error）和泛化误差（generalization error）。通俗来讲，前者指模型在训练数据集上表现出的误差，后者指模型在任意一个测试数据样本上表现出的误差的期望，并常常通过测试数据集上的误差来近似。计算训练误差和泛化误差可以使用之前介绍过的损失函数，例如线性回归用到的平方损失函数和softmax回归用到的交叉熵损失函数。</p><p>机器学习模型应关注降低泛化误差。</p><h2 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h2><h3 id="验证数据集"><a href="#验证数据集" class="headerlink" title="验证数据集"></a>验证数据集</h3><p>从严格意义上讲，测试集只能在所有超参数和模型参数选定后使用一次。不可以使用测试数据选择模型，如调参。由于无法从训练误差估计泛化误差，因此也不应只依赖训练数据选择模型。鉴于此，我们可以预留一部分在训练数据集和测试数据集以外的数据来进行模型选择。这部分数据被称为验证数据集，简称验证集（validation set）。例如，我们可以从给定的训练集中随机选取一小部分作为验证集，而将剩余部分作为真正的训练集。</p><h3 id="K折交叉验证"><a href="#K折交叉验证" class="headerlink" title="K折交叉验证"></a>K折交叉验证</h3><p>由于验证数据集不参与模型训练，当训练数据不够用时，预留大量的验证数据显得太奢侈。一种改善的方法是K折交叉验证（K-fold cross-validation）。在K折交叉验证中，我们把原始训练数据集分割成K个不重合的子数据集，然后我们做K次模型训练和验证。每一次，我们使用一个子数据集验证模型，并使用其他K-1个子数据集来训练模型。在这K次训练和验证中，每次用来验证模型的子数据集都不同。最后，我们对这K次训练误差和验证误差分别求平均。</p><h2 id="过拟合和欠拟合"><a href="#过拟合和欠拟合" class="headerlink" title="过拟合和欠拟合"></a>过拟合和欠拟合</h2><p>接下来，我们将探究模型训练中经常出现的两类典型问题：</p><ul><li>一类是模型无法得到较低的训练误差，我们将这一现象称作欠拟合（underfitting）；</li><li>另一类是模型的训练误差远小于它在测试数据集上的误差，我们称该现象为过拟合（overfitting）。<br>在实践中，我们要尽可能同时应对欠拟合和过拟合。虽然有很多因素可能导致这两种拟合问题，在这里我们重点讨论两个因素：模型复杂度和训练数据集大小。</li></ul><h3 id="模型复杂度"><a href="#模型复杂度" class="headerlink" title="模型复杂度"></a>模型复杂度</h3><p>为了解释模型复杂度，我们以多项式函数拟合为例。给定一个由标量数据特征$x$和对应的标量标签$y$组成的训练数据集，多项式函数拟合的目标是找一个$K$阶多项式函数</p><script type="math/tex; mode=display"> \hat{y} = b + \sum_{k=1}^K x^k w_k</script><p>来近似 $y$。在上式中，$w_k$是模型的权重参数，$b$是偏差参数。与线性回归相同，多项式函数拟合也使用平方损失函数。特别地，一阶多项式函数拟合又叫线性函数拟合。</p><p>给定训练数据集，模型复杂度和误差之间的关系：</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/upload/image/q5jc27wxoj.png?imageView2/0/w/960/h/960" alt="Image Name"></p><h3 id="训练数据集大小"><a href="#训练数据集大小" class="headerlink" title="训练数据集大小"></a>训练数据集大小</h3><p>影响欠拟合和过拟合的另一个重要因素是训练数据集的大小。一般来说，如果训练数据集中样本数过少，特别是比模型参数数量（按元素计）更少时，过拟合更容易发生。此外，泛化误差不会随训练数据集里样本数量增加而增大。因此，在计算资源允许的范围之内，我们通常希望训练数据集大一些，特别是在模型复杂度较高时，例如层数较多的深度学习模型。</p><h1 id="多项式函数拟合实验"><a href="#多项式函数拟合实验" class="headerlink" title="多项式函数拟合实验"></a>多项式函数拟合实验</h1><pre><code class="lang-python">%matplotlib inlineimport torchimport numpy as npimport syssys.path.append(&quot;/home/kesci/input&quot;)import d2lzh1981 as d2lprint(torch.__version__)</code></pre><pre><code>1.3.0</code></pre><h2 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h2><pre><code class="lang-python">n_train, n_test, true_w, true_b = 100, 100, [1.2, -3.4, 5.6], 5features = torch.randn((n_train + n_test, 1))poly_features = torch.cat((features, torch.pow(features, 2), torch.pow(features, 3)), 1) labels = (true_w[0] * poly_features[:, 0] + true_w[1] * poly_features[:, 1]          + true_w[2] * poly_features[:, 2] + true_b)labels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float)</code></pre><pre><code class="lang-python">features[:2], poly_features[:2], labels[:2]</code></pre><pre><code>(tensor([[-0.8589],         [-0.2534]]), tensor([[-0.8589,  0.7377, -0.6335],         [-0.2534,  0.0642, -0.0163]]), tensor([-2.0794,  4.4039]))</code></pre><h2 id="定义、训练和测试模型"><a href="#定义、训练和测试模型" class="headerlink" title="定义、训练和测试模型"></a>定义、训练和测试模型</h2><pre><code class="lang-python">def semilogy(x_vals, y_vals, x_label, y_label, x2_vals=None, y2_vals=None,             legend=None, figsize=(3.5, 2.5)):    # d2l.set_figsize(figsize)    d2l.plt.xlabel(x_label)    d2l.plt.ylabel(y_label)    d2l.plt.semilogy(x_vals, y_vals)    if x2_vals and y2_vals:        d2l.plt.semilogy(x2_vals, y2_vals, linestyle=&#39;:&#39;)        d2l.plt.legend(legend)</code></pre><pre><code class="lang-python">num_epochs, loss = 100, torch.nn.MSELoss()def fit_and_plot(train_features, test_features, train_labels, test_labels):    # 初始化网络模型    net = torch.nn.Linear(train_features.shape[-1], 1)    # 通过Linear文档可知，pytorch已经将参数初始化了，所以我们这里就不手动初始化了    # 设置批量大小    batch_size = min(10, train_labels.shape[0])        dataset = torch.utils.data.TensorDataset(train_features, train_labels)      # 设置数据集    train_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True) # 设置获取数据方式    optimizer = torch.optim.SGD(net.parameters(), lr=0.01)                      # 设置优化函数，使用的是随机梯度下降优化    train_ls, test_ls = [], []    for _ in range(num_epochs):        for X, y in train_iter:                                                 # 取一个批量的数据            l = loss(net(X), y.view(-1, 1))                                     # 输入到网络中计算输出，并和标签比较求得损失函数            optimizer.zero_grad()                                               # 梯度清零，防止梯度累加干扰优化            l.backward()                                                        # 求梯度            optimizer.step()                                                    # 迭代优化函数，进行参数优化        train_labels = train_labels.view(-1, 1)        test_labels = test_labels.view(-1, 1)        train_ls.append(loss(net(train_features), train_labels).item())         # 将训练损失保存到train_ls中        test_ls.append(loss(net(test_features), test_labels).item())            # 将测试损失保存到test_ls中    print(&#39;final epoch: train loss&#39;, train_ls[-1], &#39;test loss&#39;, test_ls[-1])        semilogy(range(1, num_epochs + 1), train_ls, &#39;epochs&#39;, &#39;loss&#39;,             range(1, num_epochs + 1), test_ls, [&#39;train&#39;, &#39;test&#39;])    print(&#39;weight:&#39;, net.weight.data,          &#39;\nbias:&#39;, net.bias.data)</code></pre><h2 id="三阶多项式函数拟合（正常）"><a href="#三阶多项式函数拟合（正常）" class="headerlink" title="三阶多项式函数拟合（正常）"></a>三阶多项式函数拟合（正常）</h2><pre><code class="lang-python">fit_and_plot(poly_features[:n_train, :], poly_features[n_train:, :], labels[:n_train], labels[n_train:])</code></pre><pre><code>final epoch: train loss 8887.298828125 test loss 1145.94287109375weight: tensor([[-8.5120, 19.0351, 12.8616]]) bias: tensor([-5.4607])</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/CD685B472B744329A1CFC47C9F0B5E89/q5jf5azjcn.png"></p><h2 id="线性函数拟合（欠拟合）"><a href="#线性函数拟合（欠拟合）" class="headerlink" title="线性函数拟合（欠拟合）"></a>线性函数拟合（欠拟合）</h2><pre><code class="lang-python">fit_and_plot(features[:n_train, :], features[n_train:, :], labels[:n_train], labels[n_train:])</code></pre><pre><code>final epoch: train loss 781.689453125 test loss 329.79852294921875weight: tensor([[26.8753]]) bias: tensor([6.1426])</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/33AD626DA0B94DB7A28D47697312B45D/q5jf5al2tv.png"></p><h2 id="训练样本不足（过拟合）"><a href="#训练样本不足（过拟合）" class="headerlink" title="训练样本不足（过拟合）"></a>训练样本不足（过拟合）</h2><pre><code class="lang-python">fit_and_plot(poly_features[0:2, :], poly_features[n_train:, :], labels[0:2], labels[n_train:])</code></pre><pre><code>final epoch: train loss 6.23520565032959 test loss 409.9844665527344weight: tensor([[ 0.9729, -0.9612,  0.7259]]) bias: tensor([1.6334])</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/AB13F65A70A9484788F8004E427EC290/q5jf5bd11u.png"></p><h1 id="权重衰减"><a href="#权重衰减" class="headerlink" title="权重衰减"></a>权重衰减</h1><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>权重衰减等价于 $L_2$ 范数正则化（regularization）。正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常用手段。</p><h2 id="L2-范数正则化（regularization）"><a href="#L2-范数正则化（regularization）" class="headerlink" title="L2 范数正则化（regularization）"></a>L2 范数正则化（regularization）</h2><p>$L_2$范数正则化在模型原损失函数基础上添加$L_2$范数惩罚项，从而得到训练所需要最小化的函数。$L_2$范数惩罚项指的是模型权重参数每个元素的平方和与一个正的常数的乘积。以线性回归中的线性回归损失函数为例</p><script type="math/tex; mode=display"> \ell(w_1, w_2, b) = \frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right)^2</script><p>其中$w_1, w_2$是权重参数，$b$是偏差参数，样本$i$的输入为$x_1^{(i)}, x_2^{(i)}$，标签为$y^{(i)}$，样本数为$n$。将权重参数用向量$\boldsymbol{w} = [w_1, w_2]$表示，带有$L_2$范数惩罚项的新损失函数为</p><script type="math/tex; mode=display">\ell(w_1, w_2, b) + \frac{\lambda}{2n} |\boldsymbol{w}|^2,</script><p>其中超参数$\lambda &gt; 0$。当权重参数均为0时，惩罚项最小。当$\lambda$较大时，惩罚项在损失函数中的比重较大，这通常会使学到的权重参数的元素较接近0。当$\lambda$设为0时，惩罚项完全不起作用。上式中$L_2$范数平方$|\boldsymbol{w}|^2$展开后得到$w_1^2 + w_2^2$。<br>有了$L_2$范数惩罚项后，在小批量随机梯度下降中，我们将线性回归一节中权重$w_1$和$w_2$的迭代方式更改为</p><script type="math/tex; mode=display"> \begin{aligned} w_1 &\leftarrow \left(1- \frac{\eta\lambda}{|\mathcal{B}|} \right)w_1 - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_1^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right),\\ w_2 &\leftarrow \left(1- \frac{\eta\lambda}{|\mathcal{B}|} \right)w_2 - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_2^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right). \end{aligned}</script><p>可见，$L_2$范数正则化令权重$w_1$和$w_2$先自乘小于1的数，再减去不含惩罚项的梯度。因此，$L_2$范数正则化又叫权重衰减。权重衰减通过惩罚绝对值较大的模型参数为需要学习的模型增加了限制，这可能对过拟合有效。</p><h2 id="高维线性回归实验从零开始的实现"><a href="#高维线性回归实验从零开始的实现" class="headerlink" title="高维线性回归实验从零开始的实现"></a>高维线性回归实验从零开始的实现</h2><p>下面，我们以高维线性回归为例来引入一个过拟合问题，并使用权重衰减来应对过拟合。设数据样本特征的维度为$p$。对于训练数据集和测试数据集中特征为$x_1, x_2, \ldots, x_p$的任一样本，我们使用如下的线性函数来生成该样本的标签：</p><script type="math/tex; mode=display"> y = 0.05 + \sum_{i = 1}^p 0.01x_i + \epsilon</script><p>其中噪声项$\epsilon$服从均值为0、标准差为0.01的正态分布。为了较容易地观察过拟合，我们考虑高维线性回归问题，如设维度$p=200$；同时，我们特意把训练数据集的样本数设低，如20。</p><pre><code class="lang-python">%matplotlib inlineimport torchimport torch.nn as nnimport numpy as npimport syssys.path.append(&quot;/home/kesci/input&quot;)import d2lzh1981 as d2lprint(torch.__version__)</code></pre><pre><code>1.3.0</code></pre><h2 id="初始化模型参数-1"><a href="#初始化模型参数-1" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h2><p>与前面观察过拟合和欠拟合现象的时候相似，在这里不再解释。</p><pre><code class="lang-python">n_train, n_test, num_inputs = 20, 100, 200true_w, true_b = torch.ones(num_inputs, 1) * 0.01, 0.05features = torch.randn((n_train + n_test, num_inputs))labels = torch.matmul(features, true_w) + true_blabels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float)train_features, test_features = features[:n_train, :], features[n_train:, :]train_labels, test_labels = labels[:n_train], labels[n_train:]</code></pre><pre><code class="lang-python"># 定义参数初始化函数，初始化模型参数并且附上梯度def init_params():    w = torch.randn((num_inputs, 1), requires_grad=True)    b = torch.zeros(1, requires_grad=True)    return [w, b]</code></pre><h2 id="定义L2范数惩罚项"><a href="#定义L2范数惩罚项" class="headerlink" title="定义L2范数惩罚项"></a>定义L2范数惩罚项</h2><pre><code class="lang-python">def l2_penalty(w):    return (w**2).sum() / 2</code></pre><h2 id="定义训练和测试"><a href="#定义训练和测试" class="headerlink" title="定义训练和测试"></a>定义训练和测试</h2><pre><code class="lang-python">batch_size, num_epochs, lr = 1, 100, 0.003net, loss = d2l.linreg, d2l.squared_lossdataset = torch.utils.data.TensorDataset(train_features, train_labels)train_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True)def fit_and_plot(lambd):    w, b = init_params()    train_ls, test_ls = [], []    for _ in range(num_epochs):        for X, y in train_iter:            # 添加了L2范数惩罚项            l = loss(net(X, w, b), y) + lambd * l2_penalty(w)            l = l.sum()            if w.grad is not None:                w.grad.data.zero_()                b.grad.data.zero_()            l.backward()            d2l.sgd([w, b], lr, batch_size)        train_ls.append(loss(net(train_features, w, b), train_labels).mean().item())        test_ls.append(loss(net(test_features, w, b), test_labels).mean().item())    d2l.semilogy(range(1, num_epochs + 1), train_ls, &#39;epochs&#39;, &#39;loss&#39;,                 range(1, num_epochs + 1), test_ls, [&#39;train&#39;, &#39;test&#39;])    print(&#39;L2 norm of w:&#39;, w.norm().item())</code></pre><h2 id="观察过拟合"><a href="#观察过拟合" class="headerlink" title="观察过拟合"></a>观察过拟合</h2><pre><code class="lang-python">fit_and_plot(lambd=0)</code></pre><pre><code>L2 norm of w: 11.6444091796875</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/C27406AAA0FD41C6801D55ED4B25D5EA/q5jf5cs7lp.svg"></p><h2 id="使用权重衰减"><a href="#使用权重衰减" class="headerlink" title="使用权重衰减"></a>使用权重衰减</h2><pre><code class="lang-python">fit_and_plot(lambd=3)</code></pre><pre><code>L2 norm of w: 0.04063604772090912</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/0770D8C23B8144C59D13D24390E471F0/q5jf5d4mwp.svg"></p><h2 id="简洁实现"><a href="#简洁实现" class="headerlink" title="简洁实现"></a>简洁实现</h2><pre><code class="lang-python">def fit_and_plot_pytorch(wd):    # 对权重参数衰减。权重名称一般是以weight结尾    net = nn.Linear(num_inputs, 1)    nn.init.normal_(net.weight, mean=0, std=1)    nn.init.normal_(net.bias, mean=0, std=1)    optimizer_w = torch.optim.SGD(params=[net.weight], lr=lr, weight_decay=wd) # 对权重参数衰减    optimizer_b = torch.optim.SGD(params=[net.bias], lr=lr)  # 不对偏差参数衰减    train_ls, test_ls = [], []    for _ in range(num_epochs):        for X, y in train_iter:            l = loss(net(X), y).mean()            optimizer_w.zero_grad()            optimizer_b.zero_grad()            l.backward()            # 对两个optimizer实例分别调用step函数，从而分别更新权重和偏差            optimizer_w.step()            optimizer_b.step()        train_ls.append(loss(net(train_features), train_labels).mean().item())        test_ls.append(loss(net(test_features), test_labels).mean().item())    d2l.semilogy(range(1, num_epochs + 1), train_ls, &#39;epochs&#39;, &#39;loss&#39;,                 range(1, num_epochs + 1), test_ls, [&#39;train&#39;, &#39;test&#39;])    print(&#39;L2 norm of w:&#39;, net.weight.data.norm().item())</code></pre><pre><code class="lang-python">fit_and_plot_pytorch(0)</code></pre><pre><code>L2 norm of w: 13.361410140991211</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/525D01167F0E40509495588D6B0A0FB9/q5jf5e5i21.svg"></p><pre><code class="lang-python">fit_and_plot_pytorch(3)</code></pre><pre><code>L2 norm of w: 0.051789578050374985</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/3FAACA854B9545A8ADADDEB6EE17A680/q5jf5fa51u.svg"></p><h1 id="丢弃法"><a href="#丢弃法" class="headerlink" title="丢弃法"></a>丢弃法</h1><p>多层感知机中神经网络图描述了一个单隐藏层的多层感知机。其中输入个数为4，隐藏单元个数为5，且隐藏单元$h_i$（$i=1, \ldots, 5$）的计算表达式为</p><script type="math/tex; mode=display"> h_i = \phi\left(x_1 w_{1i} + x_2 w_{2i} + x_3 w_{3i} + x_4 w_{4i} + b_i\right)</script><p>这里$\phi$是激活函数，$x_1, \ldots, x_4$是输入，隐藏单元$i$的权重参数为$w_{1i}, \ldots, w_{4i}$，偏差参数为$b_i$。当对该隐藏层使用丢弃法时，该层的隐藏单元将有一定概率被丢弃掉。设丢弃概率为$p$，那么有$p$的概率$h_i$会被清零，有$1-p$的概率$h_i$会除以$1-p$做拉伸。丢弃概率是丢弃法的超参数。具体来说，设随机变量$\xi_i$为0和1的概率分别为$p$和$1-p$。使用丢弃法时我们计算新的隐藏单元$h_i’$</p><script type="math/tex; mode=display"> h_i' = \frac{\xi_i}{1-p} h_i</script><p>由于$E(\xi_i) = 1-p$，因此</p><script type="math/tex; mode=display"> E(h_i') = \frac{E(\xi_i)}{1-p}h_i = h_i</script><p>即丢弃法不改变其输入的期望值。让我们对之前多层感知机的神经网络中的隐藏层使用丢弃法，一种可能的结果如图所示，其中$h_2$和$h_5$被清零。这时输出值的计算不再依赖$h_2$和$h_5$，在反向传播时，与这两个隐藏单元相关的权重的梯度均为0。由于在训练中隐藏层神经元的丢弃是随机的，即$h_1, \ldots, h_5$都有可能被清零，输出层的计算无法过度依赖$h_1, \ldots, h_5$中的任一个，从而在训练模型时起到正则化的作用，并可以用来应对过拟合。在测试模型时，我们为了拿到更加确定性的结果，一般不使用丢弃法</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/upload/image/q5jd69in3m.png?imageView2/0/w/960/h/960" alt="Image Name"></p><h2 id="丢弃法从零开始的实现"><a href="#丢弃法从零开始的实现" class="headerlink" title="丢弃法从零开始的实现"></a>丢弃法从零开始的实现</h2><pre><code class="lang-python">%matplotlib inlineimport torchimport torch.nn as nnimport numpy as npimport syssys.path.append(&quot;/home/kesci/input&quot;)import d2lzh1981 as d2lprint(torch.__version__)</code></pre><pre><code>1.3.0</code></pre><pre><code class="lang-python">def dropout(X, drop_prob):    X = X.float()    assert 0 &lt;= drop_prob &lt;= 1    keep_prob = 1 - drop_prob    # 这种情况下把全部元素都丢弃    if keep_prob == 0:        return torch.zeros_like(X)    mask = (torch.rand(X.shape) &lt; keep_prob).float()    return mask * X / keep_prob</code></pre><pre><code class="lang-python">X = torch.arange(16).view(2, 8)dropout(X, 0)</code></pre><pre><code>tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],        [ 8.,  9., 10., 11., 12., 13., 14., 15.]])</code></pre><pre><code class="lang-python">dropout(X, 0.5)</code></pre><pre><code>tensor([[ 0.,  0.,  0.,  6.,  8., 10.,  0., 14.],        [ 0.,  0., 20.,  0.,  0.,  0., 28.,  0.]])</code></pre><pre><code class="lang-python">dropout(X, 1.0)</code></pre><pre><code>tensor([[0., 0., 0., 0., 0., 0., 0., 0.],        [0., 0., 0., 0., 0., 0., 0., 0.]])</code></pre><pre><code class="lang-python"># 参数的初始化num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256W1 = torch.tensor(np.random.normal(0, 0.01, size=(num_inputs, num_hiddens1)), dtype=torch.float, requires_grad=True)b1 = torch.zeros(num_hiddens1, requires_grad=True)W2 = torch.tensor(np.random.normal(0, 0.01, size=(num_hiddens1, num_hiddens2)), dtype=torch.float, requires_grad=True)b2 = torch.zeros(num_hiddens2, requires_grad=True)W3 = torch.tensor(np.random.normal(0, 0.01, size=(num_hiddens2, num_outputs)), dtype=torch.float, requires_grad=True)b3 = torch.zeros(num_outputs, requires_grad=True)params = [W1, b1, W2, b2, W3, b3]</code></pre><pre><code class="lang-python">drop_prob1, drop_prob2 = 0.2, 0.5def net(X, is_training=True):    X = X.view(-1, num_inputs)    H1 = (torch.matmul(X, W1) + b1).relu()    if is_training:  # 只在训练模型时使用丢弃法        H1 = dropout(H1, drop_prob1)  # 在第一层全连接后添加丢弃层    H2 = (torch.matmul(H1, W2) + b2).relu()    if is_training:        H2 = dropout(H2, drop_prob2)  # 在第二层全连接后添加丢弃层    return torch.matmul(H2, W3) + b3</code></pre><pre><code class="lang-python">def evaluate_accuracy(data_iter, net):    acc_sum, n = 0.0, 0    for X, y in data_iter:        if isinstance(net, torch.nn.Module):            net.eval() # 评估模式, 这会关闭dropout            acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()            net.train() # 改回训练模式        else: # 自定义的模型            if(&#39;is_training&#39; in net.__code__.co_varnames): # 如果有is_training这个参数                # 将is_training设置成False                acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item()             else:                acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()         n += y.shape[0]    return acc_sum / n</code></pre><pre><code class="lang-python">num_epochs, lr, batch_size = 5, 100.0, 256  # 这里的学习率设置的很大，原因与之前相同。loss = torch.nn.CrossEntropyLoss()train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, root=&#39;/home/kesci/input/FashionMNIST2065&#39;)d2l.train_ch3(    net,    train_iter,    test_iter,    loss,    num_epochs,    batch_size,    params,    lr)</code></pre><pre><code>epoch 1, loss 0.0046, train acc 0.549, test acc 0.704epoch 2, loss 0.0023, train acc 0.785, test acc 0.737epoch 3, loss 0.0019, train acc 0.825, test acc 0.834epoch 4, loss 0.0017, train acc 0.842, test acc 0.763epoch 5, loss 0.0016, train acc 0.848, test acc 0.813</code></pre><h2 id="简洁实现-1"><a href="#简洁实现-1" class="headerlink" title="简洁实现"></a>简洁实现</h2><pre><code class="lang-python">net = nn.Sequential(        d2l.FlattenLayer(),        nn.Linear(num_inputs, num_hiddens1),        nn.ReLU(),        nn.Dropout(drop_prob1),        nn.Linear(num_hiddens1, num_hiddens2),         nn.ReLU(),        nn.Dropout(drop_prob2),        nn.Linear(num_hiddens2, 10)        )for param in net.parameters():    nn.init.normal_(param, mean=0, std=0.01)</code></pre><pre><code class="lang-python">optimizer = torch.optim.SGD(net.parameters(), lr=0.5)d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, optimizer)</code></pre><pre><code>epoch 1, loss 0.0046, train acc 0.553, test acc 0.736epoch 2, loss 0.0023, train acc 0.785, test acc 0.803epoch 3, loss 0.0019, train acc 0.818, test acc 0.756epoch 4, loss 0.0018, train acc 0.835, test acc 0.829epoch 5, loss 0.0016, train acc 0.848, test acc 0.851</code></pre><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul><li><p>欠拟合现象：模型无法达到一个较低的误差</p></li><li><p>过拟合现象：训练误差较低但是泛化误差依然较高，二者相差较大</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动手深度学习(三):多层感知机</title>
      <link href="/2020/02/14/dl-notes3-mlp/"/>
      <url>/2020/02/14/dl-notes3-mlp/</url>
      
        <content type="html"><![CDATA[<h1 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h1><ol><li>多层感知机的基本知识</li><li>使用多层感知机图像分类的从零开始的实现</li><li>使用pytorch的简洁实现</li></ol><h2 id="多层感知机的基本知识"><a href="#多层感知机的基本知识" class="headerlink" title="多层感知机的基本知识"></a>多层感知机的基本知识</h2><p>深度学习主要关注多层模型。在这里，我们将以多层感知机（multilayer perceptron，MLP）为例，介绍多层神经网络的概念。</p><h3 id="隐藏层"><a href="#隐藏层" class="headerlink" title="隐藏层"></a>隐藏层</h3><p>下图展示了一个多层感知机的神经网络图，它含有一个隐藏层，该层中有5个隐藏单元。</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/upload/image/q5ho684jmh.png" alt="Image Name"></p><h3 id="表达公式"><a href="#表达公式" class="headerlink" title="表达公式"></a>表达公式</h3><p>具体来说，给定一个小批量样本$\boldsymbol{X} \in \mathbb{R}^{n \times d}$，其批量大小为$n$，输入个数为$d$。假设多层感知机只有一个隐藏层，其中隐藏单元个数为$h$。记隐藏层的输出（也称为隐藏层变量或隐藏变量）为$\boldsymbol{H}$，有$\boldsymbol{H} \in \mathbb{R}^{n \times h}$。因为隐藏层和输出层均是全连接层，可以设隐藏层的权重参数和偏差参数分别为$\boldsymbol{W}_h \in \mathbb{R}^{d \times h}$和 $\boldsymbol{b}_h \in \mathbb{R}^{1 \times h}$，输出层的权重和偏差参数分别为$\boldsymbol{W}_o \in \mathbb{R}^{h \times q}$和$\boldsymbol{b}_o \in \mathbb{R}^{1 \times q}$。</p><p>我们先来看一种含单隐藏层的多层感知机的设计。其输出$\boldsymbol{O} \in \mathbb{R}^{n \times q}$的计算为</p><script type="math/tex; mode=display"> \begin{aligned} \boldsymbol{H} &= \boldsymbol{X} \boldsymbol{W}_h + \boldsymbol{b}_h,\\ \boldsymbol{O} &= \boldsymbol{H} \boldsymbol{W}_o + \boldsymbol{b}_o, \end{aligned}</script><p>也就是将隐藏层的输出直接作为输出层的输入。如果将以上两个式子联立起来，可以得到</p><script type="math/tex; mode=display"> \boldsymbol{O} = (\boldsymbol{X} \boldsymbol{W}_h + \boldsymbol{b}_h)\boldsymbol{W}_o + \boldsymbol{b}_o = \boldsymbol{X} \boldsymbol{W}_h\boldsymbol{W}_o + \boldsymbol{b}_h \boldsymbol{W}_o + \boldsymbol{b}_o.</script><p>从联立后的式子可以看出，虽然神经网络引入了隐藏层，却依然等价于一个单层神经网络：其中输出层权重参数为$\boldsymbol{W}_h\boldsymbol{W}_o$，偏差参数为$\boldsymbol{b}_h \boldsymbol{W}_o + \boldsymbol{b}_o$。不难发现，即便再添加更多的隐藏层，以上设计依然只能与仅含输出层的单层神经网络等价。</p><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>上述问题的根源在于全连接层只是对数据做仿射变换（affine transformation），而多个仿射变换的叠加仍然是一个仿射变换。解决问题的一个方法是引入非线性变换，例如对隐藏变量使用按元素运算的非线性函数进行变换，然后再作为下一个全连接层的输入。这个非线性函数被称为激活函数（activation function）。</p><p>下面我们介绍几个常用的激活函数：</p><h4 id="ReLU函数"><a href="#ReLU函数" class="headerlink" title="ReLU函数"></a>ReLU函数</h4><p>ReLU（rectified linear unit）函数提供了一个很简单的非线性变换。给定元素$x$，该函数定义为</p><script type="math/tex; mode=display">\text{ReLU}(x) = \max(x, 0).</script><p>可以看出，ReLU函数只保留正数元素，并将负数元素清零。为了直观地观察这一非线性变换，我们先定义一个绘图函数xyplot。</p><pre><code class="lang-python">%matplotlib inlineimport torchimport numpy as npimport matplotlib.pyplot as pltimport syssys.path.append(&quot;/home/kesci/input&quot;)import d2lzh1981 as d2lprint(torch.__version__)</code></pre><pre><code>1.3.0</code></pre><pre><code class="lang-python">def xyplot(x_vals, y_vals, name):    # d2l.set_figsize(figsize=(5, 2.5))    plt.plot(x_vals.detach().numpy(), y_vals.detach().numpy())    plt.xlabel(&#39;x&#39;)    plt.ylabel(name + &#39;(x)&#39;)</code></pre><pre><code class="lang-python">x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)y = x.relu()xyplot(x, y, &#39;relu&#39;)</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/070825B6A382411DA5BD7D14E67E8D54/q5hv7cdtna.png"></p><pre><code class="lang-python">y.sum().backward()xyplot(x, x.grad, &#39;grad of relu&#39;)</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/BFB05150DBD1474D9A9ECCB9CDF1DD39/q5hv7c3pxb.png"></p><h4 id="Sigmoid函数"><a href="#Sigmoid函数" class="headerlink" title="Sigmoid函数"></a>Sigmoid函数</h4><p>sigmoid函数可以将元素的值变换到0和1之间：</p><script type="math/tex; mode=display">\text{sigmoid}(x) = \frac{1}{1 + \exp(-x)}.</script><pre><code class="lang-python">y = x.sigmoid()xyplot(x, y, &#39;sigmoid&#39;)</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/68FCB4E8142144458F13128B370D1C91/q5hv7dor11.png"></p><p>依据链式法则，sigmoid函数的导数</p><script type="math/tex; mode=display">\text{sigmoid}'(x) = \text{sigmoid}(x)\left(1-\text{sigmoid}(x)\right).</script><p>下面绘制了sigmoid函数的导数。当输入为0时，sigmoid函数的导数达到最大值0.25；当输入越偏离0时，sigmoid函数的导数越接近0。</p><pre><code class="lang-python">x.grad.zero_()y.sum().backward()xyplot(x, x.grad, &#39;grad of sigmoid&#39;)</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/878C7B8823304F72860965E119A21412/q5hv7dpse9.png"></p><h4 id="tanh函数"><a href="#tanh函数" class="headerlink" title="tanh函数"></a>tanh函数</h4><p>tanh（双曲正切）函数可以将元素的值变换到-1和1之间：</p><script type="math/tex; mode=display">\text{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}.</script><p>我们接着绘制tanh函数。当输入接近0时，tanh函数接近线性变换。虽然该函数的形状和sigmoid函数的形状很像，但tanh函数在坐标系的原点上对称。</p><pre><code class="lang-python">y = x.tanh()xyplot(x, y, &#39;tanh&#39;)</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/92D16076309F42169482834C0B6ABB24/q5hv7dfeso.png"></p><p>依据链式法则，tanh函数的导数</p><script type="math/tex; mode=display">\text{tanh}'(x) = 1 - \text{tanh}^2(x).</script><p>下面绘制了tanh函数的导数。当输入为0时，tanh函数的导数达到最大值1；当输入越偏离0时，tanh函数的导数越接近0。</p><pre><code class="lang-python">x.grad.zero_()y.sum().backward()xyplot(x, x.grad, &#39;grad of tanh&#39;)</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/CB16F4B33E664E14BCE8E52D8B37C47F/q5hv7ejc8y.png"></p><h3 id="关于激活函数的选择"><a href="#关于激活函数的选择" class="headerlink" title="关于激活函数的选择"></a>关于激活函数的选择</h3><p>ReLu函数是一个通用的激活函数，目前在大多数情况下使用。但是，ReLU函数只能在隐藏层中使用。</p><p>用于分类器时，sigmoid函数及其组合通常效果更好。由于梯度消失问题，有时要避免使用sigmoid和tanh函数。  </p><p>在神经网络层数较多的时候，最好使用ReLu函数，ReLu函数比较简单计算量少，而sigmoid和tanh函数计算量大很多。</p><p>在选择激活函数的时候可以先选用ReLu函数如果效果不理想可以尝试其他激活函数。</p><h3 id="多层感知机-1"><a href="#多层感知机-1" class="headerlink" title="多层感知机"></a>多层感知机</h3><p>多层感知机就是含有至少一个隐藏层的由全连接层组成的神经网络，且每个隐藏层的输出通过激活函数进行变换。多层感知机的层数和各隐藏层中隐藏单元个数都是超参数。以单隐藏层为例并沿用本节之前定义的符号，多层感知机按以下方式计算输出：</p><script type="math/tex; mode=display"> \begin{aligned} \boldsymbol{H} &= \phi(\boldsymbol{X} \boldsymbol{W}_h + \boldsymbol{b}_h),\\ \boldsymbol{O} &= \boldsymbol{H} \boldsymbol{W}_o + \boldsymbol{b}_o, \end{aligned}</script><p>其中$\phi$表示激活函数。</p><h2 id="多层感知机从零开始的实现"><a href="#多层感知机从零开始的实现" class="headerlink" title="多层感知机从零开始的实现"></a>多层感知机从零开始的实现</h2><pre><code class="lang-python">import torchimport numpy as npimport syssys.path.append(&quot;/home/kesci/input&quot;)import d2lzh1981 as d2lprint(torch.__version__)</code></pre><pre><code>1.3.0</code></pre><h3 id="获取训练集"><a href="#获取训练集" class="headerlink" title="获取训练集"></a>获取训练集</h3><pre><code class="lang-python">batch_size = 256train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size,root=&#39;/home/kesci/input/FashionMNIST2065&#39;)</code></pre><h3 id="定义模型参数"><a href="#定义模型参数" class="headerlink" title="定义模型参数"></a>定义模型参数</h3><pre><code class="lang-python">num_inputs, num_outputs, num_hiddens = 784, 10, 256W1 = torch.tensor(np.random.normal(0, 0.01, (num_inputs, num_hiddens)), dtype=torch.float)b1 = torch.zeros(num_hiddens, dtype=torch.float)W2 = torch.tensor(np.random.normal(0, 0.01, (num_hiddens, num_outputs)), dtype=torch.float)b2 = torch.zeros(num_outputs, dtype=torch.float)params = [W1, b1, W2, b2]for param in params:    param.requires_grad_(requires_grad=True)</code></pre><h3 id="定义激活函数"><a href="#定义激活函数" class="headerlink" title="定义激活函数"></a>定义激活函数</h3><pre><code class="lang-python">def relu(X):    return torch.max(input=X, other=torch.tensor(0.0))</code></pre><h3 id="定义网络"><a href="#定义网络" class="headerlink" title="定义网络"></a>定义网络</h3><pre><code class="lang-python">def net(X):    X = X.view((-1, num_inputs))    H = relu(torch.matmul(X, W1) + b1)    return torch.matmul(H, W2) + b2</code></pre><h3 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h3><pre><code class="lang-python">loss = torch.nn.CrossEntropyLoss()</code></pre><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><pre><code class="lang-python">num_epochs, lr = 5, 100.0# def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,#               params=None, lr=None, optimizer=None):#     for epoch in range(num_epochs):#         train_l_sum, train_acc_sum, n = 0.0, 0.0, 0#         for X, y in train_iter:#             y_hat = net(X)#             l = loss(y_hat, y).sum()#             #             # 梯度清零#             if optimizer is not None:#                 optimizer.zero_grad()#             elif params is not None and params[0].grad is not None:#                 for param in params:#                     param.grad.data.zero_()#            #             l.backward()#             if optimizer is None:#                 d2l.sgd(params, lr, batch_size)#             else:#                 optimizer.step()  # “softmax回归的简洁实现”一节将用到#             #             #             train_l_sum += l.item()#             train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()#             n += y.shape[0]#         test_acc = evaluate_accuracy(test_iter, net)#         print(&#39;epoch %d, loss %.4f, train acc %.3f, test acc %.3f&#39;#               % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr)</code></pre><pre><code>epoch 1, loss 0.0030, train acc 0.712, test acc 0.806epoch 2, loss 0.0019, train acc 0.821, test acc 0.806epoch 3, loss 0.0017, train acc 0.847, test acc 0.825epoch 4, loss 0.0015, train acc 0.856, test acc 0.834epoch 5, loss 0.0015, train acc 0.863, test acc 0.847</code></pre><h2 id="多层感知机pytorch实现"><a href="#多层感知机pytorch实现" class="headerlink" title="多层感知机pytorch实现"></a>多层感知机pytorch实现</h2><pre><code class="lang-python">import torchfrom torch import nnfrom torch.nn import initimport numpy as npimport syssys.path.append(&quot;/home/kesci/input&quot;)import d2lzh1981 as d2lprint(torch.__version__)</code></pre><pre><code>1.3.0</code></pre><h3 id="初始化模型和各个参数"><a href="#初始化模型和各个参数" class="headerlink" title="初始化模型和各个参数"></a>初始化模型和各个参数</h3><pre><code class="lang-python">num_inputs, num_outputs, num_hiddens = 784, 10, 256net = nn.Sequential(        d2l.FlattenLayer(),        nn.Linear(num_inputs, num_hiddens),        nn.ReLU(),        nn.Linear(num_hiddens, num_outputs),         )for params in net.parameters():    init.normal_(params, mean=0, std=0.01)</code></pre><h3 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h3><pre><code class="lang-python">batch_size = 256train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size,root=&#39;/home/kesci/input/FashionMNIST2065&#39;)loss = torch.nn.CrossEntropyLoss()optimizer = torch.optim.SGD(net.parameters(), lr=0.5)num_epochs = 5d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, optimizer)</code></pre><pre><code>epoch 1, loss 0.0031, train acc 0.701, test acc 0.774epoch 2, loss 0.0019, train acc 0.821, test acc 0.806epoch 3, loss 0.0017, train acc 0.841, test acc 0.805epoch 4, loss 0.0015, train acc 0.855, test acc 0.834epoch 5, loss 0.0014, train acc 0.866, test acc 0.840</code></pre><h2 id="练习题"><a href="#练习题" class="headerlink" title="练习题"></a>练习题</h2><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/amdb6W3tQBRyuXE.png" alt=""></p><p>256x256x1000 + 1000x10 = 65546000</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动手深度学习(二):Softmax</title>
      <link href="/2020/02/14/dl-notes2-softmax/"/>
      <url>/2020/02/14/dl-notes2-softmax/</url>
      
        <content type="html"><![CDATA[<h1 id="softmax和分类模型"><a href="#softmax和分类模型" class="headerlink" title="softmax和分类模型"></a>softmax和分类模型</h1><p>内容包含：</p><ol><li>softmax回归的基本概念</li><li>如何获取Fashion-MNIST数据集和读取数据</li><li>softmax回归模型的从零开始实现，实现一个对Fashion-MNIST训练集中的图像数据进行分类的模型</li><li>使用pytorch重新实现softmax回归模型</li></ol><h2 id="softmax的基本概念"><a href="#softmax的基本概念" class="headerlink" title="softmax的基本概念"></a>softmax的基本概念</h2><ul><li><p>分类问题<br>一个简单的图像分类问题，输入图像的高和宽均为2像素，色彩为灰度。<br>图像中的4像素分别记为$x_1, x_2, x_3, x_4$。<br>假设真实标签为狗、猫或者鸡，这些标签对应的离散值为$y_1, y_2, y_3$。<br>我们通常使用离散的数值来表示类别，例如$y_1=1, y_2=2, y_3=3$。</p></li><li><p>权重矢量  </p><script type="math/tex; mode=display">\begin{aligned} o_1 &= x_1 w_{11} + x_2 w_{21} + x_3 w_{31} + x_4 w_{41} + b_1 \end{aligned}</script></li></ul><script type="math/tex; mode=display"> \begin{aligned} o_2 &= x_1 w_{12} + x_2 w_{22} + x_3 w_{32} + x_4 w_{42} + b_2 \end{aligned}</script><script type="math/tex; mode=display"> \begin{aligned} o_3 &= x_1 w_{13} + x_2 w_{23} + x_3 w_{33} + x_4 w_{43} + b_3 \end{aligned}</script><ul><li>神经网络图<br>下图用神经网络图描绘了上面的计算。softmax回归同线性回归一样，也是一个单层神经网络。由于每个输出$o_1, o_2, o_3$的计算都要依赖于所有的输入$x_1, x_2, x_3, x_4$，softmax回归的输出层也是一个全连接层。</li></ul><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/upload/image/q5hmymezog.png" alt="Image Name"></p><script type="math/tex; mode=display">\begin{aligned}softmax回归是一个单层神经网络\end{aligned}</script><p>既然分类问题需要得到离散的预测输出，一个简单的办法是将输出值$o_i$当作预测类别是$i$的置信度，并将值最大的输出所对应的类作为预测输出，即输出 $\underset{i}{\arg\max} o_i$。例如，如果$o_1,o_2,o_3$分别为$0.1,10,0.1$，由于$o_2$最大，那么预测类别为2，其代表猫。</p><ul><li>输出问题<br>直接使用输出层的输出有两个问题：<ol><li>一方面，由于输出层的输出值的范围不确定，我们难以直观上判断这些值的意义。例如，刚才举的例子中的输出值10表示“很置信”图像类别为猫，因为该输出值是其他两类的输出值的100倍。但如果$o_1=o_3=10^3$，那么输出值10却又表示图像类别为猫的概率很低。</li><li>另一方面，由于真实标签是离散值，这些离散值与不确定范围的输出值之间的误差难以衡量。</li></ol></li></ul><p>softmax运算符（softmax operator）解决了以上两个问题。它通过下式将输出值变换成值为正且和为1的概率分布：</p><script type="math/tex; mode=display"> \hat{y}_1, \hat{y}_2, \hat{y}_3 = \text{softmax}(o_1, o_2, o_3)</script><p>其中</p><script type="math/tex; mode=display"> \hat{y}1 = \frac{ \exp(o_1)}{\sum_{i=1}^3 \exp(o_i)},\quad \hat{y}2 = \frac{ \exp(o_2)}{\sum_{i=1}^3 \exp(o_i)},\quad \hat{y}3 = \frac{ \exp(o_3)}{\sum_{i=1}^3 \exp(o_i)}.</script><p>容易看出$\hat{y}_1 + \hat{y}_2 + \hat{y}_3 = 1$且$0 \leq \hat{y}_1, \hat{y}_2, \hat{y}_3 \leq 1$，因此$\hat{y}_1, \hat{y}_2, \hat{y}_3$是一个合法的概率分布。这时候，如果$\hat{y}_2=0.8$，不管$\hat{y}_1$和$\hat{y}_3$的值是多少，我们都知道图像类别为猫的概率是80%。此外，我们注意到</p><script type="math/tex; mode=display"> \underset{i}{\arg\max} o_i = \underset{i}{\arg\max} \hat{y}_i</script><p>因此softmax运算不改变预测类别输出。</p><ul><li>计算效率<ul><li>单样本矢量计算表达式<br>为了提高计算效率，我们可以将单样本分类通过矢量计算来表达。在上面的图像分类问题中，假设softmax回归的权重和偏差参数分别为</li></ul></li></ul><script type="math/tex; mode=display"> \boldsymbol{W} = \begin{bmatrix} w_{11} & w_{12} & w_{13} \\ w_{21} & w_{22} & w_{23} \\ w_{31} & w_{32} & w_{33} \\ w_{41} & w_{42} & w_{43} \end{bmatrix},\quad \boldsymbol{b} = \begin{bmatrix} b_1 & b_2 & b_3 \end{bmatrix},</script><p>设高和宽分别为2个像素的图像样本$i$的特征为</p><script type="math/tex; mode=display">\boldsymbol{x}^{(i)} = \begin{bmatrix}x_1^{(i)} & x_2^{(i)} & x_3^{(i)} & x_4^{(i)}\end{bmatrix},</script><p>输出层的输出为</p><script type="math/tex; mode=display">\boldsymbol{o}^{(i)} = \begin{bmatrix}o_1^{(i)} & o_2^{(i)} & o_3^{(i)}\end{bmatrix},</script><p>预测为狗、猫或鸡的概率分布为</p><script type="math/tex; mode=display">\boldsymbol{\hat{y}}^{(i)} = \begin{bmatrix}\hat{y}_1^{(i)} & \hat{y}_2^{(i)} & \hat{y}_3^{(i)}\end{bmatrix}.</script><p>softmax回归对样本$i$分类的矢量计算表达式为</p><script type="math/tex; mode=display"> \begin{aligned} \boldsymbol{o}^{(i)} &= \boldsymbol{x}^{(i)} \boldsymbol{W} + \boldsymbol{b},\\ \boldsymbol{\hat{y}}^{(i)} &= \text{softmax}(\boldsymbol{o}^{(i)}). \end{aligned}</script><ul><li>小批量矢量计算表达式<br>  为了进一步提升计算效率，我们通常对小批量数据做矢量计算。广义上讲，给定一个小批量样本，其批量大小为$n$，输入个数（特征数）为$d$，输出个数（类别数）为$q$。设批量特征为$\boldsymbol{X} \in \mathbb{R}^{n \times d}$。假设softmax回归的权重和偏差参数分别为$\boldsymbol{W} \in \mathbb{R}^{d \times q}$和$\boldsymbol{b} \in \mathbb{R}^{1 \times q}$。softmax回归的矢量计算表达式为</li></ul><script type="math/tex; mode=display"> \begin{aligned} \boldsymbol{O} &= \boldsymbol{X} \boldsymbol{W} + \boldsymbol{b},\\ \boldsymbol{\hat{Y}} &= \text{softmax}(\boldsymbol{O}), \end{aligned}</script><p>其中的加法运算使用了广播机制，$\boldsymbol{O}, \boldsymbol{\hat{Y}} \in \mathbb{R}^{n \times q}$且这两个矩阵的第$i$行分别为样本$i$的输出$\boldsymbol{o}^{(i)}$和概率分布$\boldsymbol{\hat{y}}^{(i)}$。</p><h2 id="交叉熵损失函数"><a href="#交叉熵损失函数" class="headerlink" title="交叉熵损失函数"></a>交叉熵损失函数</h2><p>对于样本$i$，我们构造向量$\boldsymbol{y}^{(i)}\in \mathbb{R}^{q}$ ，使其第$y^{(i)}$（样本$i$类别的离散数值）个元素为1，其余为0。这样我们的训练目标可以设为使预测概率分布$\boldsymbol{\hat y}^{(i)}$尽可能接近真实的标签概率分布$\boldsymbol{y}^{(i)}$。</p><ul><li>平方损失估计  </li></ul><script type="math/tex; mode=display">\begin{aligned}Loss = |\boldsymbol{\hat y}^{(i)}-\boldsymbol{y}^{(i)}|^2/2\end{aligned}</script><p>然而，想要预测分类结果正确，我们其实并不需要预测概率完全等于标签概率。例如，在图像分类的例子里，如果$y^{(i)}=3$，那么我们只需要$\hat{y}^{(i)}_3$比其他两个预测值$\hat{y}^{(i)}_1$和$\hat{y}^{(i)}_2$大就行了。即使$\hat{y}^{(i)}_3$值为0.6，不管其他两个预测值为多少，类别预测均正确。而平方损失则过于严格，例如$\hat y^{(i)}_1=\hat y^{(i)}_2=0.2$比$\hat y^{(i)}_1=0, \hat y^{(i)}_2=0.4$的损失要小很多，虽然两者都有同样正确的分类预测结果。</p><p>改善上述问题的一个方法是使用更适合衡量两个概率分布差异的测量函数。其中，交叉熵（cross entropy）是一个常用的衡量方法：</p><script type="math/tex; mode=display">H\left(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}\right ) = -\sum_{j=1}^q y_j^{(i)} \log \hat y_j^{(i)},</script><p>其中带下标的$y_j^{(i)}$是向量$\boldsymbol y^{(i)}$中非0即1的元素，需要注意将它与样本$i$类别的离散数值，即不带下标的$y^{(i)}$区分。在上式中，我们知道向量$\boldsymbol y^{(i)}$中只有第$y^{(i)}$个元素$y^{(i)}{y^{(i)}}$为1，其余全为0，于是$H(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}) = -\log \hat y_{y^{(i)}}^{(i)}$。也就是说，交叉熵只关心对正确类别的预测概率，因为只要其值足够大，就可以确保分类结果正确。当然，遇到一个样本有多个标签时，例如图像里含有不止一个物体时，我们并不能做这一步简化。但即便对于这种情况，交叉熵同样只关心对图像中出现的物体类别的预测概率。</p><p>假设训练数据集的样本数为$n$，交叉熵损失函数定义为 </p><script type="math/tex; mode=display">\ell(\boldsymbol{\Theta}) = \frac{1}{n} \sum_{i=1}^n H\left(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}\right ),</script><p>其中$\boldsymbol{\Theta}$代表模型参数。同样地，如果每个样本只有一个标签，那么交叉熵损失可以简写成$\ell(\boldsymbol{\Theta}) = -(1/n) \sum_{i=1}^n \log \hat y_{y^{(i)}}^{(i)}$。从另一个角度来看，我们知道最小化$\ell(\boldsymbol{\Theta})$等价于最大化$\exp(-n\ell(\boldsymbol{\Theta}))=\prod_{i=1}^n \hat y_{y^{(i)}}^{(i)}$，即最小化交叉熵损失函数等价于最大化训练数据集所有标签类别的联合预测概率。</p><h2 id="模型训练和预测"><a href="#模型训练和预测" class="headerlink" title="模型训练和预测"></a>模型训练和预测</h2><p>在训练好softmax回归模型后，给定任一样本特征，就可以预测每个输出类别的概率。通常，我们把预测概率最大的类别作为输出类别。如果它与真实类别（标签）一致，说明这次预测是正确的。在3.6节的实验中，我们将使用准确率（accuracy）来评价模型的表现。它等于正确预测数量与总预测数量之比。</p><h1 id="获取Fashion-MNIST训练集和读取数据"><a href="#获取Fashion-MNIST训练集和读取数据" class="headerlink" title="获取Fashion-MNIST训练集和读取数据"></a>获取Fashion-MNIST训练集和读取数据</h1><p>在介绍softmax回归的实现前我们先引入一个多类图像分类数据集。它将在后面的章节中被多次使用，以方便我们观察比较算法之间在模型精度和计算效率上的区别。图像分类数据集中最常用的是手写数字识别数据集MNIST[1]。但大部分模型在MNIST上的分类精度都超过了95%。为了更直观地观察算法之间的差异，我们将使用一个图像内容更加复杂的数据集Fashion-MNIST[2]。</p><p>我这里我们会使用torchvision包，它是服务于PyTorch深度学习框架的，主要用来构建计算机视觉模型。torchvision主要由以下几部分构成：</p><ol><li>torchvision.datasets: 一些加载数据的函数及常用的数据集接口；</li><li>torchvision.models: 包含常用的模型结构（含预训练模型），例如AlexNet、VGG、ResNet等；</li><li>torchvision.transforms: 常用的图片变换，例如裁剪、旋转等；</li><li>torchvision.utils: 其他的一些有用的方法。</li></ol><pre><code class="lang-python"># import needed package%matplotlib inlinefrom IPython import displayimport matplotlib.pyplot as pltimport torchimport torchvisionimport torchvision.transforms as transformsimport timeimport syssys.path.append(&quot;/home/kesci/input&quot;)import d2lzh1981 as d2lprint(torch.__version__)print(torchvision.__version__)</code></pre><pre><code>---------------------------------------------------------------------------ModuleNotFoundError                       Traceback (most recent call last)&lt;ipython-input-1-e3b06ab588d2&gt; in &lt;module&gt;      4 import matplotlib.pyplot as plt      5 ----&gt; 6 import torch      7 import torchvision      8 import torchvision.transforms as transformsModuleNotFoundError: No module named &#39;torch&#39;</code></pre><h2 id="get-dataset"><a href="#get-dataset" class="headerlink" title="get dataset"></a>get dataset</h2><pre><code class="lang-python">mnist_train = torchvision.datasets.FashionMNIST(root=&#39;/home/kesci/input/FashionMNIST2065&#39;, train=True, download=True, transform=transforms.ToTensor())mnist_test = torchvision.datasets.FashionMNIST(root=&#39;/home/kesci/input/FashionMNIST2065&#39;, train=False, download=True, transform=transforms.ToTensor())</code></pre><p>class torchvision.datasets.FashionMNIST(root, train=True, transform=None, target_transform=None, download=False)</p><ul><li>root（string）– 数据集的根目录，其中存放processed/training.pt和processed/test.pt文件。</li><li>train（bool, 可选）– 如果设置为True，从training.pt创建数据集，否则从test.pt创建。</li><li>download（bool, 可选）– 如果设置为True，从互联网下载数据并放到root文件夹下。如果root目录下已经存在数据，不会再次下载。</li><li>transform（可被调用 , 可选）– 一种函数或变换，输入PIL图片，返回变换之后的数据。如：transforms.RandomCrop。</li><li>target_transform（可被调用 , 可选）– 一种函数或变换，输入目标，进行变换。</li></ul><pre><code class="lang-python"># show result print(type(mnist_train))print(len(mnist_train), len(mnist_test))</code></pre><pre><code>&lt;class &#39;torchvision.datasets.mnist.FashionMNIST&#39;&gt;60000 10000</code></pre><pre><code class="lang-python"># 我们可以通过下标来访问任意一个样本feature, label = mnist_train[0]print(feature.shape, label)  # Channel x Height x Width</code></pre><pre><code>torch.Size([1, 28, 28]) 9</code></pre><p>如果不做变换输入的数据是图像，我们可以看一下图片的类型参数：</p><pre><code class="lang-python">mnist_PIL = torchvision.datasets.FashionMNIST(root=&#39;/home/kesci/input/FashionMNIST2065&#39;, train=True, download=True)PIL_feature, label = mnist_PIL[0]print(PIL_feature)</code></pre><pre><code>&lt;PIL.Image.Image image mode=L size=28x28 at 0x7F54A41612E8&gt;</code></pre><pre><code class="lang-python"># 本函数已保存在d2lzh包中方便以后使用def get_fashion_mnist_labels(labels):    text_labels = [&#39;t-shirt&#39;, &#39;trouser&#39;, &#39;pullover&#39;, &#39;dress&#39;, &#39;coat&#39;,                   &#39;sandal&#39;, &#39;shirt&#39;, &#39;sneaker&#39;, &#39;bag&#39;, &#39;ankle boot&#39;]    return [text_labels[int(i)] for i in labels]</code></pre><pre><code class="lang-python">def show_fashion_mnist(images, labels):    d2l.use_svg_display()    # 这里的_表示我们忽略（不使用）的变量    _, figs = plt.subplots(1, len(images), figsize=(12, 12))    for f, img, lbl in zip(figs, images, labels):        f.imshow(img.view((28, 28)).numpy())        f.set_title(lbl)        f.axes.get_xaxis().set_visible(False)        f.axes.get_yaxis().set_visible(False)    plt.show()</code></pre><pre><code class="lang-python">X, y = [], []for i in range(10):    X.append(mnist_train[i][0]) # 将第i个feature加到X中    y.append(mnist_train[i][1]) # 将第i个label加到y中show_fashion_mnist(X, get_fashion_mnist_labels(y))</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/056F457B00454FFD81A3CB6AD966C508/q5j7ehijw7.svg"></p><pre><code class="lang-python"># 读取数据batch_size = 256num_workers = 4train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)</code></pre><pre><code class="lang-python">start = time.time()for X, y in train_iter:    continueprint(&#39;%.2f sec&#39; % (time.time() - start))</code></pre><pre><code>4.95 sec</code></pre><h1 id="softmax从零开始的实现"><a href="#softmax从零开始的实现" class="headerlink" title="softmax从零开始的实现"></a>softmax从零开始的实现</h1><pre><code class="lang-python">import torchimport torchvisionimport numpy as npimport syssys.path.append(&quot;/home/kesci/input&quot;)import d2lzh1981 as d2lprint(torch.__version__)print(torchvision.__version__)</code></pre><pre><code>1.3.00.4.1a0+d94043a</code></pre><h2 id="获取训练集数据和测试集数据"><a href="#获取训练集数据和测试集数据" class="headerlink" title="获取训练集数据和测试集数据"></a>获取训练集数据和测试集数据</h2><pre><code class="lang-python">batch_size = 256train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, root=&#39;/home/kesci/input/FashionMNIST2065&#39;)</code></pre><h2 id="模型参数初始化"><a href="#模型参数初始化" class="headerlink" title="模型参数初始化"></a>模型参数初始化</h2><pre><code class="lang-python">num_inputs = 784print(28*28)num_outputs = 10W = torch.tensor(np.random.normal(0, 0.01, (num_inputs, num_outputs)), dtype=torch.float)b = torch.zeros(num_outputs, dtype=torch.float)</code></pre><pre><code>784</code></pre><pre><code class="lang-python">W.requires_grad_(requires_grad=True)b.requires_grad_(requires_grad=True)</code></pre><pre><code>tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)</code></pre><h2 id="对多维Tensor按维度操作"><a href="#对多维Tensor按维度操作" class="headerlink" title="对多维Tensor按维度操作"></a>对多维Tensor按维度操作</h2><pre><code class="lang-python">X = torch.tensor([[1, 2, 3], [4, 5, 6]])print(X.sum(dim=0, keepdim=True))  # dim为0，按照相同的列求和，并在结果中保留列特征print(X.sum(dim=1, keepdim=True))  # dim为1，按照相同的行求和，并在结果中保留行特征print(X.sum(dim=0, keepdim=False)) # dim为0，按照相同的列求和，不在结果中保留列特征print(X.sum(dim=1, keepdim=False)) # dim为1，按照相同的行求和，不在结果中保留行特征</code></pre><pre><code>tensor([[5, 7, 9]])tensor([[ 6],        [15]])tensor([5, 7, 9])tensor([ 6, 15])</code></pre><h2 id="定义softmax操作"><a href="#定义softmax操作" class="headerlink" title="定义softmax操作"></a>定义softmax操作</h2><script type="math/tex; mode=display"> \hat{y}_j = \frac{ \exp(o_j)}{\sum_{i=1}^3 \exp(o_i)}</script><pre><code class="lang-python">def softmax(X):    X_exp = X.exp()    partition = X_exp.sum(dim=1, keepdim=True)    # print(&quot;X size is &quot;, X_exp.size())    # print(&quot;partition size is &quot;, partition, partition.size())    return X_exp / partition  # 这里应用了广播机制</code></pre><pre><code class="lang-python">X = torch.rand((2, 5))X_prob = softmax(X)print(X_prob, &#39;\n&#39;, X_prob.sum(dim=1))</code></pre><pre><code>tensor([[0.2253, 0.1823, 0.1943, 0.2275, 0.1706],        [0.1588, 0.2409, 0.2310, 0.1670, 0.2024]])  tensor([1.0000, 1.0000])</code></pre><h2 id="softmax回归模型"><a href="#softmax回归模型" class="headerlink" title="softmax回归模型"></a>softmax回归模型</h2><script type="math/tex; mode=display"> \begin{aligned} \boldsymbol{o}^{(i)} &= \boldsymbol{x}^{(i)} \boldsymbol{W} + \boldsymbol{b},\\ \boldsymbol{\hat{y}}^{(i)} &= \text{softmax}(\boldsymbol{o}^{(i)}). \end{aligned}</script><pre><code class="lang-python">def net(X):    return softmax(torch.mm(X.view((-1, num_inputs)), W) + b)</code></pre><h2 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h2><script type="math/tex; mode=display">H\left(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}\right ) = -\sum_{j=1}^q y_j^{(i)} \log \hat y_j^{(i)},</script><script type="math/tex; mode=display">\ell(\boldsymbol{\Theta}) = \frac{1}{n} \sum_{i=1}^n H\left(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}\right ),</script><script type="math/tex; mode=display">\ell(\boldsymbol{\Theta}) = -(1/n) \sum_{i=1}^n \log \hat y_{y^{(i)}}^{(i)}</script><pre><code class="lang-python">y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])y = torch.LongTensor([0, 2])y_hat.gather(1, y.view(-1, 1))</code></pre><pre><code>tensor([[0.1000],        [0.5000]])</code></pre><pre><code class="lang-python">def cross_entropy(y_hat, y):    return - torch.log(y_hat.gather(1, y.view(-1, 1)))</code></pre><h2 id="定义准确率"><a href="#定义准确率" class="headerlink" title="定义准确率"></a>定义准确率</h2><p>我们模型训练完了进行模型预测的时候，会用到我们这里定义的准确率。</p><pre><code class="lang-python">def accuracy(y_hat, y):    return (y_hat.argmax(dim=1) == y).float().mean().item()</code></pre><pre><code class="lang-python">print(accuracy(y_hat, y))</code></pre><pre><code>0.5</code></pre><pre><code class="lang-python"># 本函数已保存在d2lzh_pytorch包中方便以后使用。该函数将被逐步改进：它的完整实现将在“图像增广”一节中描述def evaluate_accuracy(data_iter, net):    acc_sum, n = 0.0, 0    for X, y in data_iter:        acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()        n += y.shape[0]    return acc_sum / n</code></pre><pre><code class="lang-python">print(evaluate_accuracy(test_iter, net))</code></pre><pre><code>0.1445</code></pre><h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><pre><code class="lang-python">num_epochs, lr = 5, 0.1# 本函数已保存在d2lzh_pytorch包中方便以后使用def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,              params=None, lr=None, optimizer=None):    for epoch in range(num_epochs):        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0        for X, y in train_iter:            y_hat = net(X)            l = loss(y_hat, y).sum()            # 梯度清零            if optimizer is not None:                optimizer.zero_grad()            elif params is not None and params[0].grad is not None:                for param in params:                    param.grad.data.zero_()            l.backward()            if optimizer is None:                d2l.sgd(params, lr, batch_size)            else:                optimizer.step()             train_l_sum += l.item()            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()            n += y.shape[0]        test_acc = evaluate_accuracy(test_iter, net)        print(&#39;epoch %d, loss %.4f, train acc %.3f, test acc %.3f&#39;              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, batch_size, [W, b], lr)</code></pre><pre><code>epoch 1, loss 0.7851, train acc 0.750, test acc 0.791epoch 2, loss 0.5704, train acc 0.814, test acc 0.810epoch 3, loss 0.5258, train acc 0.825, test acc 0.819epoch 4, loss 0.5014, train acc 0.832, test acc 0.824epoch 5, loss 0.4865, train acc 0.836, test acc 0.827</code></pre><h2 id="模型预测"><a href="#模型预测" class="headerlink" title="模型预测"></a>模型预测</h2><p>现在我们的模型训练完了，可以进行一下预测，我们的这个模型训练的到底准确不准确。<br>现在就可以演示如何对图像进行分类了。给定一系列图像（第三行图像输出），我们比较一下它们的真实标签（第一行文本输出）和模型预测结果（第二行文本输出）。</p><pre><code class="lang-python">X, y = iter(test_iter).next()true_labels = d2l.get_fashion_mnist_labels(y.numpy())pred_labels = d2l.get_fashion_mnist_labels(net(X).argmax(dim=1).numpy())titles = [true + &#39;\n&#39; + pred for true, pred in zip(true_labels, pred_labels)]d2l.show_fashion_mnist(X[0:9], titles[0:9])</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/1DA8927186304BEBA2B3DCC4A9E027DD/q5j7fq2jer.svg"></p><h1 id="softmax的简洁实现"><a href="#softmax的简洁实现" class="headerlink" title="softmax的简洁实现"></a>softmax的简洁实现</h1><pre><code class="lang-python"># 加载各种包或者模块import torchfrom torch import nnfrom torch.nn import initimport numpy as npimport syssys.path.append(&quot;/home/kesci/input&quot;)import d2lzh1981 as d2lprint(torch.__version__)</code></pre><pre><code>1.3.0</code></pre><h2 id="初始化参数和获取数据"><a href="#初始化参数和获取数据" class="headerlink" title="初始化参数和获取数据"></a>初始化参数和获取数据</h2><pre><code class="lang-python">batch_size = 256train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, root=&#39;/home/kesci/input/FashionMNIST2065&#39;)</code></pre><h2 id="定义网络模型"><a href="#定义网络模型" class="headerlink" title="定义网络模型"></a>定义网络模型</h2><pre><code class="lang-python">num_inputs = 784num_outputs = 10class LinearNet(nn.Module):    def __init__(self, num_inputs, num_outputs):        super(LinearNet, self).__init__()        self.linear = nn.Linear(num_inputs, num_outputs)    def forward(self, x): # x 的形状: (batch, 1, 28, 28)        y = self.linear(x.view(x.shape[0], -1))        return y# net = LinearNet(num_inputs, num_outputs)class FlattenLayer(nn.Module):    def __init__(self):        super(FlattenLayer, self).__init__()    def forward(self, x): # x 的形状: (batch, *, *, ...)        return x.view(x.shape[0], -1)from collections import OrderedDictnet = nn.Sequential(        # FlattenLayer(),        # LinearNet(num_inputs, num_outputs)         OrderedDict([           (&#39;flatten&#39;, FlattenLayer()),           (&#39;linear&#39;, nn.Linear(num_inputs, num_outputs))]) # 或者写成我们自己定义的 LinearNet(num_inputs, num_outputs) 也可以        )</code></pre><h2 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h2><pre><code class="lang-python">init.normal_(net.linear.weight, mean=0, std=0.01)init.constant_(net.linear.bias, val=0)</code></pre><pre><code>Parameter containing:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)</code></pre><h2 id="定义损失函数-1"><a href="#定义损失函数-1" class="headerlink" title="定义损失函数"></a>定义损失函数</h2><pre><code class="lang-python">loss = nn.CrossEntropyLoss() # 下面是他的函数原型# class torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction=&#39;mean&#39;)</code></pre><h2 id="定义优化函数"><a href="#定义优化函数" class="headerlink" title="定义优化函数"></a>定义优化函数</h2><pre><code class="lang-python">optimizer = torch.optim.SGD(net.parameters(), lr=0.1) # 下面是函数原型# class torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False)</code></pre><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><pre><code class="lang-python">num_epochs = 5d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, optimizer)</code></pre><pre><code>epoch 1, loss 0.0031, train acc 0.751, test acc 0.795epoch 2, loss 0.0022, train acc 0.813, test acc 0.809epoch 3, loss 0.0021, train acc 0.825, test acc 0.806epoch 4, loss 0.0020, train acc 0.833, test acc 0.813epoch 5, loss 0.0019, train acc 0.837, test acc 0.822</code></pre><h2 id="练习题"><a href="#练习题" class="headerlink" title="练习题"></a>练习题</h2><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/cwADXPBIbdjvGhN.png" alt=""></p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/8pOqEmTBAYGcWMs.png" alt="解析"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动手深度学习(一):线性回归</title>
      <link href="/2020/02/14/dl-notes1-lin-re/"/>
      <url>/2020/02/14/dl-notes1-lin-re/</url>
      
        <content type="html"><![CDATA[<h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><p>主要内容包括：</p><ol><li>线性回归的基本要素</li><li>线性回归模型从零开始的实现</li><li>线性回归模型使用pytorch的简洁实现</li></ol><h2 id="线性回归的基本要素"><a href="#线性回归的基本要素" class="headerlink" title="线性回归的基本要素"></a>线性回归的基本要素</h2><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>为了简单起见，这里我们假设价格只取决于房屋状况的两个因素，即面积（平方米）和房龄（年）。接下来我们希望探索价格与这两个因素的具体关系。线性回归假设输出与各个输入之间是线性关系:</p><script type="math/tex; mode=display">\mathrm{price} = w_{\mathrm{area}} \cdot \mathrm{area} + w_{\mathrm{age}} \cdot \mathrm{age} + b</script><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>我们通常收集一系列的真实数据，例如多栋房屋的真实售出价格和它们对应的面积和房龄。我们希望在这个数据上面寻找模型参数来使模型的预测价格与真实价格的误差最小。在机器学习术语里，该数据集被称为训练数据集（training data set）或训练集（training set），一栋房屋被称为一个样本（sample），其真实售出价格叫作标签（label），用来预测标签的两个因素叫作特征（feature）。特征用来表征样本的特点。</p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>在模型训练中，我们需要衡量价格预测值与真实值之间的误差。通常我们会选取一个非负数作为误差，且数值越小表示误差越小。一个常用的选择是平方函数。 它在评估索引为 $i$ 的样本误差的表达式为</p><script type="math/tex; mode=display">l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2,</script><script type="math/tex; mode=display">L(\mathbf{w}, b) =\frac{1}{n}\sum_{i=1}^n l^{(i)}(\mathbf{w}, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2.</script><h3 id="优化函数-随机梯度下降"><a href="#优化函数-随机梯度下降" class="headerlink" title="优化函数 - 随机梯度下降"></a>优化函数 - 随机梯度下降</h3><p>当模型和损失函数形式较为简单时，上面的误差最小化问题的解可以直接用公式表达出来。这类解叫作解析解（analytical solution）。本节使用的线性回归和平方误差刚好属于这个范畴。然而，大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。这类解叫作数值解（numerical solution）。</p><p>在求数值解的优化算法中，小批量随机梯度下降（mini-batch stochastic gradient descent）在深度学习中被广泛使用。它的算法很简单：先选取一组模型参数的初始值，如随机选取；接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量（mini-batch）$\mathcal{B}$，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后用此结果与预先设定的一个正数的乘积作为模型参数在本次迭代的减小量。   </p><script type="math/tex; mode=display">(\mathbf{w},b) \leftarrow (\mathbf{w},b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(\mathbf{w},b)} l^{(i)}(\mathbf{w},b)</script><p>学习率: $\eta$代表在每次优化中，能够学习的步长的大小<br>批量大小: $\mathcal{B}$是小批量计算中的批量大小batch size   </p><p>总结一下，优化函数的有以下两个步骤：</p><ul><li>(i)初始化模型参数，一般来说使用随机初始化；</li><li>(ii)我们在数据上迭代多次，通过在负梯度方向移动参数来更新每个参数。</li></ul><h2 id="矢量计算"><a href="#矢量计算" class="headerlink" title="矢量计算"></a>矢量计算</h2><p>在模型训练或预测时，我们常常会同时处理多个数据样本并用到矢量计算。在介绍线性回归的矢量计算表达式之前，让我们先考虑对两个向量相加的两种方法。</p><ol><li>向量相加的一种方法是，将这两个向量按元素逐一做标量加法。</li><li>向量相加的另一种方法是，将这两个向量直接做矢量加法。</li></ol><pre><code class="lang-python">import torchimport time# init variable a, b as 1000 dimension vectorn = 1000a = torch.ones(n)b = torch.ones(n)</code></pre><pre><code class="lang-python"># define a timer class to record timeclass Timer(object):    &quot;&quot;&quot;Record multiple running times.&quot;&quot;&quot;    def __init__(self):        self.times = []        self.start()    def start(self):        # start the timer        self.start_time = time.time()    def stop(self):        # stop the timer and record time into a list        self.times.append(time.time() - self.start_time)        return self.times[-1]    def avg(self):        # calculate the average and return        return sum(self.times)/len(self.times)    def sum(self):        # return the sum of recorded time        return sum(self.times)</code></pre><p>现在我们可以来测试了。首先将两个向量使用for循环按元素逐一做标量加法。</p><pre><code class="lang-python">timer = Timer()c = torch.zeros(n)for i in range(n):    c[i] = a[i] + b[i]&#39;%.5f sec&#39; % timer.stop()</code></pre><pre><code>&#39;0.01162 sec&#39;</code></pre><p>另外是使用torch来将两个向量直接做矢量加法：</p><pre><code class="lang-python">timer.start()d = a + b&#39;%.5f sec&#39; % timer.stop()</code></pre><pre><code>&#39;0.00027 sec&#39;</code></pre><p>结果很明显,后者比前者运算速度更快。因此，我们应该尽可能采用矢量计算，以提升计算效率。</p><h2 id="线性回归模型从零开始的实现"><a href="#线性回归模型从零开始的实现" class="headerlink" title="线性回归模型从零开始的实现"></a>线性回归模型从零开始的实现</h2><pre><code class="lang-python"># import packages and modules%matplotlib inlineimport torchfrom IPython import displayfrom matplotlib import pyplot as pltimport numpy as npimport randomprint(torch.__version__)</code></pre><pre><code>1.3.0</code></pre><h3 id="生成数据集"><a href="#生成数据集" class="headerlink" title="生成数据集"></a>生成数据集</h3><p>使用线性模型来生成数据集，生成一个1000个样本的数据集，下面是用来生成数据的线性关系：</p><script type="math/tex; mode=display">\mathrm{price} = w_{\mathrm{area}} \cdot \mathrm{area} + w_{\mathrm{age}} \cdot \mathrm{age} + b</script><pre><code class="lang-python"># set input feature number num_inputs = 2# set example numbernum_examples = 1000# set true weight and bias in order to generate corresponded labeltrue_w = [2, -3.4]true_b = 4.2features = torch.randn(num_examples, num_inputs,                      dtype=torch.float32)labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_blabels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()),   # 不可能严格线性分布，加一个偏差                       dtype=torch.float32)</code></pre><h3 id="使用图像来展示生成的数据"><a href="#使用图像来展示生成的数据" class="headerlink" title="使用图像来展示生成的数据"></a>使用图像来展示生成的数据</h3><pre><code class="lang-python">plt.scatter(features[:, 1].numpy(), labels.numpy(), 1)</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.kesci.com/rt_upload/8E2E1E16060241C6A33E4CF1EC65DF1D/q5mgua7bvt.png"></p><h3 id="读取数据集"><a href="#读取数据集" class="headerlink" title="读取数据集"></a>读取数据集</h3><pre><code class="lang-python">def data_iter(batch_size, features, labels):    num_examples = len(features)    indices = list(range(num_examples))    random.shuffle(indices)  # random read 10 samples 顺序打乱读取    for i in range(0, num_examples, batch_size):        # the last time may be not enough for a whole batch 防止越界        j = torch.LongTensor(indices[i: min(i + batch_size, num_examples)]) #每次跳跃 b_size        yield features.index_select(0, j), labels.index_select(0, j)   # 返回一个就退出</code></pre><pre><code class="lang-python">batch_size = 10for X, y in data_iter(batch_size, features, labels):    print(X, &#39;\n&#39;, y)    break</code></pre><pre><code>tensor([[ 0.6802, -0.1529],        [-1.1139, -1.0506],        [ 1.1355,  0.6313],        [ 0.9057,  1.4527],        [-0.7477, -0.4656],        [-0.4012, -0.1810],        [ 0.0038, -1.3701],        [-0.9438, -1.6571],        [ 0.4016,  0.4233],        [ 0.8824, -1.0067]])  tensor([6.0793, 5.5343, 4.3112, 1.0657, 4.2856, 4.0040, 8.8709, 7.9642, 3.5696,        9.3887])</code></pre><h3 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><pre><code class="lang-python">w = torch.tensor(np.random.normal(0, 0.01, (num_inputs, 1)), dtype=torch.float32)b = torch.zeros(1, dtype=torch.float32)w.requires_grad_(requires_grad=True)  # 要求梯度b.requires_grad_(requires_grad=True)</code></pre><pre><code>tensor([0.], requires_grad=True)</code></pre><h3 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h3><p>定义用来训练参数的训练模型：</p><script type="math/tex; mode=display">\mathrm{price} = w_{\mathrm{area}} \cdot \mathrm{area} + w_{\mathrm{age}} \cdot \mathrm{age} + b</script><pre><code class="lang-python">def linreg(X, w, b):    return torch.mm(X, w) + b   # mm 相乘</code></pre><h3 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h3><p>我们使用的是均方误差损失函数：</p><script type="math/tex; mode=display">l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2,</script><pre><code class="lang-python">def squared_loss(y_hat, y):     return (y_hat - y.view(y_hat.size())) ** 2 / 2   # view 表示数据相同，大小不同# s = squared_loss(2.33,3.14) + squared_loss(1.07,0.98) + squared_loss(1.23,1.32)y_hat = torch.tensor([[2.33],        [ 1.07],        [ 1.23]])y = torch.tensor([3.14, 0.98, 1.32])s = squared_loss(y_hat, y)print(s.mean())</code></pre><pre><code>tensor(0.1121)</code></pre><h3 id="定义优化函数"><a href="#定义优化函数" class="headerlink" title="定义优化函数"></a>定义优化函数</h3><p>在这里优化函数使用的是小批量随机梯度下降：</p><script type="math/tex; mode=display">(\mathbf{w},b) \leftarrow (\mathbf{w},b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(\mathbf{w},b)} l^{(i)}(\mathbf{w},b)</script><pre><code class="lang-python">def sgd(params, lr, batch_size):     # params : [w, b]    for param in params:        # 在梯度的负方向累加一个值，达到优化的效果（趋于谷底）        param.data -= lr * param.grad / batch_size # ues .data to operate param without gradient track</code></pre><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>当数据集、模型、损失函数和优化函数定义完了之后就可来准备进行模型的训练了。</p><pre><code class="lang-python"># super parameters initlr = 0.03  # lr 学习率num_epochs = 5net = linregloss = squared_loss# trainingfor epoch in range(num_epochs):  # training repeats num_epochs times    # in each epoch, all the samples in dataset will be used once    # X is the feature and y is the label of a batch sample    for X, y in data_iter(batch_size, features, labels):        l = loss(net(X, w, b), y).sum()          # calculate the gradient of batch sample loss         l.backward()          # using small batch random gradient descent to iter model parameters        sgd([w, b], lr, batch_size)          # reset parameter gradient        w.grad.data.zero_()  # 每一次运算后都需要将上一次的梯度记录清空        b.grad.data.zero_()    train_l = loss(net(features, w, b), labels)    print(&#39;epoch %d, loss %f&#39; % (epoch + 1, train_l.mean().item()))</code></pre><h2 id="线性回归模型使用pytorch的简洁实现"><a href="#线性回归模型使用pytorch的简洁实现" class="headerlink" title="线性回归模型使用pytorch的简洁实现"></a>线性回归模型使用pytorch的简洁实现</h2><pre><code class="lang-python">import torchfrom torch import nnimport numpy as nptorch.manual_seed(1)   # 同样的随机初始化种子,保证结果可以复现print(torch.__version__)torch.set_default_tensor_type(&#39;torch.FloatTensor&#39;)</code></pre><pre><code>1.3.0</code></pre><h3 id="生成数据集-1"><a href="#生成数据集-1" class="headerlink" title="生成数据集"></a>生成数据集</h3><p>在这里生成数据集跟从零开始的实现中是完全一样的。</p><pre><code class="lang-python">num_inputs = 2num_examples = 1000true_w = [2, -3.4]true_b = 4.2features = torch.tensor(np.random.normal(0, 1, (num_examples, num_inputs)), dtype=torch.float)labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_blabels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float)</code></pre><h3 id="读取数据集-1"><a href="#读取数据集-1" class="headerlink" title="读取数据集"></a>读取数据集</h3><pre><code class="lang-python">import torch.utils.data as Databatch_size = 10# combine featues and labels of datasetdataset = Data.TensorDataset(features, labels)# put dataset into DataLoaderdata_iter = Data.DataLoader(    dataset=dataset,            # torch TensorDataset format    batch_size=batch_size,      # mini batch size    shuffle=True,               # whether shuffle the data or not    num_workers=2,              # read data in multithreading)</code></pre><pre><code class="lang-python">for X, y in data_iter:    print(X, &#39;\n&#39;, y)    break</code></pre><pre><code>tensor([[ 0.2134,  0.7981],        [-0.3899, -0.4544],        [ 1.4472, -1.2160],        [-0.7354,  1.2216],        [-1.3233,  0.6937],        [-0.2810,  0.5505],        [-1.6620, -0.1457],        [-0.7635,  0.7058],        [ 0.6079, -0.7497],        [ 0.4924, -0.1376]])  tensor([ 1.9045,  4.9770, 11.2503, -1.4180, -0.8159,  1.7737,  1.3910,  0.2721,         7.9648,  5.6408])</code></pre><h3 id="定义模型-1"><a href="#定义模型-1" class="headerlink" title="定义模型"></a>定义模型</h3><pre><code class="lang-python">class LinearNet(nn.Module):    def __init__(self, n_feature):        super(LinearNet, self).__init__()      # call father function to init         self.linear = nn.Linear(n_feature, 1)  # function prototype: `torch.nn.Linear(in_features, out_features, bias=True)`    def forward(self, x):        y = self.linear(x)        return ynet = LinearNet(num_inputs)print(net)</code></pre><pre><code>LinearNet(  (linear): Linear(in_features=2, out_features=1, bias=True))</code></pre><pre><code class="lang-python"># ways to init a multilayer network# method onenet = nn.Sequential(   # 一个时序容器，Modules 会以他们传入的顺序被添加到容器中    nn.Linear(num_inputs, 1)    # other layers can be added here    )# method twonet = nn.Sequential()net.add_module(&#39;linear&#39;, nn.Linear(num_inputs, 1))# net.add_module ......# method threefrom collections import OrderedDictnet = nn.Sequential(OrderedDict([          (&#39;linear&#39;, nn.Linear(num_inputs, 1))          # ......        ]))print(net[0].weight)print(net[0].bias)# print(net)</code></pre><pre><code>Parameter containing:tensor([[0.6652, 0.1260]], requires_grad=True)Parameter containing:tensor([0.5704], requires_grad=True)</code></pre><h3 id="初始化模型参数-1"><a href="#初始化模型参数-1" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><pre><code class="lang-python">from torch.nn import initinit.normal_(net[0].weight, mean=0.0, std=0.01)  # weight重新生成init.constant_(net[0].bias, val=0.0)  # bias[ 被赋值为 0print(net[0].weight)print(net[0].bias)</code></pre><pre><code>Parameter containing:tensor([[0.0016, 0.0029]], requires_grad=True)Parameter containing:tensor([0.], requires_grad=True)</code></pre><pre><code class="lang-python">for param in net.parameters():    print(param)  # param 就是上面的 weight 和 bias</code></pre><pre><code>Parameter containing:tensor([[0.0016, 0.0029]], requires_grad=True)Parameter containing:tensor([0.], requires_grad=True)</code></pre><h3 id="定义损失函数-1"><a href="#定义损失函数-1" class="headerlink" title="定义损失函数"></a>定义损失函数</h3><pre><code class="lang-python">loss = nn.MSELoss()    # nn built-in squared loss function   采用计算 MSE 的 loss                       # function prototype: `torch.nn.MSELoss(size_average=None, reduce=None, reduction=&#39;mean&#39;)`</code></pre><h3 id="定义优化函数-1"><a href="#定义优化函数-1" class="headerlink" title="定义优化函数"></a>定义优化函数</h3><pre><code class="lang-python">import torch.optim as optimoptimizer = optim.SGD(net.parameters(), lr=0.03)   # built-in random gradient descent functionprint(optimizer)  # function prototype: `torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False)`</code></pre><pre><code>SGD (Parameter Group 0    dampening: 0    lr: 0.03    momentum: 0    nesterov: False    weight_decay: 0)</code></pre><h3 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h3><pre><code class="lang-python">num_epochs = 3for epoch in range(1, num_epochs + 1):    for X, y in data_iter:        output = net(X)        l = loss(output, y.view(-1, 1))        optimizer.zero_grad() # reset gradient, equal to net.zero_grad()        l.backward()        optimizer.step()    print(&#39;epoch %d, loss: %f&#39; % (epoch, l.item()))</code></pre><pre><code>epoch 1, loss: 0.000299epoch 2, loss: 0.000058epoch 3, loss: 0.000051</code></pre><pre><code class="lang-python"># result comparisiondense = net[0]print(true_w, dense.weight.data)print(true_b, dense.bias.data)</code></pre><pre><code>[2, -3.4] tensor([[ 2.0007, -3.3992]])4.2 tensor([4.1989])</code></pre><h2 id="练习题"><a href="#练习题" class="headerlink" title="练习题"></a>练习题</h2><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/19/XkyKfEtjT1l95wp.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习基础</title>
      <link href="/2020/02/14/dl-notes/"/>
      <url>/2020/02/14/dl-notes/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo 踩坑记录</title>
      <link href="/2020/02/09/hexo-experience/"/>
      <url>/2020/02/09/hexo-experience/</url>
      
        <content type="html"><![CDATA[<h1 id="Hexo-踩坑记录"><a href="#Hexo-踩坑记录" class="headerlink" title="Hexo 踩坑记录"></a>Hexo 踩坑记录</h1><p>Hexo 作一个优秀的博客框架，本人也是一直参考着网上零零碎碎的博文一步步搭建的，中间遇到不少坑。每次做一个稍大的项目都要经历这种折腾，今后决定把这些经验记录下来沉淀，有未完成的也留给以后思考。</p><h3 id="1-git-分支未知出错"><a href="#1-git-分支未知出错" class="headerlink" title="1. git 分支未知出错"></a>1. git 分支未知出错</h3><p>修改 matery.css 的时候总是没法同步到 <a href="https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io" target="_blank" rel="noopener">https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io</a>… 的远端，后来发现存在分支的问题，于是在主题的 config.yml 中添加一个 <code>@master</code>:</p><pre><code class="lang-yml">jsDelivr:  url: https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io@master</code></pre><p>更改的内容部分可以生效，但仍然有一些冥顽不化的配色，本地已经渲染出来了，远端 jsdelivr 死活不改变，加了 @master 也不变，于是直接在 post/post-detail.ejs 页面中添加 css 代码强行修改:</p><pre><code class="lang-css">#articleContent a {    color: #267871; !important;}#artDetail .post-cate a {    color: #267871; !important;}blockquote {    border-left: 5px solid #267871; !important;}#artDetail .reprint a {    color: #267871; !important;}</code></pre><p>挖个坑，希望今后可以好好研究一下 git</p><h3 id="2-Hexo-无法解析代码块"><a href="#2-Hexo-无法解析代码块" class="headerlink" title="2. Hexo 无法解析代码块"></a>2. Hexo 无法解析代码块</h3><p>比如上面的 css 代码块，Hexo 在解析的时候会把它识别成网页 css 的内容，最后没有显示，只能主动适应 Hexo 这种方式：</p><ol><li>不添加 <code>&lt;style type=&quot;text/css&quot;&gt;</code>这种会与网页混淆的内容</li><li>代码块中不添加(1) (2) 序号</li></ol><h3 id="3-Hexo-代码块无法高亮的问题"><a href="#3-Hexo-代码块无法高亮的问题" class="headerlink" title="3. Hexo 代码块无法高亮的问题"></a>3. Hexo 代码块无法高亮的问题</h3><ol><li><p>主要矛盾：<br>hexo 对于 mathjax 的显示支持有问题，需要安装新的 kmarked 插件，修改对于<code>{, (</code>等等的解析规则，而修改规则支持 Mathjax 后，prism 的高亮功能又会失效，简而言之不可兼得</p></li><li><p>解决：<br>我使用的是<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank" rel="noopener">hexo-matery</a>主题，使用了大家推荐的 hexo-prism-plugin 主题，但是出现和 prism 高亮和 Mathjax 语法不兼容的问题。代码不能高亮简直失去了博客写代码的意义。最后使用 <a href="https://highlightjs.org/" target="_blank" rel="noopener">highlight.js</a>成功实现代码高亮，参考以下这位博主的设置：<a href="http://cps.ninja/2019/03/25/add-highlightjs-to-hexo-blog/" target="_blank" rel="noopener">使用 Highlight.js 优化代码块高亮效果</a></p></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数学基础备忘录</title>
      <link href="/2020/02/07/math-notes/"/>
      <url>/2020/02/07/math-notes/</url>
      
        <content type="html"><![CDATA[<h1 id="数学基础备忘录"><a href="#数学基础备忘录" class="headerlink" title="数学基础备忘录"></a>数学基础备忘录</h1><h2 id="排列组合公式"><a href="#排列组合公式" class="headerlink" title="排列组合公式"></a>排列组合公式</h2><p>(A_{n}^{m}=n(n-1) \cdots(n-m+1))  m个数递减相乘<br>(C_{n}^{m}=\frac{A_{n}^{m}}{m !}=\frac{n !}{m !(n-m) !}=C_{n}^{n-m})</p><p>$A^4_4 = 4\times3\times2\times1 = 24$</p><p>$A^4_4$ 计算方法：从4 开始，从 4 到 3 到 2…连续乘 4 个数。<br>$C^3_4$ 计算方法：（先计算 $A^3_4$ 再得到 $C^3_4$ ）$C^3_4 = \frac{A^3_4}{3!} = \frac{4\times3\times2}{3\times2\times1} = 24\div6 = 4$</p><p>比如 $A^2_4$ 表示从 4 个东西东西中抽取任意 2 样，一共有 $4\times3 = 12$ <code>排列</code>方式。<br>$C^3_4$ 表示从 4 个东西东西中抽取任意 3 样，一共有 $\frac{4\times3\times2}{3\times2\times1} = 24\div6 = 4$ 两种<code>组成</code>方式。</p><p>A 表示排列，是有序的，而 C 表示组合，表示有多少种<code>组成</code>方式，只看成员，不看顺序。</p><p>对于 A（排列）来说，<code>4 3 1</code>和<code>1 3 4</code>是两种构成，而对于 C（组合）来说，<code>4 3 1</code>和<code>1 3 4</code>就是同一回事。</p><p>$C^3_5$ = 多少？ </p>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> math </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习概述</title>
      <link href="/2020/02/06/ml-study-notes/"/>
      <url>/2020/02/06/ml-study-notes/</url>
      
        <content type="html"><![CDATA[<h1 id="机器学习概述"><a href="#机器学习概述" class="headerlink" title="机器学习概述"></a>机器学习概述</h1><h2 id="机器学习分类"><a href="#机器学习分类" class="headerlink" title="机器学习分类"></a>机器学习分类</h2><ol><li>监督学习：已经有数据，和数据对应的标签。</li><li>非监督学习：给定的样本无需输出/标签，让机器自己学习样本中隐含的内部结构。</li><li>半监督学习：二者结合。</li><li>强化学习：通过打分/评价的形式，类似于监督学习中的标签。</li></ol><h2 id="机器学习模型"><a href="#机器学习模型" class="headerlink" title="机器学习模型"></a>机器学习模型</h2><p>机器学习 = 数据 data + 模型 model + 优化方法 optimal strategy</p><h2 id="偏差-方差权衡"><a href="#偏差-方差权衡" class="headerlink" title="偏差/方差权衡"></a>偏差/方差权衡</h2><p>variance 和 bias，分别对应过拟合和欠拟合</p><p>来自 Wikipedia：</p><blockquote><p>在监督学习中，如果能将模型的方差与误差权衡好，那么可以认为该模型的泛化性能（对于新数据）将会表现出好的结果。</p><p>偏差刻画的是算法本身的性能。高偏差将会造成欠拟合(Underfitting) [miss the relevant relations between features and target outputs]。换句话说，模型越复杂偏差就越小；而模型越简单，偏差就越大。</p><p>方差用来衡量因训练集数据波动(fluctuations)而造成的误差影响。高方差将会造成过拟合(Overfitting)。</p></blockquote><p>在周志华老师&lt;机器学习&gt;书中是这样阐述的：</p><blockquote><p><em>偏差</em> 度量了学习算法的期望预测与真实结果的偏离程度，即刻画了算法本身的拟合能力；</p><p><em>方差</em> 度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响；</p><p><em>噪声</em> 则表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题的本身难度</p><p>偏差-方差分解说明，泛化性能是由学习算法的能力、数据的充分性以及学习任务本身的难度所共同决定的。给定的学习任务，为了取得好的泛化性能，则需使偏差较小，即能够充分拟合数据，并且使方差较小，即使数据扰动产生的影响小。一般来说方差与偏差是有冲突的，这称为方差-偏差窘境。</p></blockquote><h2 id="常见机器学习算法概览"><a href="#常见机器学习算法概览" class="headerlink" title="常见机器学习算法概览"></a>常见机器学习算法概览</h2><h3 id="1-Linear-Algorithm-线性算法"><a href="#1-Linear-Algorithm-线性算法" class="headerlink" title="1. Linear Algorithm 线性算法"></a>1. Linear Algorithm 线性算法</h3><ol><li><p><strong>Linear Regression 线性回归</strong>：使用最小二乘法 Least Squares 拟合一条直线 → 计算 R<sup>2</sup> → 计算 R<sup>2</sup> 的 p 值。R<sup>2</sup> 表示 x 能多大程度反映 y 的变化，p 值表示可靠程度。拟合直线的过程使用「随机梯度下降」（SGD）</p></li><li><p><strong>Lasso 回归 和 Ridge 回归</strong>：都可以减少共线性带来的影响，即 X 自变量之间有相互关联。区别可以归结为L2和L1正则化的性质差异。</p></li><li><p><strong>Polynomial Regression 多项式回归</strong>：能够模拟非线性可分的数据（曲线），线性回归不能做到这一点。但容易过拟合。</p></li><li><p><strong>Logistic Regression 逻辑回归</strong>：判断 True or False，Y 值为 0-1 表示概率，用于分类。线性回归使用「Residual 偏差」，而逻辑回归使用「maximum likelihood 最大似然」</p></li></ol><h3 id="2-Decision-Tree-决策树"><a href="#2-Decision-Tree-决策树" class="headerlink" title="2. Decision Tree 决策树"></a>2. Decision Tree 决策树</h3><ol><li><p><strong>ID3</strong>: 计算「信息熵」 $Entropy(D)$，值越小，说明样本集合D的纯度就越高，进而选择用样本的某一个属性a来划分样本集合D时，就可以得出用属性a对样本D进行划分所带来的「信息增益」 $Gain(D, a)$，值越大，说明如果用属性a来划分样本集合D，那么纯度会提升。 <script type="math/tex">Entropy(t)=-\sum_{k} p\left(c_{k} | t\right) \log p\left(c_{k} | t\right)</script>  <script type="math/tex">Classificationerror (t)=1-\max _{k}\left[p\left(c_{k} | t\right)\right]</script></p></li><li><p><strong>C4.5</strong>: 提出Gainratio 「增益率」，解决ID3决策树的一个缺点，当一个属性的可取值数目较多时，那么可能在这个属性对应的可取值下的样本只有一个或者是很少个，那么这个时候它的信息增益是非常高的，这个时候纯度很高，ID3决策树会认为这个属性很适合划分，但是较多取值的属性来进行划分带来的问题是它的泛化能力比较弱。用 $I(·)$ 表示不纯度——可以是熵可以是基尼，信息增益：<script type="math/tex">\Delta=I(\text { parent })-\sum_{i=1}^{n} \frac{N\left(a_{i}\right)}{N} I\left(a_{i}\right)</script>信息增益率：<script type="math/tex">Gainratio =\frac{\Delta}{Entropy({parent})}</script></p></li><li><p><strong>CART(Classification and Regression Tree)</strong>: 通过计算 Gini 基尼系数（尽可能小），判断 impurity 不纯洁度。离散数据用「是否」划分子树，连续数据可以用「两两之间平均值」划分子树。<script type="math/tex">{Gini}(t)=1-\sum_{k}\left[p\left(c_{k} | t\right)\right]^{2}</script>D 分裂为 DL 和 DR，分裂后的信息增益<script type="math/tex">Gain(D, A)=\frac{\left|D_{L}\right|}{|D|} \operatorname{Gini}\left(D_{L}\right)+\frac{\left|D_{R}\right|}{|D|} \operatorname{Gini}\left(D_{R}\right)</script></p></li></ol><h3 id="3-SVM-支持向量机"><a href="#3-SVM-支持向量机" class="headerlink" title="3. SVM 支持向量机"></a>3. SVM 支持向量机</h3><p>SVM：<a href="https://blog.csdn.net/liugan528/article/details/79448379" target="_blank" rel="noopener">https://blog.csdn.net/liugan528/article/details/79448379</a></p><p>KKT：<a href="https://blog.csdn.net/qq_32763149/article/details/81055062" target="_blank" rel="noopener">https://blog.csdn.net/qq_32763149/article/details/81055062</a></p><p><strong>SVM 分类</strong>：</p><ol><li>硬间隔支持向量机（线性可分支持向量机）：当训练数据线性可分时，可通过硬间隔最大化学得一个线性可分支持向量机。</li><li>软间隔支持向量机：当训练数据近似线性可分时，可通过软间隔最大化得到一个线性支持向量机。</li><li>非线性支持向量机：当训练数据线性不可分时，可通过核方法以及软间隔最大化得一个非线性支持向量机。</li></ol><p><strong>基本原理</strong>：</p><ol><li><p>Maximum Margin Classifier：只看边界。</p></li><li><p>Soft Margin Classifier（即 Support Vector Classifier）：允许 misclassification误分类，寻找两个支撑向量来确定分类边界。</p></li><li><p>Kernel Function：非线性SVM，从低维数据开始，通过「核函数」给数据升维，然后找到一个 Support Vector Classifier 将数据分成两组。核函数的选择，支撑向量的选择，都用 cross validation 交叉验证。</p></li><li><p>Kernel Trick: 根据升维的距离进行计算，但是不进行实际的升维。</p></li></ol><p><strong>具体过程</strong>：</p><ol><li><p>线性可分的情况：对于超平面 $w \cdot x+b=0$ 和 $margin$ 有关系<script type="math/tex">{margin}=\frac{2}{\|w\|}</script><br> 最大化 $margin$ 等效于最小化 $\frac{1}{2}|w|^{2}$</p><p> 形成一个拉格朗日乘子α的约束问题 <script type="math/tex">\begin{array}{ll}{\min _{w, b}} & {\frac{1}{2}|w|^{2}} {\text {s.t.}} & {y_{i}\left(w \cdot x_{i}+b\right)-1 \geq 0}\end{array}</script><br> 可以列式 <script type="math/tex">L(w, b, \alpha)=\frac{1}{2}|w|^{2}-\sum_{i=1}^{N} \alpha_{i}\left[y_{i}\left(w \cdot x_{i}+b\right)-1\right]</script><br> 拉格朗日对偶性：解决「凸二次规划」（convex quadratic propgramming）问题，即将原始的约束最优化问题可等价于极大极小的对偶问题（以 w,b 作参数时的最小值，以α作参数时的最大值）<br> <script type="math/tex">\max _{\alpha} \min _{w, b} \quad L(w, b, \alpha)</script>通过求导一系列步骤，转换成\begin{array}{ll}<br>{\min _{\alpha}} &amp; {\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i}} \\<br>{\text { s.t. }} &amp; {\sum_{i=1}^{N} \alpha_{i} y_{i}=0} \\<br>{} &amp; {\alpha_{i} \geq 0, \quad i=1,2, \cdots, N}<br>\end{array}</p></li><li><p>线性不可分的情况：对每个样本引入一个松弛变量 $\xi_{i} \geq 0$, 约束条件和目标函数变为</p><script type="math/tex; mode=display">\begin{aligned}&y_{i}\left(w \cdot x_{i}+b\right) \geq 1-\xi_{i}\\&\min _{w, b, \xi} \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i}\end{aligned}</script></li></ol><p><strong>部分术语</strong>：</p><ol><li><p>KKT 条件：是拉格朗日乘子的泛化，把所有的不等式约束、等式约束和目标函数全部写为一个式子L(a, b, x)= f(x) + a<em>g(x) + b</em>h(x)，KKT条件是说最优值必须满足以下条件：（1）L(a, b, x)对x求导为零；（2）h(x) =0; （3）a*g(x) = 0;</p></li><li><p>SMO：Sequential Minimal Optimization用二次规划来求解α，要用到 KKT</p></li><li><p>SVR：支持向量回归</p></li></ol><p><strong>优点</strong>：<br>SVM在中小量样本规模的时候容易得到数据和特征之间的非线性关系，可以避免使用神经网络结构选择和局部极小值问题，可解释性强，可以解决高维问题。</p><p><strong>缺点</strong>：<br>SVM对缺失数据敏感，对非线性问题没有通用的解决方案，核函数的正确选择不容易，计算复杂度高，主流的算法可以达到O(n2)O(n2)的复杂度，这对大规模的数据是吃不消的。</p><h3 id="4-Naive-Bayes-Algorithms-朴素贝叶斯"><a href="#4-Naive-Bayes-Algorithms-朴素贝叶斯" class="headerlink" title="4. Naive Bayes Algorithms 朴素贝叶斯"></a>4. Naive Bayes Algorithms 朴素贝叶斯</h3><ol><li>Naive Bayes</li><li>Gaussian Naive Bayes</li><li>Multinomial Naive Bayes</li><li>Bayesian Belief Network (BBN)</li><li>Bayesian Network (BN)</li></ol><p>朴素贝叶斯基本公式：$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$<br><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/09/J46IjSoqFuD1vtW.png" alt="J46IjSoqFuD1vtW"></p><h3 id="5-KNN-k-NearestNeighbor-K-最邻近算法"><a href="#5-KNN-k-NearestNeighbor-K-最邻近算法" class="headerlink" title="5. KNN (k-NearestNeighbor) K 最邻近算法"></a>5. KNN (k-NearestNeighbor) K 最邻近算法</h3><p>用于分类</p><ol><li>计算测试数据与各个训练数据之间的距离；</li><li>按照距离的递增关系进行排序；</li><li>选取距离最小的K个点；</li><li>确定前K个点所在类别的出现频率；</li><li>返回前K个点中出现频率最高的类别作为测试数据的预测分类</li></ol><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/09/ScAOKyPqMGWdrLH.png" alt="ScAOKyPqMGWdrLH"></p><h3 id="6-Clustering-Algorithm-聚类算法"><a href="#6-Clustering-Algorithm-聚类算法" class="headerlink" title="6. Clustering Algorithm 聚类算法"></a>6. Clustering Algorithm 聚类算法</h3><ol><li>k-Means：选取平均值</li><li>k-Medians：由选取平均值改为选取中位数</li><li>Expectation Maximisation (EM)：有隐含随机变量的概率模型的参数的估计方法，它是一种无监督的算法</li><li><p>Hierarchical Clustering 层次聚类：<br>算法如下：</p><p>(1) 将每个对象看作一类，计算两两之间的最小距离；</p><p>(2) 将距离最小的两个类合并成一个新类；</p><p>(3) 重新计算新类与所有类之间的距离；</p><p>(4) 重复(2)、(3)，直到所有类最后合并成一类。</p></li></ol><h3 id="7-K-means-算法"><a href="#7-K-means-算法" class="headerlink" title="7. K-means 算法"></a>7. K-means 算法</h3><p>算法如下：</p><pre><code>选取k个初始质心(作为初始cluster);repeat:    对每个样本点，计算得到距其最近的质心，将其类别标为该质心所对应的cluster;    重新计算k个cluser对应的质心;until 质心不再发生变化</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/09/VLCReWo92QXSNP4.jpg" alt="VLCReWo92QXSNP4"></p><h3 id="8-Random-Forest-随机森林"><a href="#8-Random-Forest-随机森林" class="headerlink" title="8. Random Forest 随机森林"></a>8. Random Forest 随机森林</h3><h3 id="9-Dimensionality-Reduction-Algorithms-降维算法"><a href="#9-Dimensionality-Reduction-Algorithms-降维算法" class="headerlink" title="9. Dimensionality Reduction Algorithms 降维算法"></a>9. Dimensionality Reduction Algorithms 降维算法</h3><h3 id="10-Gradient-Boosting-algorithms-梯度提升算法"><a href="#10-Gradient-Boosting-algorithms-梯度提升算法" class="headerlink" title="10. Gradient Boosting algorithms 梯度提升算法"></a>10. Gradient Boosting algorithms 梯度提升算法</h3><ol><li>GBM</li><li>XGBoost</li><li>LightGBM</li><li>CatBoost</li></ol><h3 id="11-Deep-Learning-Algorithms-深度学习"><a href="#11-Deep-Learning-Algorithms-深度学习" class="headerlink" title="11. Deep Learning Algorithms 深度学习"></a>11. Deep Learning Algorithms 深度学习</h3><ol><li>Convolutional Neural Network (CNN)</li><li>Recurrent Neural Networks (RNNs)</li><li>Long Short-Term Memory Networks (LSTMs)</li><li>Stacked Auto-Encoders</li><li>Deep Boltzmann Machine (DBM)</li><li>Deep Belief Networks (DBN)</li></ol><hr><h2 id="机器学习损失函数"><a href="#机器学习损失函数" class="headerlink" title="机器学习损失函数"></a>机器学习损失函数</h2><ol><li>0-1损失函数<script type="math/tex; mode=display">L(y,f(x)) =\begin{cases}0, & \text{y = f(x)}  \\1, & \text{y $\neq$ f(x)}\end{cases}</script></li><li>绝对值损失函数<script type="math/tex; mode=display">L(y,f(x))=|y-f(x)|</script></li><li>平方损失函数<script type="math/tex; mode=display">L(y,f(x))=(y-f(x))^2</script></li><li>log对数损失函数<script type="math/tex; mode=display">L(y,f(x))=log(1+e^{-yf(x)})</script></li><li>指数损失函数<script type="math/tex; mode=display">L(y,f(x))=exp(-yf(x))</script></li><li>Hinge损失函数<script type="math/tex; mode=display">L(w,b)=max\{0,1-yf(x)\}</script></li></ol><hr><h2 id="机器学习优化方法"><a href="#机器学习优化方法" class="headerlink" title="机器学习优化方法"></a>机器学习优化方法</h2><p>梯度下降是最常用的优化方法之一，它使用梯度的反方向 $ \nabla_\theta J(\theta) $ 更新参数 $ \theta $，使得目标函数$J(\theta)$达到最小化的一种优化方法，这种方法我们叫做梯度更新. </p><ol><li>(全量)梯度下降<script type="math/tex; mode=display">\theta=\theta-\eta\nabla_\theta J(\theta)</script></li><li>随机梯度下降<script type="math/tex; mode=display">\theta=\theta-\eta\nabla_\theta J(\theta;x^{(i)},y^{(i)})</script></li><li>小批量梯度下降<script type="math/tex; mode=display">\theta=\theta-\eta\nabla_\theta J(\theta;x^{(i:i+n)},y^{(i:i+n)})</script></li><li>引入动量的梯度下降<script type="math/tex; mode=display">\begin{cases}v_t=\gamma v_{t-1}+\eta \nabla_\theta J(\theta)  \\\theta=\theta-v_t\end{cases}</script></li><li>自适应学习率的Adagrad算法<script type="math/tex; mode=display">\begin{cases}g_t= \nabla_\theta J(\theta)  \\\theta_{t+1}=\theta_{t,i}-\frac{\eta}{\sqrt{G_t+\varepsilon}} \cdot g_t\end{cases}</script></li><li><p>牛顿法</p><script type="math/tex; mode=display">\theta_{t+1}=\theta_t-H^{-1}\nabla_\theta J(\theta_t)</script><p> 其中:<br> $t$: 迭代的轮数</p><p> $\eta$: 学习率</p><p> $G_t$: 前t次迭代的梯度和</p><p> $\varepsilon:$很小的数,防止除0错误</p><p> $H$: 损失函数相当于$\theta$的Hession矩阵在$\theta_t$处的估计</p></li></ol><hr><h2 id="机器学习的评价指标"><a href="#机器学习的评价指标" class="headerlink" title="机器学习的评价指标"></a>机器学习的评价指标</h2><ol><li>MSE(Mean Squared Error)<script type="math/tex; mode=display">MSE(y,f(x))=\frac{1}{N}\sum_{i=1}^{N}(y-f(x))^2</script></li><li>MAE(Mean Absolute Error)<script type="math/tex; mode=display">MSE(y,f(x))=\frac{1}{N}\sum_{i=1}^{N}|y-f(x)|</script></li><li>RMSE(Root Mean Squard Error)<script type="math/tex; mode=display">RMSE(y,f(x))=\frac{1}{1+MSE(y,f(x))}</script></li><li>Top-k准确率<script type="math/tex; mode=display">Top_k(y,pre_y)=\begin{cases}1, {y \in pre_y}  \\0, {y \notin pre_y}\end{cases}</script></li><li>混淆矩阵</li></ol><div class="table-container"><table><thead><tr><th style="text-align:center">混淆矩阵</th><th style="text-align:center">Predicted as Positive</th><th style="text-align:center">Predicted as Negative</th></tr></thead><tbody><tr><td style="text-align:center">Labeled as Positive</td><td style="text-align:center">True Positive(TP)</td><td style="text-align:center">False Negative(FN)</td></tr><tr><td style="text-align:center">Labeled as Negative</td><td style="text-align:center">False Positive(FP)</td><td style="text-align:center">True Negative(TN)</td></tr></tbody></table></div><ul><li>真正例(True Positive, TP):真实类别为正例, 预测类别为正例</li><li>假负例(False Negative, FN): 真实类别为正例, 预测类别为负例</li><li>假正例(False Positive, FP): 真实类别为负例, 预测类别为正例 </li><li><p>真负例(True Negative, TN): 真实类别为负例, 预测类别为负例</p></li><li><p>真正率(True Positive Rate, TPR): 被预测为正的正样本数 / 正样本实际数</p><script type="math/tex; mode=display">TPR=\frac{TP}{TP+FN}</script></li><li><p>假负率(False Negative Rate, FNR): 被预测为负的正样本数/正样本实际数</p><script type="math/tex; mode=display">FNR=\frac{FN}{TP+FN}</script></li><li><p>假正率(False Positive Rate, FPR): 被预测为正的负样本数/负样本实际数，</p><script type="math/tex; mode=display">FPR=\frac{FP}{FP+TN}</script></li><li>真负率(True Negative Rate, TNR): 被预测为负的负样本数/负样本实际数，<script type="math/tex; mode=display">TNR=\frac{TN}{FP+TN}</script></li><li>准确率(Accuracy)<script type="math/tex; mode=display">ACC=\frac{TP+TN}{TP+FN+FP+TN}</script></li><li>精准率<script type="math/tex; mode=display">P=\frac{TP}{TP+FP}</script></li><li>召回率<script type="math/tex; mode=display">R=\frac{TP}{TP+FN}</script></li><li>F1-Score<script type="math/tex; mode=display">\frac{2}{F_1}=\frac{1}{P}+\frac{1}{R}</script></li><li><strong>ROC</strong></li></ul><p>ROC曲线的横轴为“假正例率”，纵轴为“真正例率”. 以FPR为横坐标，TPR为纵坐标，那么ROC曲线就是改变各种阈值后得到的所有坐标点 (FPR,TPR) 的连线，画出来如下。红线是随机乱猜情况下的ROC，曲线越靠左上角，分类器越佳. </p><ul><li><strong>AUC(Area Under Curve)</strong></li></ul><p>AUC就是ROC曲线下的面积. 真实情况下，由于数据是一个一个的，阈值被离散化，呈现的曲线便是锯齿状的，当然数据越多，阈值分的越细，”曲线”越光滑. </p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/09/Ky4FT1MVe3PUYai.jpg" alt="Ky4FT1MVe3PUYai"></p><p>用AUC判断分类器（预测模型）优劣的标准:</p><ul><li>AUC = 1 是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数预测的场合，不存在完美分类器.</li><li>0.5 &lt; AUC &lt; 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值.</li><li>AUC &lt; 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测.</li></ul><h2 id="机器学习模型选择"><a href="#机器学习模型选择" class="headerlink" title="机器学习模型选择"></a>机器学习模型选择</h2><ol><li>交叉验证</li></ol><p>所有数据分为三部分：训练集、交叉验证集和测试集。交叉验证集不仅在选择模型时有用，在超参数选择、正则项参数 [公式] 和评价模型中也很有用。</p><ol><li>k-折叠交叉验证</li></ol><ul><li>假设训练集为S ，将训练集等分为k份:$\{S_1, S_2, …, S_k\}$. </li><li>然后每次从集合中拿出k-1份进行训练</li><li>利用集合中剩下的那一份来进行测试并计算损失值</li><li>最后得到k次测试得到的损失值，并选择平均损失值最小的模型</li></ul><ol><li>Bias与Variance，欠拟合与过拟合</li></ol><p><strong>欠拟合</strong>一般表示模型对数据的表现能力不足，通常是模型的复杂度不够，并且Bias高，训练集的损失值高，测试集的损失值也高.</p><p><strong>过拟合</strong>一般表示模型对数据的表现能力过好，通常是模型的复杂度过高，并且Variance高，训练集的损失值低，测试集的损失值高.</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/09/dIiSlJVkjFL6EpB.jpg" alt="dIiSlJVkjFL6EpB"></p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/09/Dn5bB8wUVgmsIzC.jpg" alt="Dn5bB8wUVgmsIzC"></p><ol><li>解决方法</li></ol><ul><li>增加训练样本: 解决高Variance情况</li><li>减少特征维数: 解决高Variance情况</li><li>增加特征维数: 解决高Bias情况</li><li>增加模型复杂度: 解决高Bias情况</li><li>减小模型复杂度: 解决高Variance情况</li></ul><h2 id="机器学习参数调优"><a href="#机器学习参数调优" class="headerlink" title="机器学习参数调优"></a>机器学习参数调优</h2><ol><li>网格搜索</li></ol><p>一种调参手段；穷举搜索：在所有候选的参数选择中，通过循环遍历，尝试每一种可能性，表现最好的参数就是最终的结果</p><ol><li>随机搜索</li></ol><p>与网格搜索相比，随机搜索并未尝试所有参数值，而是从指定的分布中采样固定数量的参数设置。它的理论依据是，如果随即样本点集足够大，那么也可以找到全局的最大或最小值，或它们的近似值。通过对搜索范围的随机取样，随机搜索一般会比网格搜索要快一些。</p><ol><li>贝叶斯优化算法</li></ol><p>贝叶斯优化用于机器学习调参由J. Snoek(2012)提出，主要思想是，给定优化的目标函数(广义的函数，只需指定输入和输出即可，无需知道内部结构以及数学性质)，通过不断地添加样本点来更新目标函数的后验分布(高斯过程,直到后验分布基本贴合于真实分布。简单的说，就是考虑了上一次参数的信息，从而更好的调整当前的参数。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PAT-甲级 做题笔记</title>
      <link href="/2020/02/06/pat-advance/"/>
      <url>/2020/02/06/pat-advance/</url>
      
        <content type="html"><![CDATA[<h1 id="PAT-甲级-做题笔记"><a href="#PAT-甲级-做题笔记" class="headerlink" title="PAT-甲级 做题笔记"></a>PAT-甲级 做题笔记</h1><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/06/Y49WNI6DpA3hSKX.png" alt="甲级题目分类"></p><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><p>0000 <a href="#0000">做题 Tips 基本经验</a><br>1003 <a href="#1003">Emergency (Dijkstra 算法)</a><br>1004 <a href="#1004">Counting Leaves (计算叶节点数，DFS/BFS 树算法)</a><br>1007 <a href="#1007">Maximum Subsequence Sum(最大子序列和)</a><br>1010 <a href="#1010">Radix (进制转换/二分法)</a><br>1012 <a href="#1012">The Best Rank (应用问题，数据结构设计，多维度排序)</a><br>1013 <a href="#1013">Battle Over Cities (图的遍历，统计强连通分量的个数，DFS)</a><br>1014 <a href="#1014">Waiting in Line (队列应用，排队问题)</a><br>1015 <a href="https://pintia.cn/problem-sets/994805342720868352/problems/994805495863296000" target="_blank" rel="noopener">Reversible Primes (进制转换+判断素数：可取用函数)</a><br>1016 <a href="#1016">Phone Bills (日期类计算)</a><br>1018 <a href="#1018">Public Bike Management (图最短路径 Dijkstra + DFS)</a><br>1020 <a href="#1020">Tree Traversals (已知后序和中序，转前序/层序)</a><br>1021 <a href="#1021">Deepest Root (求树中最长的路径，DFS，连通分量)</a><br>1040 <a href="#1040">Longest Symmetric String (求最长对称子串, DP)</a><br>1045 <a href="#1045">Favorite Color Stripe (最长不下降子序列, DP)</a><br>1068 <a href="#1068">Find More Coins (0-1 背包问题, DP)</a><br>1044 <a href="#1044">Shopping in Mars (特定值子序列和, 二分查找)</a><br>1085 <a href="#1085">Perfect Sequence (二分查找, upper_bound, 双指针)</a></p><hr><h2 id="0000-做题-Tips-基本经验"><a href="#0000-做题-Tips-基本经验" class="headerlink" title="0000 做题 Tips 基本经验"></a>0000 做题 Tips 基本经验<span id="0000"></span></h2><ol><li>最后千万别栽在头文件上，比如 reverse() 属于 &lt; algorithm&gt;</li><li><p>输出多行的时候，最后有一个多余的空行也没问题<br>比如使用 <code>cout &lt;&lt; a &lt;&lt; endl</code> 三次，样例输出：</p><pre><code> 3 4 5</code></pre><p> 实际输出：最后有一个多余空行没关系（实际上是需要一个\n 或者 endl 才正确）</p><pre><code> 3 4 5</code></pre></li><li>int 最大范围 $-2^{31}$=-2147483648 到 $2^{31}$-1=2147483647  (10 位数)</li></ol><hr><p><span id="1003"></span></p><h2 id="1003-Emergency-Dijkstra-算法"><a href="#1003-Emergency-Dijkstra-算法" class="headerlink" title="1003 Emergency (Dijkstra 算法)"></a>1003 Emergency (Dijkstra 算法)</h2><h3 id="1-题目大意"><a href="#1-题目大意" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>n个城市m条路，每个城市有救援小组，所有的边的边权已知。给定起点和终点，求从起点到终点的最短路径条数以及最短路径上的救援小组数目之和。如果有多条就输出点权（城市救援小组数目）最大的那个</p><h3 id="2-分析"><a href="#2-分析" class="headerlink" title="2. 分析"></a>2. 分析</h3><p>用一遍Dijkstra算法～救援小组个数相当于点权，用Dijkstra求边权最小的最短路径的条数，以及这些最短路径中点权最大的值～dis[i]表示从出发点到i结点最短路径的路径长度，num[i]表示从出发点到i结点最短路径的条数，w[i]表示从出发点到i点救援队的数目之和～当判定dis[u] + e[u][v] &lt; dis[v]的时候，不仅仅要更新dis[v]，还要更新num[v] = num[u], w[v] = weight[v] + w[u]; 如果dis[u] + e[u][v] == dis[v]，还要更新num[v] += num[u]，而且判断一下是否权重w[v]更小，如果更小了就更新w[v] = weight[v] + w[u]</p><h3 id="3-个人代码"><a href="#3-个人代码" class="headerlink" title="3. 个人代码"></a>3. 个人代码</h3><p><a href="https://pintia.cn/problem-sets/994805342720868352/problems/994805523835109376" target="_blank" rel="noopener">PTA-1003</a></p><pre><code class="lang-c++">#include &lt;iostream&gt;#include &lt;algorithm&gt;using namespace std;//Dijkstra 算法：单源最短路径int n, m, c1, c2;int e[510][510], weight[510], dis[510], num[510], w[510];//边的邻接矩阵，每个点的权值，从出发点到i的距离，最短距离边数，最大权重和const int Inf = 99999999;bool visit[510];int main(){    cin &gt;&gt; n &gt;&gt; m &gt;&gt; c1 &gt;&gt; c2;    for (int i = 0; i &lt; n; ++i) {        cin &gt;&gt; weight[i];    }    fill(e[0], e[0]+510*510, Inf);  //整个邻接矩阵填正无穷，默认不可达    fill(dis, dis+510, Inf);    int a,b,c;    for (int j = 0; j &lt; m; ++j) {        cin &gt;&gt; a &gt;&gt; b &gt;&gt; c;        e[a][b] = e[b][a] = c;  //一定要对称    }    dis[c1] = 0;    num[c1] = 1;    w[c1] = weight[c1];  //初始化    for (int i = 0; i &lt; n; ++i) {    //每个点都要作为起始点 开始一遍        int u = -1, min_d = Inf;        //每次循环重置        //本次循环表示从 i 出发, i -&gt; j        for (int j = 0; j &lt; n; ++j) {            if(!visit[j] &amp;&amp; dis[j] &lt; min_d){   //找从本点出发的最小边                u = j;                min_d = dis[j];            }        }        if(u == -1) break;        visit[u] = true;        //得到一条 i-&gt;u, 即 dis[], 用来更新最短路径        //之前没更新的、在后续得到更新，达到最优        for (int v = 0; v &lt; n; ++v) {            if(!visit[v] &amp;&amp; e[u][v] != Inf){                if(dis[u] + e[u][v] &lt; dis[v]){                    dis[v] = dis[u] + e[u][v];                    w[v] = w[u] + weight[v];                    num[v] = num[u];                } else if(dis[u] + e[u][v] == dis[v]){                    num[v] += num[u];                    if(w[v] &lt; w[u]+weight[v])                        w[v] = w[u]+weight[v];   //携带尽可能多的人                }            }        }    }    cout &lt;&lt; num[c2] &lt;&lt; &quot; &quot; &lt;&lt; w[c2];   //从出发点到c2    return 0;}</code></pre><h3 id="4-学习要点"><a href="#4-学习要点" class="headerlink" title="4. 学习要点"></a>4. 学习要点</h3><blockquote><p>1.真正理解单源最短路径，每一次选择的 dis 都是相对于源点的最短路径<br>2.大数组要设为全局变量<br>3.外层循环 i：n 次循环，每次访问一个新点，保证 n 个点全访问到；内层循环 j：继续寻找还未访问的点，找 dis 最小的访问；内层循环 k：更新所有能够更新的 dis</p></blockquote><hr><p><span id="1004"></span></p><h2 id="1004-Counting-Leaves-dfs-bfs-树算法"><a href="#1004-Counting-Leaves-dfs-bfs-树算法" class="headerlink" title="1004 Counting Leaves (dfs/bfs 树算法)"></a>1004 Counting Leaves (dfs/bfs 树算法)</h2><h3 id="1-题目大意-1"><a href="#1-题目大意-1" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>给出一棵树，问每一层各有多少个叶子结点</p><h3 id="2-坑"><a href="#2-坑" class="headerlink" title="2. 坑"></a>2. 坑</h3><pre><code>list[tmp].lev = list[fa].lev + 1;  //出问题,不一定按照顺序输入</code></pre><p>有可能先 03 后 01，fa 的 lev 还没确定，不能给孩子节点 +1</p><pre><code>3 203 1 0201 1 03</code></pre><h3 id="3-个人代码-1"><a href="#3-个人代码-1" class="headerlink" title="3. 个人代码"></a>3. 个人代码</h3><p><a href="https://pintia.cn/problem-sets/994805342720868352/problems/994805521431773184" target="_blank" rel="noopener">PTA-1004</a></p><h3 id="4-正确方法"><a href="#4-正确方法" class="headerlink" title="4. 正确方法"></a>4. 正确方法</h3><pre><code class="lang-cpp">void dfs(int fa){    for (auto &amp;ch : list[fa].v) {        list[ch].lev = list[fa].lev + 1;        dfs(ch);    }}dfs(1);   //通过 DFS 一层层给叶子节点 lev+1</code></pre><hr><p><span id="1007"></span></p><h2 id="1007-Maximum-Subsequence-Sum-最大子序列和"><a href="#1007-Maximum-Subsequence-Sum-最大子序列和" class="headerlink" title="1007 Maximum Subsequence Sum (最大子序列和)"></a>1007 Maximum Subsequence Sum (最大子序列和)</h2><h3 id="1-题目大意-2"><a href="#1-题目大意-2" class="headerlink" title="1.题目大意"></a>1.题目大意</h3><p>求最大连续子序列和，输出最大的和以及这个子序列的开始值和结束值。如果所有数都小于0，那么认为最大的和为0，并且输出首尾元素</p><h3 id="2-分析-1"><a href="#2-分析-1" class="headerlink" title="2.分析"></a>2.分析</h3><p>本质上是<strong>动态规划</strong>的思想，数组为<code>vec[]</code>，设<code>dp[i]</code> 是以<code>vec[i]</code>结尾的子数组的最大和，对于元素<code>vec[i+1]</code>, 它有两种选择：<code>vec[i+1]</code>接着前面的子数组构成最大和; <code>vec[i+1]</code>自己单独构成子数组。则<code>dp[i+1] = max{dp[i]+vec[i+1],  vec[i+1]}</code></p><p>简化则用一个 temp_sum 和一个 temp_first 解决，真正最大和为 sum，起始点为 first 和 last，建立局部和全局的关系。如果 <code>temp_sum &lt; 0</code>, 说明目前这一段对后续的序列和已经没有加成作用，可以舍弃另立门户，令<code>temp_sum = 0</code></p><h3 id="3-个人代码-2"><a href="#3-个人代码-2" class="headerlink" title="3. 个人代码"></a>3. 个人代码</h3><pre><code class="lang-cpp">//初始值设置容易坑：sum要为-1 才能开始int sum=-1, temp_sum=0, first=0, last=k-1, temp_first=0;for (int j = 0; j &lt; k; ++j) {    temp_sum += a[j];    if(temp_sum &lt; 0){        temp_sum = 0;        temp_first = j+1;   //本段已经可以舍弃，开始新一段    } else if(temp_sum &gt; sum){        sum = temp_sum;        first = temp_first;        last = j;    }}if(sum &lt; 0) sum = 0;  //如果全负则为 0，根据题目要求不要漏情况</code></pre><h3 id="4-类似题"><a href="#4-类似题" class="headerlink" title="4. 类似题"></a>4. 类似题</h3><p><a href="https://www.cnblogs.com/grandyang/p/4480780.html" target="_blank" rel="noopener">最长无重复子串</a></p><hr><p>)<span id="1010"></span></p><h2 id="1010-Radix-进制转换-二分法"><a href="#1010-Radix-进制转换-二分法" class="headerlink" title="1010 Radix (进制转换/二分法"></a>1010 Radix (进制转换/二分法</h2><h3 id="1-题目大意-3"><a href="#1-题目大意-3" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>给定两个相等数，已知一个数的进制，求另外一个数的进制（radix-&gt;基数）</p><h3 id="2-分析-2"><a href="#2-分析-2" class="headerlink" title="2. 分析"></a>2. 分析</h3><p>通用函数<code>cal(string str, int radix)</code>把任意进制的数转换为 10 进制数<br>在使用搜索遍历，找到另一个数对应的进制</p><h3 id="3-个人代码-3"><a href="#3-个人代码-3" class="headerlink" title="3.个人代码"></a>3.个人代码</h3><p><a href="https://pintia.cn/problem-sets/994805342720868352/problems/994805507225665536" target="_blank" rel="noopener">PTA-1010</a></p><p>转换函数</p><pre><code class="lang-cpp">long long cal(string str, long long radix){    long long target=0, bak = 1;    reverse(str.begin(), str.end());    for (char c : str) {        if(isdigit(c))            target += (c - &#39;0&#39;) * bak;        else            target += (c - &#39;a&#39; + 10) * bak;        bak *= radix;  //也可以使用&lt;cmath&gt;的 pow()函数    }    return target;}</code></pre><p>搜索函数</p><pre><code class="lang-cpp">long long find_radix(string str, long long tar){    char c = *max_element(str.begin(), str.end());  //最大字母    //进制至少比最大字母要大    long long low = isdigit(c) ? c-&#39;0&#39; : c-&#39;a&#39;+10;    low += 1;  //必须要加一,至少多1    long long high = max(low, tar);  //进制最大不会大于目标 tar    while (low &lt;= high){        long long mid = (low+high)/2;        long long tmp = cal(str, mid);        if(tmp &lt; 0 || tmp &gt; tar) high = mid - 1;  //小于 0 也是进制太大        else if(tmp == tar) return mid;        else low = mid + 1;    }    return -1;}</code></pre><h3 id="4-学习要点-1"><a href="#4-学习要点-1" class="headerlink" title="4. 学习要点"></a>4. 学习要点</h3><blockquote><ol><li>进制转换这种大数字的乘法很容易溢出，必须要用 <code>long long</code></li><li>暴力搜索（从 2 到 ∞ 容易超时）最好用二分法，并且限定范围：最小是 <code>low</code> 至少是最大的那个字母+1，比如 fff 最少是 15+1=16 进制，最大是<code>high</code>不超过目标 <code>tar</code></li><li>可以使用反向迭代器 <code>it = n.rbegin(); it != n.rend()</code>代替<code>reverse()</code></li></ol></blockquote><hr><h2 id="1012-The-Best-Rank-数据结构设计-多维度排序"><a href="#1012-The-Best-Rank-数据结构设计-多维度排序" class="headerlink" title="1012 The Best Rank (数据结构设计/多维度排序)"></a>1012 The Best Rank (数据结构设计/多维度排序)<span id="1012"></span></h2><h3 id="1-题目大意-4"><a href="#1-题目大意-4" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>找出每个人自己最优势的科目，也就是单独排名最好的科目，优先级 A &gt; C &gt; M &gt; E</p><ul><li>Sample Input:<pre><code>5 6310101 98 85 88310102 70 95 88310103 82 87 94310104 91 91 91310105 85 90 90310101310102310103310104310105999999</code></pre></li><li>Sample Output:<pre><code>1 C1 M1 E1 A3 AN/A</code></pre></li></ul><h3 id="2-个人代码"><a href="#2-个人代码" class="headerlink" title="2. 个人代码"></a>2. 个人代码</h3><p><a href="https://pintia.cn/problem-sets/994805342720868352/problems/994805502658068480" target="_blank" rel="noopener">PTA-1012</a></p><h3 id="3-学习要点"><a href="#3-学习要点" class="headerlink" title="3. 学习要点"></a>3. 学习要点</h3><p>数据结构设计</p><pre><code class="lang-cpp">struct node{    //一定要用数组形式，避免过多变量 int c, m, e, c_r, m_r, e_r, a_r;    int id, best;    int score[4], rank[4];}stu[2001];  //输入学生序列，非学号排序int exist[1000000], flag=-1; //exist 至少 &gt;0, 顺便建立 id-输入序号 映射//使用 flag 配置作用，避免写过多重复的 cmp 函数bool cmp(node &amp;a, node &amp;b){ return a.score[flag] &gt; b.score[flag];}</code></pre><p>输入后，利用<code>exist[stu[k].id] = k + 1; k++</code> 依次建立映射<br>输出时，利用<code>cin &gt;&gt; tmp; ex = exist[tmp]</code>和<code>stu[ex-1]</code>实现映射，无需 find<br>处理时，使用<code>sort(stu, stu+n, cmp)</code> 循环 <code>flag++</code>，实现分学科多次排序，确定 <code>rank[]</code></p><hr><p><span id="1013"></span></p><h2 id="1013-Battle-Over-Cities（图的遍历，统计强连通分量的个数，dfs）"><a href="#1013-Battle-Over-Cities（图的遍历，统计强连通分量的个数，dfs）" class="headerlink" title="1013 Battle Over Cities（图的遍历，统计强连通分量的个数，dfs）"></a>1013 Battle Over Cities（图的遍历，统计强连通分量的个数，dfs）</h2><h3 id="1-题目大意-5"><a href="#1-题目大意-5" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>给出n个城市之间有相互连接的m条道路，当删除一个城市和其连接的道路的时候，问其他几个剩余的城市至少要添加多少个路线，才能让它们重新变为连通图</p><h3 id="2-分析-3"><a href="#2-分析-3" class="headerlink" title="2. 分析"></a>2. 分析</h3><p>n 连通分量，最少需要 n-1 条边相连，所以本题实质就是求「去除某点及其相连的边」后，剩余的「连通分量个数」是一个图算法，需要用图的遍历来解决 (DFS)</p><blockquote><p>求连通分量依据：一次 <code>dfs()</code>走到底，可以访问完一个连通分量</p></blockquote><p>数据结构：</p><blockquote><p>涉及图的遍历，要考虑 <code>visit[]</code>数组<br>图的路径存储，使用二维矩阵</p></blockquote><h3 id="3-个人代码-4"><a href="#3-个人代码-4" class="headerlink" title="3. 个人代码"></a>3. 个人代码</h3><p><a href="https://pintia.cn/problem-sets/994805342720868352/problems/994805500414115840" target="_blank" rel="noopener">PTA-1013</a></p><pre><code class="lang-cpp">for (int i = 0; i &lt; k; ++i) {        int cur, cnt=0;        cin &gt;&gt; cur;        fill(visited, visited+n, false);        visited[cur] = true;  //相当于把本点去除        for (int j = 1; j &lt;= n; ++j) {  //依次计算所有连通分量，而非从本点出发            if(!visited[j]){                dfs(j);                cnt++;            }        }        cout &lt;&lt; cnt-1 &lt;&lt; endl;    }</code></pre><pre><code class="lang-cpp">void dfs(int st){   //只用来 visit，不负责计数    visited[st] = true;    for (int i = 0; i &lt;= n; ++i)        if(!visited[i] &amp;&amp; e[st][i]){   //未访问且有路            dfs(i);        }}</code></pre><hr><p><span id="1014"></span></p><h2 id="1014-Reversible-Primes（队列应用-排队问题"><a href="#1014-Reversible-Primes（队列应用-排队问题" class="headerlink" title="1014 Reversible Primes（队列应用 排队问题"></a>1014 Reversible Primes（队列应用 排队问题</h2><h3 id="1-题目大意-6"><a href="#1-题目大意-6" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>n个窗口，每个窗口可以排队m人。有k位用户需要服务，给出了每位用户需要的minute数，所有客户在8点开始服务，如果有窗口还没排满就入队，否则就在黄线外等候。如果有某一列有一个用户走了服务完毕了，黄线外的人就进来一个。如果同时就选窗口数小的。求q个人的服务结束时间。<br>如果一个客户在17:00以及以后还没有开始服务（此处不是结束服务是开始17:00）就不再服务输出sorry；如果这个服务已经开始了，无论时间多长都要等他服务完毕。</p><h3 id="2-学习要点"><a href="#2-学习要点" class="headerlink" title="2. 学习要点"></a>2. 学习要点</h3><p>面对此类题目，总想按照 timeline 每分每秒去决策，其实队列的问题用队列解决。</p><p>以每个窗口为单位设计结构体，<code>poptime</code> 用于刚入黄线的人决策，<code>endtime</code> 用于已经在队内的人是否会 sorry 决策，<code>q</code> 表示本队列内排队情况。</p><p>给定客户序列，则以每一个客户为循环单位<code>(index++)</code>，队列有一个进就会有一个出，每次变动一个单位，更新各个队列的<code>poptime endtime q</code>等变量。</p><pre><code class="lang-cpp">struct node {    int poptime, endtime;  //队首的人出队（结束）的时间, 队尾的人结束的时间    queue&lt;int&gt; q;};int index = 1;    //第 1 步：n*m 个客户先抢占黄线内, 直接瞬间涌入，不存在选择    for (int i = 1; i &lt;= cap; ++i) {    //看做行和列，先抢第一行        for (int j = 1; j &lt;= n; ++j) {            if(index &lt;= k){                win[j].q.push(time[index]);                if(win[j].endtime &gt;= 540)                    sorry[index] = true;                win[j].endtime += time[index];                if(i == 1)  //对于第一行，出队时间即本队结束时间，做一个初始化                    win[j].poptime = win[j].endtime;                res[index] = win[j].endtime;  //刚进来的，自己是队尾，队伍结束就是自己结束                index++;            }        }    }    //第 2 步：后续的客户    while (index &lt;= k){   //while一次循环表示一个客户的选择        int temp_min = win[1].poptime, temp_win = 1;  //初始化        for (int i = 2; i &lt;= n; ++i) {   //找最早出队的            if(win[i].poptime &lt; temp_min){                temp_win = i;                temp_min = win[i].poptime;            }        }        win[temp_win].q.pop();        win[temp_win].q.push(time[index]);        win[temp_win].poptime += win[temp_win].q.front(); //更新：最前面那个人完成的时间就是出队时间        if(win[temp_win].endtime &gt;= 540)  //endtime 还未更新，是看的前一个人的完成时间，是否超时            sorry[index] = true;        win[temp_win].endtime += time[index];  //更新：刚进来的这个人的完成时间就是本队的完成时间        res[index] = win[temp_win].endtime;        index++;    }</code></pre><hr><p><span id="1016"></span></p><h2 id="1016-Phone-Bills（日期类计算"><a href="#1016-Phone-Bills（日期类计算" class="headerlink" title="1016 Phone Bills（日期类计算"></a>1016 Phone Bills（日期类计算</h2><h3 id="1-题目大意-7"><a href="#1-题目大意-7" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>给定固定格式的日期<code>mm:dd:hh:mm</code>计算各个时间的差值，以及每个时间段有不同的费率，计算总的账单费用。</p><h3 id="2-学习要点-1"><a href="#2-学习要点-1" class="headerlink" title="2. 学习要点"></a>2. 学习要点</h3><p>1.数据结构设计<br>因为输入是给定一个个 call 序列，所以便于存储，也以每个 call 为单位，统一放在一个数组里，按照名字、时间排序，计算的时候，只要考虑 i 和 i-1 前后两个元素是否有同一个 name 以及 status 一个为 0 一个为 1 即可。</p><pre><code class="lang-cpp">struct node {    string name;    int status, month, time, day, hour, minute;};bool cmp(node a, node b) {    return a.name != b.name ? a.name &lt; b.name : a.time &lt; b.time;}</code></pre><p>2.日期差值计算<br>直接”01:05:02:24“-”01:04:23:59“ 难以计算，每个小时还有不同的费率，所以 <strong>统一从本月 0 天 0 点开始计算</strong>，以<code>dd:hh:mm</code>格式再相减即可，隔天的情况则加上这一整天</p><pre><code class="lang-cpp">double billFromZero(node call, int *rate) {    double total;    total = rate[call.hour];  //本小时的费用    total += call.minute + rate[24] * 60 * call.day;  //从本月第 0 天到今天    for (int i = 0; i &lt; call.hour; i++)        total += rate[i] * 60;  //本天内 0 点到现在累加    return total / 100.0;}</code></pre><hr><h2 id="1018-Public-Bike-Management（图最短路径-Dijkstra-DFS）"><a href="#1018-Public-Bike-Management（图最短路径-Dijkstra-DFS）" class="headerlink" title="1018 Public Bike Management（图最短路径 Dijkstra + DFS）"></a>1018 Public Bike Management（图最短路径 Dijkstra + DFS）<span id="1018"></span></h2><p>图例经过 Dijkstra 之后的结果</p><pre><code class="lang-cpp">dis[1] = 1      pre[1] = {0}dis[2] = 1      pre[2] = {0}dis[3] = 2      pre[3] = {1, 2}</code></pre><hr><p><span id="1020"></span></p><h2 id="1020-Tree-Traversals-已知后序和中序，转前序-层序"><a href="#1020-Tree-Traversals-已知后序和中序，转前序-层序" class="headerlink" title="1020 Tree Traversals (已知后序和中序，转前序/层序)"></a>1020 Tree Traversals (已知后序和中序，转前序/层序)</h2><h3 id="1-输入格式"><a href="#1-输入格式" class="headerlink" title="1. 输入格式"></a>1. 输入格式</h3><pre><code>72 3 1 5 7 6 41 2 3 4 5 6 7</code></pre><h3 id="2-分析-4"><a href="#2-分析-4" class="headerlink" title="2. 分析"></a>2. 分析</h3><p>和手动模拟差不多，找到根节点 (pre 的第一个，post 的最后一个) -&gt; 找到左右两段的端点，分别作为左右子树 -&gt; 递归重复。关键在于递归的写法，端点的寻找。</p><h3 id="3-个人代码-5"><a href="#3-个人代码-5" class="headerlink" title="3. 个人代码"></a>3. 个人代码</h3><p><code>level</code> 的作用即按照 <code>index</code> 存储，利用 2n+1 2n+2 的公式可以定位左右孩子，而索引的顺序恰好是层序遍历的顺序（具体第几层不知道）<br>转为前序遍历，只需在 pre 递归前打印 root 即可</p><pre><code class="lang-cpp">vector&lt;int&gt; in, post, level(100000, -1);void pre(int root, int start, int end, int index){      //start end 都是相对于中序序列, 但是 root 是相对于 post 序列的！    if(start &gt; end) return;    level[index] = post[root];    int i = start;    while (i &lt; end &amp;&amp; in[i] != post[root]) i++;  //找到中序 in 序列中的根节点    int left_size = i-start, right_size = end-i;    pre(root-right_size-1, start, i-1, index*2+1);  //此 root 即为左孩子节点，可以确定为 index*2+1    pre(root-1, i+1, end, index*2+2);    //找到根节点 i，左子树 start ~ i-1, 右子树 i+1 ~ end}pre(n-1, 0, n-1, 0);</code></pre><h3 id="4-类题"><a href="#4-类题" class="headerlink" title="4. 类题"></a>4. 类题</h3><p><a href="https://pintia.cn/problem-sets/994805342720868352/problems/994805380754817024" target="_blank" rel="noopener">PTA-1086</a>（已知前序和中序遍历，求后序遍历）<br>给出用栈进行中序遍历的过程，求后序遍历。有一个隐含条件，push 的顺序即为前序遍历，相对于已知 pre 和 in，求 post，同样找 root, start, end 即可。</p><hr><p><span id="1021"></span></p><h2 id="1021-Deepest-Root-求树中最长的路径，DFS，连通分量"><a href="#1021-Deepest-Root-求树中最长的路径，DFS，连通分量" class="headerlink" title="1021 Deepest Root (求树中最长的路径，DFS，连通分量)"></a>1021 Deepest Root (求树中最长的路径，DFS，连通分量)</h2><h3 id="1-题目大意-8"><a href="#1-题目大意-8" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>给出n个结点（1~n）之间的n条边，问是否能构成一棵树，如果不能构成则输出它有的连通分量个数；如果能构成一棵树，输出能构成最深的树的高度时，树的根结点。如果有多个，按照从小到大输出。</p><h3 id="2-分析-5"><a href="#2-分析-5" class="headerlink" title="2. 分析"></a>2. 分析</h3><p>连通分量的个数：基本套路，一次 DFS 走完一个连通分量，统计循环中 DFS 开启的次数即可<br>本题关键在于，两次 DFS 即可求出所有最远端点（即所有最长路径中的所有端点）如果是一棵树的话，线段无非是两个端点，第一次 DFS 能找到一端，在这些最远端点中（可能有多个同样最远的，可能只有一个最远的）随便选一个作为起点，再来一次 DFS，即可找到另一端的最远端点。</p><h3 id="3-个人代码-6"><a href="#3-个人代码-6" class="headerlink" title="3. 个人代码"></a>3. 个人代码</h3><p><a href="https://pintia.cn/problem-sets/994805342720868352/problems/994805482919673856" target="_blank" rel="noopener">PTA-1003</a></p><pre><code class="lang-cpp">void dfs(int node, int height) {    if(height &gt; maxheight) {        temp.clear();        temp.push_back(node);        maxheight = height;    } else if(height == maxheight){        temp.push_back(node);    }    visit[node] = true;    for(int i = 0; i &lt; v[node].size(); i++) {        if(visit[v[node][i]] == false)            dfs(v[node][i], height + 1);    }</code></pre><h3 id="4-做题-Tips"><a href="#4-做题-Tips" class="headerlink" title="4. 做题 Tips"></a>4. 做题 Tips</h3><ol><li>本题不用考虑权重，所以无需开一个二维数组用邻接矩阵存储<br><code>e[10010][10010]; e[a]=e[b]=weight;</code><br>只需使用邻接表，把和自己直接相连的点存起来即可<br><code>v[node][i]; resize(n+1);</code><br><br></li><li>DFS 基本套路<pre><code class="lang-cpp"> dfs(int node, ...){   //遍历过程中顺带做其它操作     visit[node] = true;     for 与本点直接相连的 i:         if(!visit[i])             dfs(i, ...) }</code></pre></li></ol><hr><p><span id="1102"></span></p><h2 id="1102-Invert-a-Binary-Tree-翻转二叉树"><a href="#1102-Invert-a-Binary-Tree-翻转二叉树" class="headerlink" title="1102 Invert a Binary Tree (翻转二叉树)"></a>1102 Invert a Binary Tree (翻转二叉树)</h2><h3 id="1-题目大意-9"><a href="#1-题目大意-9" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>给定一棵二叉树，先翻转好，输出层序遍历和中序遍历</p><h3 id="2-分析-6"><a href="#2-分析-6" class="headerlink" title="2. 分析"></a>2. 分析</h3><p>可以写一个 invert 函数，交换本节点左右元素，然后对左右孩子递归即可。</p><pre><code class="lang-cpp">struct node{    int left=-1, right=-1;  //-1 表示 NULL};void invert(int r){    int a = v[r].left, b = v[r].right;    if(a == -1 &amp;&amp; b == -1)        return;    v[r].left = b;    v[r].right = a;    if(v[r].left != -1) invert(v[r].left);    if(v[r].right != -1) invert(v[r].right);}</code></pre><p>也可以不用写，正常遍历即可，只是先遍历右孩子，再遍历左孩子即可。</p><pre><code class="lang-cpp">//层序遍历标准模式：void level(int r){    queue&lt;int&gt; q;    q.push(r);    while (!q.empty()){        r = q.front();        q.pop();        // if(q.empty()) break;        le_ans.push_back(r);        if(v[r].left != -1) q.push(v[r].left);        if(v[r].right != -1) q.push(v[r].right);    }}</code></pre><h3 id="3-坑点："><a href="#3-坑点：" class="headerlink" title="3. 坑点："></a>3. 坑点：</h3><ol><li>一般的题，都要考虑元素可能有重复，可能不按顺序。本题使用    <code>mark</code>数组确定<code>root</code>是最好的方式</li><li><code>queue</code>和一般的<code>vector</code>有本质的不同，不能用迭代器遍历，不能索引，只能通过<code>front(), back()</code>访问。<code>quque</code>进行<code>pop(),front()</code>等操作时，一定要检查<code>q.empty()</code></li><li>当<code>queue</code>中只有一个元素时，<code>pop()</code>后，在访问<code>front()</code>可能会出现异常数字</li></ol><hr><p><span id="1040"></span></p><h2 id="1040-Longest-Symmetric-String-求最长对称子串-DP"><a href="#1040-Longest-Symmetric-String-求最长对称子串-DP" class="headerlink" title="1040 Longest Symmetric String (求最长对称子串 DP)"></a>1040 Longest Symmetric String (求最长对称子串 DP)</h2><h3 id="1-题目大意-10"><a href="#1-题目大意-10" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>给定一个字符串，找到最长的、对称的子串，输出长度<br>比如<code>Is PAT&amp;TAP symmetric?</code> 输出 11, 即<code>s PAT&amp;TAP s</code>的长度</p><h3 id="2-分析-7"><a href="#2-分析-7" class="headerlink" title="2. 分析"></a>2. 分析</h3><ol><li><p>基本递归的思路，大问题拆成小问题，拆成一个已知条件下的具体问题。对于 子串/子序列 问题，要么用 <code>dp[i][j]</code>表示 i 和 j 的子串，要么用 <code>dp[i]</code>表示 0 到 i 之间的子串.</p></li><li><p>本题要求每一对<code>dp[i][j]</code>就要已知<code>dp[i+1][j-1]</code>, 再根据<code>str[i]</code>和<code>str[j]</code>来判断本段是否对称, 复杂问题被简化, 逐层逐层抛弃.</p></li><li><p>dp 的另外一个关键就在于初始化, 本题将所有<code>dp[i][i]</code>和相邻相等的所有<code>dp[i][i-1]</code>赋值为 1</p></li><li><p>本题一个难点在于, 普通的 dp 都可以从序列开头递归即可, 本题要根据子串的长度, 从短到长进行循环, 从中沉淀出最长的 length</p></li></ol><h3 id="3-个人代码-7"><a href="#3-个人代码-7" class="headerlink" title="3. 个人代码"></a>3. 个人代码</h3><p><a href="https://pintia.cn/problem-sets/994805342720868352/problems/994805446102073344" target="_blank" rel="noopener">PTA-1040</a></p><pre><code class="lang-c++">for (int L = 3; L &lt;= len; ++L) {   //线段长度由小到大        for (int i = 0; i &lt; len - L + 1; ++i) {  //能够到达的最右端点            int j = i + L - 1;  //本次 i 对应的右端点 j            if(str[i] == str[j] &amp;&amp; dp[i+1][j-1]){  //更新dp                dp[i][j] = 1;                ans = L;            }        }    }</code></pre><hr><p><span id="1045"></span></p><h2 id="1045-Favorite-Color-Stripe-最长不下降子序列-DP"><a href="#1045-Favorite-Color-Stripe-最长不下降子序列-DP" class="headerlink" title="1045 Favorite Color Stripe (最长不下降子序列 DP)"></a>1045 Favorite Color Stripe (最长不下降子序列 DP)</h2><h3 id="1-题目大意-11"><a href="#1-题目大意-11" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>给出m中颜色作为喜欢的颜色（同时也给出顺序），然后给出一串长度为L的颜色序列，现在要去掉这个序列中的不喜欢的颜色，然后求剩下序列的一个子序列，使得这个子序列表示的颜色顺序符合自己喜欢的颜色的顺序，不一定要所有喜欢的颜色都出现。<br>1 - N 表示 N 种颜色, 比如给出 2 3 1 5 6 表示自己喜欢的颜色序列</p><h3 id="2-分析-8"><a href="#2-分析-8" class="headerlink" title="2. 分析"></a>2. 分析</h3><p>在原串中要找出符合 2 3 1 5 6 顺序的所有子串, 可能需要多层循环遍历, 还要用额外数组 tmp 临时记录, 数据难以比较和判断, 很麻烦. 所以考虑转化成有序数组, 一一对应成 1 2 3 4 5, 通过比大小即可判断, 等价于转化成一个<strong>最长不下降子序列</strong>问题.</p><p>维护单调数组 dp, <code>dp[i]</code>表示从 0 - i 最长不下降子序列的长度, 可以 DP 的精髓就在于, 本位置 i 对应的<code>dp[i]</code>可以由前面的<code>dp[0]</code> — <code>dp[i-1]</code> 直接推出.</p><p>举例:</p><pre><code>i  2 1 5 3 6 4 8 9 7dp 1 1 2 2 3 3 4 5 4j  2 1 5 3 6 4 8 9 7</code></pre><p>此过程中沉淀出的 maxn 就是 5</p><h3 id="3-个人代码-8"><a href="#3-个人代码-8" class="headerlink" title="3. 个人代码"></a>3. 个人代码</h3><pre><code class="lang-c++">int book[201], a[10001], dp[10001];//dp[i]内存储的是, 从 0 到 i 为止最长的长度, 与 book 无关int main(){    int n, m, l, tmp, maxn = -1;    cin &gt;&gt; n &gt;&gt; m;    for (int i = 1; i &lt;= m; ++i) {  //颜色为 1-n 将颜色序列映射为 idx 递增序列        cin &gt;&gt; tmp;        book[tmp] = i;    }    cin &gt;&gt; l;    int num = 0;    for (int i = 0; i &lt; l; ++i) {  //关键一步，把所有颜色转化为序号，依次存入 a 中        cin &gt;&gt; tmp;        if(book[tmp] &gt;= 1){            a[num++] = book[tmp];  //a 存储 idx        }    }    for (int i = 0; i &lt; num; ++i) {   //不喜欢的颜色直接忽略了,不计入考察范围        dp[i] = 1;   //每次初始赋值为 1, 检查 0-i        for (int j = 0; j &lt; i; ++j) {            if(a[i] &gt;= a[j]){   //以 a 为评判标准, 大于等于就能做下一步                dp[i] = max(dp[i], dp[j]+1);                //核心思想，已知0 ~ i-1 的最大值，新来的 i 最多比0 ~ i-1 多 1            }        }        maxn = max(maxn, dp[i]);    }    cout &lt;&lt; maxn;    return 0;}</code></pre><hr><p><span id="1068"></span></p><h2 id="1068-Find-More-Coins-01-背包问题-DP"><a href="#1068-Find-More-Coins-01-背包问题-DP" class="headerlink" title="1068 Find More Coins (01 背包问题 DP)"></a>1068 Find More Coins (01 背包问题 DP)</h2><h3 id="1-题目大意-12"><a href="#1-题目大意-12" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>用n个硬币买价值为m的东西，输出使用方案，使得正好几个硬币加起来价值为m。从小到大排列，输出最小的那个排列方案</p><h3 id="2-尝试-DFS"><a href="#2-尝试-DFS" class="headerlink" title="2. 尝试(DFS)"></a>2. 尝试(DFS)</h3><p>本题可以采用递归方案, DFS 递归到底, 直到刚好<code>target==0</code>即可, 类似于二叉树的寻找路径和. 但是某个测试点会超时.</p><p>核心代码(DFS 递归):</p><pre><code class="lang-c++">bool sear(int st, int target){   //搜索 st ~ n, 每次从 st 搜到末位    if(0 == target)        return true;    for (int i = st; i &lt; n; ++i) {        if(target &lt; a[i])   //后面的面额只会越来越大,不用搜了(剪枝)            return false;        tmp.push_back(a[i]);        if (sear(i+1, target-a[i]))  //target减去当前,大目标拆成小目标            return true;   //找到一个为 true 就不用再搜了,返回        else tmp.pop_back();  //sear()==false 表示走不通, pop出来换下一个    }    return false;}sort(a, a+n);  //从小到大排序, 便于输出最小的方案if(a[0] &gt; m){ printf(&quot;No Solution\n&quot;); return 0; } //面额太大,付不了小钱for (int i = 0; i &lt; n; ++i) {  //逐个作为起点进行搜索    tmp.clear();    if(sear(i, m))        break;}</code></pre><h3 id="3-0-1背包问题-DP"><a href="#3-0-1背包问题-DP" class="headerlink" title="3. 0-1背包问题(DP)"></a>3. 0-1背包问题(DP)</h3><p>给定 n 种物品和一个容量为 C 的背包，物品 i 的重量是 wi，其价值为 vi 。</p><p>问：应该如何选择装入背包的物品，使得装入背包中的物品的总价值最大？<br>DP 的题还是要用 DP 来做, 关键在于搞清楚每一个数组的作用</p><p>🎒背包问题的经典转移公式</p><pre><code class="lang-c++">if(j &gt;= w[i])    dp[i][j] = max(dp[i-1][j], dp[i-1][j-w[i]]+v[i]);else    dp[i][j] = dp[i-1][j];  //面对这个 i 物品, 不拿(或装不下), 就等同于 i-1 的情况</code></pre><p><code>dp[i][j]</code> 表示 在面对第 i 件物品，且背包容量为 j 时所能获得的最大价值<br><code>dp[i-1][j-w[i]]+v[i]</code>则是容量空出<code>w[i]</code>的情况下( i-1表示回溯一个序号), 装入<code>v[i]</code>所能得到的价值.</p><h3 id="4-正解-DP"><a href="#4-正解-DP" class="headerlink" title="4. 正解(DP)"></a>4. 正解(DP)</h3><pre><code>8 95 9 8 7 2 3 4 1</code></pre><pre><code>1 3 5</code></pre><pre><code class="lang-c++">int dp[101], choice[10001][101], w[10001];//dp[j] 表示容量为 j 的时候, 最多能装多少钱数; w为每个硬币的面额//choice[i][j]=true 表示总目标钱数为 j 时, i 可以作为一个选择</code></pre><p>本题不是求最大价值, 只要求刚好装满, 只需要加一个判断<code>if(dp[m] != m)</code>, 然后以 m 为目标, 添加所有 choice 筛选的 [<strong>总目标钱数为 m 时, 可以作为一个选择</strong>] 的序列</p><p>本题的判断精髓就在这一句<code>if(dp[j] &lt;= dp[j-w[i]] + w[i])</code> 含义就是, 选择 w[i] 之后, 也能凑得和<code>dp[j]</code>之前 <strong>一样的, 甚至更多的</strong> 钱数.</p><p>本题实际发挥作用的时候, 小于号<code>&lt;</code>实际发挥的作用就是初始化, 让<code>dp[j]</code>从0变成一个数值, 比如<code>dp[9]=9</code>, 9 的面额直接满足; 等于号<code>=</code>实际发挥的作用就是寻找新的路径, 更新<code>choice[i][j]</code> 让<code>1 3 5</code>也组成进来.</p><p>比如, i=5 的时候, w[i]=4 已经更新了 dp[9]=9, dp[5]=5, 此时 j 从 m 遍历递减, 又从 9 开始, 有<code>dp[9] = dp[5] + w[i]</code>, 就把 i=5, 即 w[i]=4 添加进<code>choice[9][5] = true</code>, 作为路径之一</p><pre><code class="lang-c++">sort(w + 1, w + n + 1, cmp);  //从大的面额找起for (int i = 1; i &lt;= n; ++i) {    for (int j = m; j &gt;= w[i]; --j) {    //容量从 m 递减,直到比 w[j] 小则装不下,忽略        if(dp[j] &lt;= dp[j-w[i]] + w[i]){            choice[i][j] = true;            dp[j] = dp[j-w[i]] + w[i];        }    }}if(dp[m] != m) cout &lt;&lt; &quot;No Solution&quot;;else{    int v = m, index = n;  //v 表示目标容量, n 表示初始序号在末尾(从最小的选起)    while (v &gt; 0){        if(choice[index][v]){  //目标为 v 的时候, w[index] 可以作为选项            arr.push_back(w[index]);            v -= w[index];        }        index--;    }    //输出 arr}</code></pre><h3 id="5-后记"><a href="#5-后记" class="headerlink" title="5. 后记"></a>5. 后记</h3><p>一道基础的背包问题, 居然思考了我一整个下午, 算法基础真的薄弱. 鉴于网上的题解要么不够大佬, 代码不够最优; 要么太过于大佬, 解释得很简略. 于是我吧最大佬的代码, 用最清楚的语言解释, 是对自己思路的一个锻炼, 也希望帮助后来的萌新们.</p><hr><p><span id="1044"></span></p><h2 id="1044-Shopping-in-Mars-特定值子序列和-二分查找"><a href="#1044-Shopping-in-Mars-特定值子序列和-二分查找" class="headerlink" title="1044 Shopping in Mars (特定值子序列和, 二分查找)"></a>1044 Shopping in Mars (特定值子序列和, 二分查找)</h2><h3 id="1-题目大意-13"><a href="#1-题目大意-13" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>求一串的数字中连续的一段，使得这个连续的段内数字的和恰好等于所期望的值m. 如果不能找到恰好等于，就找让自己付出最少的价格（总和必须大于等于所给值）的那段区间, 求所有可能的结果.<br>简而言之, 就是找一个连续子序列, 恰好等于 target</p><h3 id="2-个人代码-1"><a href="#2-个人代码-1" class="headerlink" title="2. 个人代码"></a>2. 个人代码</h3><p>因为给定的金币序列是乱序的, 这题一开始没想到可以二分法. 其实题目要求的是连续的子段, 那么就可以逐级叠加求<code>sum</code>, <code>a[j] - a[i-1]</code>即是 i~j 这一段的子段和. 而这个逐级叠加的<code>sum</code>数组, 就是一个完美的递增序列.</p><p>注意解题技巧, 需要多个参数时, 可以采用<code>int &amp;j, int &amp;temp_sum</code>引用传参.</p><pre><code class="lang-c++">#include &lt;iostream&gt;#include &lt;stdlib.h&gt;#include &lt;vector&gt;#include &lt;map&gt;#include &lt;algorithm&gt;using namespace std;vector&lt;int&gt; sum, ans;int n, m;// 从 i 开始, 以 n 结尾的序列, 二分查找// 目标: 找到右端点, 刚好比左端点大 mvoid search(int i, int &amp;j, int &amp;temp_sum){    int left = i, right = n;    int target = sum[left-1] + m;    while (left &lt; right){        // 保留右侧的二分查找, 不求找到而退出        // 最后让 left==right 时退出, 此时是比 target 大的最小数        // 因为 mid 小了的时候, 总是舍弃左端点, mid 大了的时候, 右端点被保留        int mid = (left+right)/2;        if(sum[mid] &lt; target)            left = mid + 1;        else            right = mid;    }    j = right;    temp_sum = sum[right] - sum[i-1];}int main(){    cin &gt;&gt; n &gt;&gt; m;    sum.resize(n+1);    sum[0] = 0;    for (int i = 1; i &lt;= n; ++i) {  //转化为递增序列, 得以应用二分, 提高效率        cin &gt;&gt; sum[i];        sum[i] += sum[i-1];    }    int min_ans = sum[n];  //求比 m 大的最小和    for (int i = 1; i &lt;= n; ++i) {        int j=0, temp_sum=0;  //仅仅作为参数传回来而已        search(i, j, temp_sum);        if(temp_sum &lt; m) continue;        else{            if(temp_sum &lt; min_ans){                min_ans = temp_sum;                ans.clear();                ans.push_back(i);                ans.push_back(j);            }            else if (temp_sum == min_ans){                ans.push_back(i);                ans.push_back(j);            }        }    }    for (int i = 0; i &lt; ans.size(); i += 2) {        cout &lt;&lt; ans[i] &lt;&lt; &quot;-&quot; &lt;&lt; ans[i+1] &lt;&lt; endl;    }    return 0;}</code></pre><hr><p><span id="1085"></span></p><h2 id="1085-Perfect-Sequence-二分查找-upper-bound-双指针"><a href="#1085-Perfect-Sequence-二分查找-upper-bound-双指针" class="headerlink" title="1085 Perfect Sequence (二分查找, upper_bound, 双指针)"></a>1085 Perfect Sequence (二分查找, upper_bound, 双指针)</h2><h3 id="1-题目大意-14"><a href="#1-题目大意-14" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>设这个数列中的最大值是M，最小值是m，如果M &lt;= m * p，则称这个数列是完美数列. 现在给定一个序列, 求满足完美序列的最长子序列长度</p><h3 id="2-二分法"><a href="#2-二分法" class="headerlink" title="2.二分法"></a>2.二分法</h3><p>本题可以应用 upper_bound 来查找, 既然要求满足最大的<code>max &lt;= p*min</code>, 那么找到第一个<code>max &gt; p*min</code>即可, 即找到第一个大于target 的元素.</p><p><strong>upper_bound 代码</strong></p><pre><code class="lang-c++">int bin_sear_upper(int left, int right, int target){    int mid;    while (left &lt; right){        mid = (left+right)/2;        if(a[mid] &gt; target)            right = mid;        else            left = mid + 1;    }    return right;}</code></pre><p>关键点 1: <code>left &lt; right</code>到最后必然会夹出 left==right 返回即可.<br>而普通的查找定值<code>if(a[mid]==target)</code>, 需要<code>left &lt;= right</code>作为条件, 找不到就返回-1</p><p>关键点 2: 先判断<code>&gt;</code>, 让<code>right = mid</code>目的是尽可能保留大数,<code>left = mid + 1</code>小数不保留无所谓.</p><p><strong>lower_bound 代码</strong></p><pre><code class="lang-c++">if(a[mid] &gt;= target)    right = mid;else    left = mid + 1;</code></pre><p>只需换为<code>&gt;=</code>即可, 其他完全一样, 也是保大.</p><p>比如 3 4 5 5 7 8 8 8 9, target 为 8.<br>upper_bound 找到的为 3 4 5 5 7 [8] 8 8 9, lower_bound 找到的为 3 4 5 5 7 8 8 8 [9].</p><p>这三种二分法基本可以涵盖所有二分查找的应用.</p><h3 id="3-个人代码-9"><a href="#3-个人代码-9" class="headerlink" title="3. 个人代码"></a>3. 个人代码</h3><pre><code class="lang-c++">int search(int left, long long target){  // 寻找比 target 小的最大数 upper_bound    int right = n-1, mid;    if(a[right] &lt;= target){  //最大的比 target 小        return right;    }    while (left &lt; right){        mid = (left + right)/2;        if(a[mid] &gt; target)            right = mid;        else            left = mid + 1;    }    return right-1;  // 最后 left==right 找到的是第一个比 tar 大的数, -1 即可}</code></pre><p>应用的是 upper_bound, <code>right-1</code>即可得到满足<code>max &lt;= p*min</code>的最大数字.</p><h3 id="4-坑点"><a href="#4-坑点" class="headerlink" title="4. 坑点"></a>4. 坑点</h3><ol><li><code>段错误</code> 数组开的不够大, 递归调用过多</li><li><code>答案错误</code> 发现存在溢出, int 最大管到 $10^9$, 此题乘起来之后会超过, 要用 long long !</li></ol><h3 id="5-Two-points-双指针法"><a href="#5-Two-points-双指针法" class="headerlink" title="5. Two points 双指针法"></a>5. Two points 双指针法</h3><p>瞬间非常简单</p><pre><code class="lang-c++">    sort(v.begin(), v.end());    int result = 0, temp = 0;    for (int i = 0; i &lt; n; i++) {        for (int j = i + result; j &lt; n; j++) {          //result 已经是最长了, 之后的直接从 result 以上的找起            if (v[j] &lt;= v[i] * p) {                temp = j - i + 1;                if (temp &gt; result)                    result = temp;            } else {                break;            }        }    }    cout &lt;&lt; result;</code></pre><p>效率对比差距明显</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/12/Buth7AGTcCpPonS.png" alt="图片替换文本" width="140" height="180"/></p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PAT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>欢迎来到 Hexo</title>
      <link href="/2020/02/02/hello-world/"/>
      <url>/2020/02/02/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><p>本博客采用 Hexo 快速搭建，也欢迎大家上手使用 Hexo，选择自己喜欢的主题，省时省力，工具齐全，生态优越，可扩展性好</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre><code class="lang-bash">$ hexo new &quot;My New Post&quot;</code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre><code class="lang-bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre><code class="lang-bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre><code class="lang-bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
