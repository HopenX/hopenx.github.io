<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>数学基础备忘录</title>
      <link href="/2020/02/07/math-notes/"/>
      <url>/2020/02/07/math-notes/</url>
      
        <content type="html"><![CDATA[<h1 id="数学基础备忘录"><a href="#数学基础备忘录" class="headerlink" title="数学基础备忘录"></a>数学基础备忘录</h1><h2 id="排列组合公式"><a href="#排列组合公式" class="headerlink" title="排列组合公式"></a>排列组合公式</h2><p>(A_{n}^{m}=n(n-1) \cdots(n-m+1))  m个数递减相乘<br>(C_{n}^{m}=\frac{A_{n}^{m}}{m !}=\frac{n !}{m !(n-m) !}=C_{n}^{n-m})</p><p>$A^4_4 = 4\times3\times2\times1 = 24$</p><p>$A^4_4$ 计算方法：从4 开始，从 4 到 3 到 2…连续乘 4 个数。<br>$C^3_4$ 计算方法：（先计算 $A^3_4$ 再得到 $C^3_4$ ）$C^3_4 = \frac{A^3_4}{3!} = \frac{4\times3\times2}{3\times2\times1} = 24\div6 = 4$</p><p>比如 $A^2_4$ 表示从 4 个东西东西中抽取任意 2 样，一共有 $4\times3 = 12$ <code>排列</code>方式。<br>$C^3_4$ 表示从 4 个东西东西中抽取任意 3 样，一共有 $\frac{4\times3\times2}{3\times2\times1} = 24\div6 = 4$ 两种<code>组成</code>方式。</p><p>A 表示排列，是有序的，而 C 表示组合，表示有多少种<code>组成</code>方式，只看成员，不看顺序。</p><p>对于 A（排列）来说，<code>4 3 1</code>和<code>1 3 4</code>是两种构成，而对于 C（组合）来说，<code>4 3 1</code>和<code>1 3 4</code>就是同一回事。</p><p>$C^3_5$ = 多少？ </p>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> math </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习概述</title>
      <link href="/2020/02/06/ml-study-notes/"/>
      <url>/2020/02/06/ml-study-notes/</url>
      
        <content type="html"><![CDATA[<h1 id="机器学习概述"><a href="#机器学习概述" class="headerlink" title="机器学习概述"></a>机器学习概述</h1><h2 id="机器学习分类"><a href="#机器学习分类" class="headerlink" title="机器学习分类"></a>机器学习分类</h2><ol><li>监督学习：已经有数据，和数据对应的标签。</li><li>非监督学习：给定的样本无需输出/标签，让机器自己学习样本中隐含的内部结构。</li><li>半监督学习：二者结合。</li><li>强化学习：通过打分/评价的形式，类似于监督学习中的标签。</li></ol><h2 id="机器学习模型"><a href="#机器学习模型" class="headerlink" title="机器学习模型"></a>机器学习模型</h2><p>机器学习 = 数据 data + 模型 model + 优化方法 optimal strategy</p><h2 id="偏差-方差权衡"><a href="#偏差-方差权衡" class="headerlink" title="偏差/方差权衡"></a>偏差/方差权衡</h2><p>variance 和 bias，分别对应过拟合和欠拟合</p><p>来自 Wikipedia：</p><blockquote><p>在监督学习中，如果能将模型的方差与误差权衡好，那么可以认为该模型的泛化性能（对于新数据）将会表现出好的结果。</p><p>偏差刻画的是算法本身的性能。高偏差将会造成欠拟合(Underfitting) [miss the relevant relations between features and target outputs]。换句话说，模型越复杂偏差就越小；而模型越简单，偏差就越大。</p><p>方差用来衡量因训练集数据波动(fluctuations)而造成的误差影响。高方差将会造成过拟合(Overfitting)。</p></blockquote><p>在周志华老师&lt;机器学习&gt;书中是这样阐述的：</p><blockquote><p><em>偏差</em> 度量了学习算法的期望预测与真实结果的偏离程度，即刻画了算法本身的拟合能力；</p><p><em>方差</em> 度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响；</p><p><em>噪声</em> 则表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题的本身难度</p><p>偏差-方差分解说明，泛化性能是由学习算法的能力、数据的充分性以及学习任务本身的难度所共同决定的。给定的学习任务，为了取得好的泛化性能，则需使偏差较小，即能够充分拟合数据，并且使方差较小，即使数据扰动产生的影响小。一般来说方差与偏差是有冲突的，这称为方差-偏差窘境。</p></blockquote><h2 id="常见机器学习算法概览"><a href="#常见机器学习算法概览" class="headerlink" title="常见机器学习算法概览"></a>常见机器学习算法概览</h2><h3 id="1-Linear-Algorithm-线性算法"><a href="#1-Linear-Algorithm-线性算法" class="headerlink" title="1. Linear Algorithm 线性算法"></a>1. Linear Algorithm 线性算法</h3><ol><li><p><strong>Linear Regression 线性回归</strong>：使用最小二乘法 Least Squares 拟合一条直线 → 计算 R<sup>2</sup> → 计算 R<sup>2</sup> 的 p 值。R<sup>2</sup> 表示 x 能多大程度反映 y 的变化，p 值表示可靠程度。拟合直线的过程使用「随机梯度下降」（SGD）</p></li><li><p><strong>Lasso 回归 和 Ridge 回归</strong>：都可以减少共线性带来的影响，即 X 自变量之间有相互关联。区别可以归结为L2和L1正则化的性质差异。</p></li><li><p><strong>Polynomial Regression 多项式回归</strong>：能够模拟非线性可分的数据（曲线），线性回归不能做到这一点。但容易过拟合。</p></li><li><p><strong>Logistic Regression 逻辑回归</strong>：判断 True or False，Y 值为 0-1 表示概率，用于分类。线性回归使用「Residual 偏差」，而逻辑回归使用「maximum likelihood 最大似然」</p></li></ol><h3 id="2-Decision-Tree-决策树"><a href="#2-Decision-Tree-决策树" class="headerlink" title="2. Decision Tree 决策树"></a>2. Decision Tree 决策树</h3><ol><li><p><strong>ID3</strong>: 计算「信息熵」 $Entropy(D)$，值越小，说明样本集合D的纯度就越高，进而选择用样本的某一个属性a来划分样本集合D时，就可以得出用属性a对样本D进行划分所带来的「信息增益」 $Gain(D, a)$，值越大，说明如果用属性a来划分样本集合D，那么纯度会提升。 <script type="math/tex">Entropy(t)=-\sum_{k} p\left(c_{k} | t\right) \log p\left(c_{k} | t\right)</script>  <script type="math/tex">Classificationerror (t)=1-\max _{k}\left[p\left(c_{k} | t\right)\right]</script></p></li><li><p><strong>C4.5</strong>: 提出Gainratio 「增益率」，解决ID3决策树的一个缺点，当一个属性的可取值数目较多时，那么可能在这个属性对应的可取值下的样本只有一个或者是很少个，那么这个时候它的信息增益是非常高的，这个时候纯度很高，ID3决策树会认为这个属性很适合划分，但是较多取值的属性来进行划分带来的问题是它的泛化能力比较弱。用 $I(·)$ 表示不纯度——可以是熵可以是基尼，信息增益：<script type="math/tex">\Delta=I(\text { parent })-\sum_{i=1}^{n} \frac{N\left(a_{i}\right)}{N} I\left(a_{i}\right)</script>信息增益率：<script type="math/tex">Gainratio =\frac{\Delta}{Entropy({parent})}</script></p></li><li><p><strong>CART(Classification and Regression Tree)</strong>: 通过计算 Gini 基尼系数（尽可能小），判断 impurity 不纯洁度。离散数据用「是否」划分子树，连续数据可以用「两两之间平均值」划分子树。<script type="math/tex">{Gini}(t)=1-\sum_{k}\left[p\left(c_{k} | t\right)\right]^{2}</script>D 分裂为 DL 和 DR，分裂后的信息增益<script type="math/tex">Gain(D, A)=\frac{\left|D_{L}\right|}{|D|} \operatorname{Gini}\left(D_{L}\right)+\frac{\left|D_{R}\right|}{|D|} \operatorname{Gini}\left(D_{R}\right)</script></p></li></ol><h3 id="3-SVM-支持向量机"><a href="#3-SVM-支持向量机" class="headerlink" title="3. SVM 支持向量机"></a>3. SVM 支持向量机</h3><p>SVM：<a href="https://blog.csdn.net/liugan528/article/details/79448379" target="_blank" rel="noopener">https://blog.csdn.net/liugan528/article/details/79448379</a></p><p>KKT：<a href="https://blog.csdn.net/qq_32763149/article/details/81055062" target="_blank" rel="noopener">https://blog.csdn.net/qq_32763149/article/details/81055062</a></p><p><strong>SVM 分类</strong>：</p><ol><li>硬间隔支持向量机（线性可分支持向量机）：当训练数据线性可分时，可通过硬间隔最大化学得一个线性可分支持向量机。</li><li>软间隔支持向量机：当训练数据近似线性可分时，可通过软间隔最大化得到一个线性支持向量机。</li><li>非线性支持向量机：当训练数据线性不可分时，可通过核方法以及软间隔最大化得一个非线性支持向量机。</li></ol><p><strong>基本原理</strong>：</p><ol><li><p>Maximum Margin Classifier：只看边界。</p></li><li><p>Soft Margin Classifier（即 Support Vector Classifier）：允许 misclassification误分类，寻找两个支撑向量来确定分类边界。</p></li><li><p>Kernel Function：非线性SVM，从低维数据开始，通过「核函数」给数据升维，然后找到一个 Support Vector Classifier 将数据分成两组。核函数的选择，支撑向量的选择，都用 cross validation 交叉验证。</p></li><li><p>Kernel Trick: 根据升维的距离进行计算，但是不进行实际的升维。</p></li></ol><p><strong>具体过程</strong>：</p><ol><li><p>线性可分的情况：对于超平面 $w \cdot x+b=0$ 和 $margin$ 有关系<script type="math/tex">{margin}=\frac{2}{\|w\|}</script><br> 最大化 $margin$ 等效于最小化 $\frac{1}{2}|w|^{2}$</p><p> 形成一个拉格朗日乘子α的约束问题 <script type="math/tex">\begin{array}{ll}{\min _{w, b}} & {\frac{1}{2}|w|^{2}} {\text {s.t.}} & {y_{i}\left(w \cdot x_{i}+b\right)-1 \geq 0}\end{array}</script><br> 可以列式 <script type="math/tex">L(w, b, \alpha)=\frac{1}{2}|w|^{2}-\sum_{i=1}^{N} \alpha_{i}\left[y_{i}\left(w \cdot x_{i}+b\right)-1\right]</script><br> 拉格朗日对偶性：解决「凸二次规划」（convex quadratic propgramming）问题，即将原始的约束最优化问题可等价于极大极小的对偶问题（以 w,b 作参数时的最小值，以α作参数时的最大值）<br> <script type="math/tex">\max _{\alpha} \min _{w, b} \quad L(w, b, \alpha)</script>通过求导一系列步骤，转换成\begin{array}{ll}<br>{\min _{\alpha}} &amp; {\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i}} \\<br>{\text { s.t. }} &amp; {\sum_{i=1}^{N} \alpha_{i} y_{i}=0} \\<br>{} &amp; {\alpha_{i} \geq 0, \quad i=1,2, \cdots, N}<br>\end{array}</p></li><li><p>线性不可分的情况：对每个样本引入一个松弛变量 $\xi_{i} \geq 0$, 约束条件和目标函数变为</p><script type="math/tex; mode=display">\begin{aligned}&y_{i}\left(w \cdot x_{i}+b\right) \geq 1-\xi_{i}\\&\min _{w, b, \xi} \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i}\end{aligned}</script></li></ol><p><strong>部分术语</strong>：</p><ol><li><p>KKT 条件：是拉格朗日乘子的泛化，把所有的不等式约束、等式约束和目标函数全部写为一个式子L(a, b, x)= f(x) + a<em>g(x) + b</em>h(x)，KKT条件是说最优值必须满足以下条件：（1）L(a, b, x)对x求导为零；（2）h(x) =0; （3）a*g(x) = 0;</p></li><li><p>SMO：Sequential Minimal Optimization用二次规划来求解α，要用到 KKT</p></li><li><p>SVR：支持向量回归</p></li></ol><p><strong>优点</strong>：<br>SVM在中小量样本规模的时候容易得到数据和特征之间的非线性关系，可以避免使用神经网络结构选择和局部极小值问题，可解释性强，可以解决高维问题。</p><p><strong>缺点</strong>：<br>SVM对缺失数据敏感，对非线性问题没有通用的解决方案，核函数的正确选择不容易，计算复杂度高，主流的算法可以达到O(n2)O(n2)的复杂度，这对大规模的数据是吃不消的。</p><h3 id="4-Naive-Bayes-Algorithms-朴素贝叶斯"><a href="#4-Naive-Bayes-Algorithms-朴素贝叶斯" class="headerlink" title="4. Naive Bayes Algorithms 朴素贝叶斯"></a>4. Naive Bayes Algorithms 朴素贝叶斯</h3><ol><li>Naive Bayes</li><li>Gaussian Naive Bayes</li><li>Multinomial Naive Bayes</li><li>Bayesian Belief Network (BBN)</li><li>Bayesian Network (BN)</li></ol><p>朴素贝叶斯基本公式：$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$<br><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.imgur.com/O9FsXbG.png" alt=""></p><h3 id="5-KNN-k-NearestNeighbor-K-最邻近算法"><a href="#5-KNN-k-NearestNeighbor-K-最邻近算法" class="headerlink" title="5. KNN (k-NearestNeighbor) K 最邻近算法"></a>5. KNN (k-NearestNeighbor) K 最邻近算法</h3><p>用于分类</p><ol><li>计算测试数据与各个训练数据之间的距离；</li><li>按照距离的递增关系进行排序；</li><li>选取距离最小的K个点；</li><li>确定前K个点所在类别的出现频率；</li><li>返回前K个点中出现频率最高的类别作为测试数据的预测分类</li></ol><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.imgur.com/85vZVhC.png" alt=""></p><h3 id="6-Clustering-Algorithm-聚类算法"><a href="#6-Clustering-Algorithm-聚类算法" class="headerlink" title="6. Clustering Algorithm 聚类算法"></a>6. Clustering Algorithm 聚类算法</h3><ol><li>k-Means：选取平均值</li><li>k-Medians：由选取平均值改为选取中位数</li><li>Expectation Maximisation (EM)：有隐含随机变量的概率模型的参数的估计方法，它是一种无监督的算法</li><li>Hierarchical Clustering 层次聚类：<br>```<br>(1) 将每个对象看作一类，计算两两之间的最小距离；</li></ol><p>(2) 将距离最小的两个类合并成一个新类；</p><p>(3) 重新计算新类与所有类之间的距离；</p><p>(4) 重复(2)、(3)，直到所有类最后合并成一类。</p><pre><code>### 7. K-means 算法</code></pre><p>选取k个初始质心(作为初始cluster);<br>repeat:<br>    对每个样本点，计算得到距其最近的质心，将其类别标为该质心所对应的cluster;<br>    重新计算k个cluser对应的质心;<br>until 质心不再发生变化<br>```<br><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.imgur.com/a6Sp3Ee.png" alt=""></p><h3 id="8-Random-Forest-随机森林"><a href="#8-Random-Forest-随机森林" class="headerlink" title="8. Random Forest 随机森林"></a>8. Random Forest 随机森林</h3><h3 id="9-Dimensionality-Reduction-Algorithms-降维算法"><a href="#9-Dimensionality-Reduction-Algorithms-降维算法" class="headerlink" title="9. Dimensionality Reduction Algorithms 降维算法"></a>9. Dimensionality Reduction Algorithms 降维算法</h3><h3 id="10-Gradient-Boosting-algorithms-梯度提升算法"><a href="#10-Gradient-Boosting-algorithms-梯度提升算法" class="headerlink" title="10. Gradient Boosting algorithms 梯度提升算法"></a>10. Gradient Boosting algorithms 梯度提升算法</h3><ol><li>GBM</li><li>XGBoost</li><li>LightGBM</li><li>CatBoost</li></ol><h3 id="11-Deep-Learning-Algorithms-深度学习"><a href="#11-Deep-Learning-Algorithms-深度学习" class="headerlink" title="11. Deep Learning Algorithms 深度学习"></a>11. Deep Learning Algorithms 深度学习</h3><ol><li>Convolutional Neural Network (CNN)</li><li>Recurrent Neural Networks (RNNs)</li><li>Long Short-Term Memory Networks (LSTMs)</li><li>Stacked Auto-Encoders</li><li>Deep Boltzmann Machine (DBM)</li><li>Deep Belief Networks (DBN)</li></ol><hr><h2 id="机器学习损失函数"><a href="#机器学习损失函数" class="headerlink" title="机器学习损失函数"></a>机器学习损失函数</h2><ol><li>0-1损失函数<script type="math/tex; mode=display">L(y,f(x)) =\begin{cases}0, & \text{y = f(x)}  \\1, & \text{y $\neq$ f(x)}\end{cases}</script></li><li>绝对值损失函数<script type="math/tex; mode=display">L(y,f(x))=|y-f(x)|</script></li><li>平方损失函数<script type="math/tex; mode=display">L(y,f(x))=(y-f(x))^2</script></li><li>log对数损失函数<script type="math/tex; mode=display">L(y,f(x))=log(1+e^{-yf(x)})</script></li><li>指数损失函数<script type="math/tex; mode=display">L(y,f(x))=exp(-yf(x))</script></li><li>Hinge损失函数<script type="math/tex; mode=display">L(w,b)=max\{0,1-yf(x)\}</script></li></ol><hr><h2 id="机器学习优化方法"><a href="#机器学习优化方法" class="headerlink" title="机器学习优化方法"></a>机器学习优化方法</h2><p>梯度下降是最常用的优化方法之一，它使用梯度的反方向 $ \nabla_\theta J(\theta) $ 更新参数 $ \theta $，使得目标函数$J(\theta)$达到最小化的一种优化方法，这种方法我们叫做梯度更新. </p><ol><li>(全量)梯度下降<script type="math/tex; mode=display">\theta=\theta-\eta\nabla_\theta J(\theta)</script></li><li>随机梯度下降<script type="math/tex; mode=display">\theta=\theta-\eta\nabla_\theta J(\theta;x^{(i)},y^{(i)})</script></li><li>小批量梯度下降<script type="math/tex; mode=display">\theta=\theta-\eta\nabla_\theta J(\theta;x^{(i:i+n)},y^{(i:i+n)})</script></li><li>引入动量的梯度下降<script type="math/tex; mode=display">\begin{cases}v_t=\gamma v_{t-1}+\eta \nabla_\theta J(\theta)  \\\theta=\theta-v_t\end{cases}</script></li><li>自适应学习率的Adagrad算法<script type="math/tex; mode=display">\begin{cases}g_t= \nabla_\theta J(\theta)  \\\theta_{t+1}=\theta_{t,i}-\frac{\eta}{\sqrt{G_t+\varepsilon}} \cdot g_t\end{cases}</script></li><li><p>牛顿法</p><script type="math/tex; mode=display">\theta_{t+1}=\theta_t-H^{-1}\nabla_\theta J(\theta_t)</script><p> 其中:<br> $t$: 迭代的轮数</p><p> $\eta$: 学习率</p><p> $G_t$: 前t次迭代的梯度和</p><p> $\varepsilon:$很小的数,防止除0错误</p><p> $H$: 损失函数相当于$\theta$的Hession矩阵在$\theta_t$处的估计</p></li></ol><hr><h2 id="机器学习的评价指标"><a href="#机器学习的评价指标" class="headerlink" title="机器学习的评价指标"></a>机器学习的评价指标</h2><ol><li>MSE(Mean Squared Error)<script type="math/tex; mode=display">MSE(y,f(x))=\frac{1}{N}\sum_{i=1}^{N}(y-f(x))^2</script></li><li>MAE(Mean Absolute Error)<script type="math/tex; mode=display">MSE(y,f(x))=\frac{1}{N}\sum_{i=1}^{N}|y-f(x)|</script></li><li>RMSE(Root Mean Squard Error)<script type="math/tex; mode=display">RMSE(y,f(x))=\frac{1}{1+MSE(y,f(x))}</script></li><li>Top-k准确率<script type="math/tex; mode=display">Top_k(y,pre_y)=\begin{cases}1, {y \in pre_y}  \\0, {y \notin pre_y}\end{cases}</script></li><li>混淆矩阵</li></ol><p>混淆矩阵|Predicted as Positive|Predicted as Negative<br>|:-:|:-:|:-:|<br>|Labeled as Positive|True Positive(TP)|False Negative(FN)|<br>|Labeled as Negative|False Positive(FP)|True Negative(TN)|</p><ul><li>真正例(True Positive, TP):真实类别为正例, 预测类别为正例</li><li>假负例(False Negative, FN): 真实类别为正例, 预测类别为负例</li><li>假正例(False Positive, FP): 真实类别为负例, 预测类别为正例 </li><li><p>真负例(True Negative, TN): 真实类别为负例, 预测类别为负例</p></li><li><p>真正率(True Positive Rate, TPR): 被预测为正的正样本数 / 正样本实际数</p><script type="math/tex; mode=display">TPR=\frac{TP}{TP+FN}</script></li><li><p>假负率(False Negative Rate, FNR): 被预测为负的正样本数/正样本实际数</p><script type="math/tex; mode=display">FNR=\frac{FN}{TP+FN}</script></li><li><p>假正率(False Positive Rate, FPR): 被预测为正的负样本数/负样本实际数，</p><script type="math/tex; mode=display">FPR=\frac{FP}{FP+TN}</script></li><li>真负率(True Negative Rate, TNR): 被预测为负的负样本数/负样本实际数，<script type="math/tex; mode=display">TNR=\frac{TN}{FP+TN}</script></li><li>准确率(Accuracy)<script type="math/tex; mode=display">ACC=\frac{TP+TN}{TP+FN+FP+TN}</script></li><li>精准率<script type="math/tex; mode=display">P=\frac{TP}{TP+FP}</script></li><li>召回率<script type="math/tex; mode=display">R=\frac{TP}{TP+FN}</script></li><li>F1-Score<script type="math/tex; mode=display">\frac{2}{F_1}=\frac{1}{P}+\frac{1}{R}</script></li><li><strong>ROC</strong></li></ul><p>ROC曲线的横轴为“假正例率”，纵轴为“真正例率”. 以FPR为横坐标，TPR为纵坐标，那么ROC曲线就是改变各种阈值后得到的所有坐标点 (FPR,TPR) 的连线，画出来如下。红线是随机乱猜情况下的ROC，曲线越靠左上角，分类器越佳. </p><ul><li><strong>AUC(Area Under Curve)</strong></li></ul><p>AUC就是ROC曲线下的面积. 真实情况下，由于数据是一个一个的，阈值被离散化，呈现的曲线便是锯齿状的，当然数据越多，阈值分的越细，”曲线”越光滑. </p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://gss3.bdstatic.com/-Po3dSag_xI4khGkpoWK1HF6hhy/baike/c0%3Dbaike80%2C5%2C5%2C80%2C26/sign=b9cb389a68d0f703f2bf9d8e69933a58/f11f3a292df5e0feaafde78c566034a85fdf7251.jpg"></p><p>用AUC判断分类器（预测模型）优劣的标准:</p><ul><li>AUC = 1 是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数预测的场合，不存在完美分类器.</li><li>0.5 &lt; AUC &lt; 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值.</li><li>AUC &lt; 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测.</li></ul><h2 id="机器学习模型选择"><a href="#机器学习模型选择" class="headerlink" title="机器学习模型选择"></a>机器学习模型选择</h2><ol><li>交叉验证</li></ol><p>所有数据分为三部分：训练集、交叉验证集和测试集。交叉验证集不仅在选择模型时有用，在超参数选择、正则项参数 [公式] 和评价模型中也很有用。</p><ol><li>k-折叠交叉验证</li></ol><ul><li>假设训练集为S ，将训练集等分为k份:$\{S_1, S_2, …, S_k\}$. </li><li>然后每次从集合中拿出k-1份进行训练</li><li>利用集合中剩下的那一份来进行测试并计算损失值</li><li>最后得到k次测试得到的损失值，并选择平均损失值最小的模型</li></ul><ol><li>Bias与Variance，欠拟合与过拟合</li></ol><p><strong>欠拟合</strong>一般表示模型对数据的表现能力不足，通常是模型的复杂度不够，并且Bias高，训练集的损失值高，测试集的损失值也高.</p><p><strong>过拟合</strong>一般表示模型对数据的表现能力过好，通常是模型的复杂度过高，并且Variance高，训练集的损失值低，测试集的损失值高.</p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://pic3.zhimg.com/80/v2-e20cd1183ec930a3edc94b30274be29e_hd.jpg"></p><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://pic1.zhimg.com/80/v2-22287dec5b6205a5cd45cf6c24773aac_hd.jpg"></p><ol><li>解决方法</li></ol><ul><li>增加训练样本: 解决高Variance情况</li><li>减少特征维数: 解决高Variance情况</li><li>增加特征维数: 解决高Bias情况</li><li>增加模型复杂度: 解决高Bias情况</li><li>减小模型复杂度: 解决高Variance情况</li></ul><h2 id="机器学习参数调优"><a href="#机器学习参数调优" class="headerlink" title="机器学习参数调优"></a>机器学习参数调优</h2><ol><li>网格搜索</li></ol><p>一种调参手段；穷举搜索：在所有候选的参数选择中，通过循环遍历，尝试每一种可能性，表现最好的参数就是最终的结果</p><ol><li>随机搜索</li></ol><p>与网格搜索相比，随机搜索并未尝试所有参数值，而是从指定的分布中采样固定数量的参数设置。它的理论依据是，如果随即样本点集足够大，那么也可以找到全局的最大或最小值，或它们的近似值。通过对搜索范围的随机取样，随机搜索一般会比网格搜索要快一些。</p><ol><li>贝叶斯优化算法</li></ol><p>贝叶斯优化用于机器学习调参由J. Snoek(2012)提出，主要思想是，给定优化的目标函数(广义的函数，只需指定输入和输出即可，无需知道内部结构以及数学性质)，通过不断地添加样本点来更新目标函数的后验分布(高斯过程,直到后验分布基本贴合于真实分布。简单的说，就是考虑了上一次参数的信息，从而更好的调整当前的参数。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PAT-甲级做题笔记</title>
      <link href="/2020/02/06/pat-advance/"/>
      <url>/2020/02/06/pat-advance/</url>
      
        <content type="html"><![CDATA[<h1 id="PAT-甲级-做题笔记"><a href="#PAT-甲级-做题笔记" class="headerlink" title="PAT-甲级 做题笔记"></a>PAT-甲级 做题笔记</h1><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><p>0000 <a href="#0000">做题 Tips 基本经验</a><br>1003 <a href="#1003">Emergency (Dijkstra 算法)</a><br>1004 <a href="#1004">Counting Leaves (计算叶节点数，DFS/BFS 树算法)</a><br>1007 <a href="#1007">Maximum Subsequence Sum(最大子序列和)</a><br>1010 <a href="#1010">Radix (进制转换/二分法)</a><br>1012 <a href="#1012">The Best Rank (应用问题，数据结构设计，多维度排序)</a><br>1013 <a href="#1013">Battle Over Cities (图的遍历，统计强连通分量的个数，DFS)</a><br>1014 <a href="#1014">Waiting in Line (队列应用，排队问题)</a><br>1015 <a href="https://pintia.cn/problem-sets/994805342720868352/problems/994805495863296000" target="_blank" rel="noopener">Reversible Primes (进制转换+判断素数：可取用函数)</a><br>1016 <a href="#1016">Phone Bills (日期类计算)</a><br>1018 <a href="#1018">Public Bike Management (图最短路径 Dijkstra + DFS)</a><br>1020 <a href="#1020">Tree Traversals (已知后序和中序，转前序/层序)</a><br>1021 <a href="#1021">Deepest Root (求树中最长的路径，DFS，连通分量)</a></p><hr><h2 id="0000-做题-Tips-基本经验"><a href="#0000-做题-Tips-基本经验" class="headerlink" title="0000 做题 Tips 基本经验"></a>0000 做题 Tips 基本经验<span id="0000"></span></h2><ol><li>最后千万别栽在头文件上，比如 reverse() 属于 &lt; algorithm&gt;</li><li><p>输出多行的时候，最后有一个多余的空行也没问题<br>比如使用 <code>cout &lt;&lt; a &lt;&lt; endl</code> 三次，样例输出：</p><pre><code> 3 4 5</code></pre><p> 实际输出：最后有一个多余空行没关系</p><pre><code> 3 4 5</code></pre></li></ol><hr><h2 id="1003-Emergency-Dijkstra-算法"><a href="#1003-Emergency-Dijkstra-算法" class="headerlink" title="1003 Emergency (Dijkstra 算法)"></a>1003 Emergency (Dijkstra 算法)<span id="1003"></span></h2><h3 id="1-题目大意"><a href="#1-题目大意" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>n个城市m条路，每个城市有救援小组，所有的边的边权已知。给定起点和终点，求从起点到终点的最短路径条数以及最短路径上的救援小组数目之和。如果有多条就输出点权（城市救援小组数目）最大的那个</p><h3 id="2-分析"><a href="#2-分析" class="headerlink" title="2. 分析"></a>2. 分析</h3><p>用一遍Dijkstra算法～救援小组个数相当于点权，用Dijkstra求边权最小的最短路径的条数，以及这些最短路径中点权最大的值～dis[i]表示从出发点到i结点最短路径的路径长度，num[i]表示从出发点到i结点最短路径的条数，w[i]表示从出发点到i点救援队的数目之和～当判定dis[u] + e[u][v] &lt; dis[v]的时候，不仅仅要更新dis[v]，还要更新num[v] = num[u], w[v] = weight[v] + w[u]; 如果dis[u] + e[u][v] == dis[v]，还要更新num[v] += num[u]，而且判断一下是否权重w[v]更小，如果更小了就更新w[v] = weight[v] + w[u]</p><h3 id="3-个人代码"><a href="#3-个人代码" class="headerlink" title="3. 个人代码"></a>3. 个人代码</h3><p><a href="https://pintia.cn/problem-sets/994805342720868352/problems/994805523835109376" target="_blank" rel="noopener">PTA-1003</a></p><h3 id="4-学习要点"><a href="#4-学习要点" class="headerlink" title="4. 学习要点"></a>4. 学习要点</h3><blockquote><p>1.真正理解单源最短路径，每一次选择的 dis 都是相对于源点的最短路径<br>2.大数组要设为全局变量<br>3.外层循环 i：n 次循环，每次访问一个新点，保证 n 个点全访问到；内层循环 j：继续寻找还未访问的点，找 dis 最小的访问；内层循环 k：更新所有能够更新的 dis</p></blockquote><hr><h2 id="1004-Counting-Leaves-dfs-bfs-树算法"><a href="#1004-Counting-Leaves-dfs-bfs-树算法" class="headerlink" title="1004 Counting Leaves (dfs/bfs 树算法)"></a>1004 Counting Leaves (dfs/bfs 树算法)<span id="1004"></span></h2><h3 id="1-题目大意-1"><a href="#1-题目大意-1" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>给出一棵树，问每一层各有多少个叶子结点</p><h3 id="2-坑"><a href="#2-坑" class="headerlink" title="2. 坑"></a>2. 坑</h3><pre><code>list[tmp].lev = list[fa].lev + 1;  //出问题,不一定按照顺序输入</code></pre><p>有可能先 03 后 01，fa 的 lev 还没确定，不能给孩子节点 +1</p><pre><code>3 203 1 0201 1 03</code></pre><h3 id="3-个人代码-1"><a href="#3-个人代码-1" class="headerlink" title="3. 个人代码"></a>3. 个人代码</h3><p><a href="https://pintia.cn/problem-sets/994805342720868352/problems/994805521431773184" target="_blank" rel="noopener">PTA-1004</a></p><h3 id="4-正确方法"><a href="#4-正确方法" class="headerlink" title="4. 正确方法"></a>4. 正确方法</h3><pre class=" language-lang-C++"><code class="language-lang-C++">void dfs(int fa){    for (auto &ch : list[fa].v) {        list[ch].lev = list[fa].lev + 1;        dfs(ch);    }}dfs(1);   //通过 DFS 一层层给叶子节点 lev+1</code></pre><hr><h2 id="1007-Maximum-Subsequence-Sum-最大子序列和"><a href="#1007-Maximum-Subsequence-Sum-最大子序列和" class="headerlink" title="1007 Maximum Subsequence Sum (最大子序列和)"></a>1007 Maximum Subsequence Sum (最大子序列和)<span id="1007"></span></h2><h3 id="1-题目大意-2"><a href="#1-题目大意-2" class="headerlink" title="1.题目大意"></a>1.题目大意</h3><p>求最大连续子序列和，输出最大的和以及这个子序列的开始值和结束值。如果所有数都小于0，那么认为最大的和为0，并且输出首尾元素</p><h3 id="2-分析-1"><a href="#2-分析-1" class="headerlink" title="2.分析"></a>2.分析</h3><p>本质上是<strong>动态规划</strong>的思想，数组为<code>vec[]</code>，设<code>dp[i]</code> 是以<code>vec[i]</code>结尾的子数组的最大和，对于元素<code>vec[i+1]</code>, 它有两种选择：<code>vec[i+1]</code>接着前面的子数组构成最大和; <code>vec[i+1]</code>自己单独构成子数组。则<code>dp[i+1] = max{dp[i]+vec[i+1],  vec[i+1]}</code></p><p>简化则用一个 temp_sum 和一个 temp_first 解决，真正最大和为 sum，起始点为 first 和 last，建立局部和全局的关系。如果 <code>temp_sum &lt; 0</code>, 说明目前这一段对后续的序列和已经没有加成作用，可以舍弃另立门户，令<code>temp_sum = 0</code></p><h3 id="3-个人代码-2"><a href="#3-个人代码-2" class="headerlink" title="3. 个人代码"></a>3. 个人代码</h3><pre class=" language-lang-c++"><code class="language-lang-c++">//初始值设置容易坑：sum要为-1 才能开始int sum=-1, temp_sum=0, first=0, last=k-1, temp_first=0;for (int j = 0; j < k; ++j) {    temp_sum += a[j];    if(temp_sum < 0){        temp_sum = 0;        temp_first = j+1;   //本段已经可以舍弃，开始新一段    } else if(temp_sum > sum){        sum = temp_sum;        first = temp_first;        last = j;    }}if(sum < 0) sum = 0;  //如果全负则为 0，根据题目要求不要漏情况</code></pre><h3 id="4-类似题"><a href="#4-类似题" class="headerlink" title="4. 类似题"></a>4. 类似题</h3><p><a href="https://www.cnblogs.com/grandyang/p/4480780.html" target="_blank" rel="noopener">最长无重复子串</a></p><hr><h2 id="1010-Radix-进制转换-二分法"><a href="#1010-Radix-进制转换-二分法" class="headerlink" title="1010 Radix (进制转换/二分法)"></a>1010 Radix (进制转换/二分法)<span id="1010"></span></h2><h3 id="1-题目大意-3"><a href="#1-题目大意-3" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>给定两个相等数，已知一个数的进制，求另外一个数的进制（radix-&gt;基数）</p><h3 id="2-分析-2"><a href="#2-分析-2" class="headerlink" title="2. 分析"></a>2. 分析</h3><p>通用函数<code>cal(string str, int radix)</code>把任意进制的数转换为 10 进制数<br>在使用搜索遍历，找到另一个数对应的进制</p><h3 id="3-个人代码-3"><a href="#3-个人代码-3" class="headerlink" title="3.个人代码"></a>3.个人代码</h3><p><a href="https://pintia.cn/problem-sets/994805342720868352/problems/994805507225665536" target="_blank" rel="noopener">PTA-1010</a></p><p>转换函数</p><pre class=" language-lang-c++"><code class="language-lang-c++">long long cal(string str, long long radix){    long long target=0, bak = 1;    reverse(str.begin(), str.end());    for (char c : str) {        if(isdigit(c))            target += (c - '0') * bak;        else            target += (c - 'a' + 10) * bak;        bak *= radix;  //也可以使用<cmath>的 pow()函数    }    return target;}</code></pre><p>搜索函数</p><pre class=" language-lang-c++"><code class="language-lang-c++">long long find_radix(string str, long long tar){    char c = *max_element(str.begin(), str.end());  //最大字母    //进制至少比最大字母要大    long long low = isdigit(c) ? c-'0' : c-'a'+10;    low += 1;  //必须要加一,至少多1    long long high = max(low, tar);  //进制最大不会大于目标 tar    while (low <= high){        long long mid = (low+high)/2;        long long tmp = cal(str, mid);        if(tmp < 0 || tmp > tar) high = mid - 1;  //小于 0 也是进制太大        else if(tmp == tar) return mid;        else low = mid + 1;    }    return -1;}</code></pre><h3 id="4-学习要点-1"><a href="#4-学习要点-1" class="headerlink" title="4. 学习要点"></a>4. 学习要点</h3><blockquote><ol><li>进制转换这种大数字的乘法很容易溢出，必须要用 <code>long long</code></li><li>暴力搜索（从 2 到 ∞ 容易超时）最好用二分法，并且限定范围：最小是 <code>low</code> 至少是最大的那个字母+1，比如 fff 最少是 15+1=16 进制，最大是<code>high</code>不超过目标 <code>tar</code></li><li>可以使用反向迭代器 <code>it = n.rbegin(); it != n.rend()</code>代替<code>reverse()</code></li></ol></blockquote><hr><h2 id="1012-The-Best-Rank-数据结构设计-多维度排序"><a href="#1012-The-Best-Rank-数据结构设计-多维度排序" class="headerlink" title="1012 The Best Rank (数据结构设计/多维度排序)"></a>1012 The Best Rank (数据结构设计/多维度排序)<span id="1012"></span></h2><h3 id="1-题目大意-4"><a href="#1-题目大意-4" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>找出每个人自己最优势的科目，也就是单独排名最好的科目，优先级 A &gt; C &gt; M &gt; E</p><ul><li>Sample Input:<pre><code>5 6310101 98 85 88310102 70 95 88310103 82 87 94310104 91 91 91310105 85 90 90310101310102310103310104310105999999</code></pre></li><li>Sample Output:<pre><code>1 C1 M1 E1 A3 AN/A</code></pre></li></ul><h3 id="2-个人代码"><a href="#2-个人代码" class="headerlink" title="2. 个人代码"></a>2. 个人代码</h3><p><a href="https://pintia.cn/problem-sets/994805342720868352/problems/994805502658068480" target="_blank" rel="noopener">PTA-1012</a></p><h3 id="3-学习要点"><a href="#3-学习要点" class="headerlink" title="3. 学习要点"></a>3. 学习要点</h3><p>数据结构设计</p><pre class=" language-lang-c++"><code class="language-lang-c++">struct node{    //一定要用数组形式，避免过多变量 int c, m, e, c_r, m_r, e_r, a_r;    int id, best;    int score[4], rank[4];}stu[2001];  //输入学生序列，非学号排序int exist[1000000], flag=-1; //exist 至少 >0, 顺便建立 id-输入序号 映射//使用 flag 配置作用，避免写过多重复的 cmp 函数bool cmp(node &a, node &b){ return a.score[flag] > b.score[flag];}</code></pre><p>输入后，利用<code>exist[stu[k].id] = k + 1; k++</code> 依次建立映射<br>输出时，利用<code>cin &gt;&gt; tmp; ex = exist[tmp]</code>和<code>stu[ex-1]</code>实现映射，无需 find<br>处理时，使用<code>sort(stu, stu+n, cmp)</code> 循环 <code>flag++</code>，实现分学科多次排序，确定 <code>rank[]</code></p><hr><h2 id="1013-Battle-Over-Cities（图的遍历，统计强连通分量的个数，dfs）"><a href="#1013-Battle-Over-Cities（图的遍历，统计强连通分量的个数，dfs）" class="headerlink" title="1013 Battle Over Cities（图的遍历，统计强连通分量的个数，dfs）"></a>1013 Battle Over Cities（图的遍历，统计强连通分量的个数，dfs）<span id="1013"></span></h2><h3 id="1-题目大意-5"><a href="#1-题目大意-5" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>给出n个城市之间有相互连接的m条道路，当删除一个城市和其连接的道路的时候，问其他几个剩余的城市至少要添加多少个路线，才能让它们重新变为连通图</p><h3 id="2-分析-3"><a href="#2-分析-3" class="headerlink" title="2. 分析"></a>2. 分析</h3><p>n 连通分量，最少需要 n-1 条边相连，所以本题实质就是求「去除某点及其相连的边」后，剩余的「连通分量个数」是一个图算法，需要用图的遍历来解决 (DFS)</p><blockquote><p>求连通分量依据：一次 <code>dfs()</code>走到底，可以访问完一个连通分量</p></blockquote><p>数据结构：</p><blockquote><p>涉及图的遍历，要考虑 <code>visit[]</code>数组<br>图的路径存储，使用二维矩阵</p></blockquote><h3 id="3-个人代码-4"><a href="#3-个人代码-4" class="headerlink" title="3. 个人代码"></a>3. 个人代码</h3><p><a href="https://pintia.cn/problem-sets/994805342720868352/problems/994805500414115840" target="_blank" rel="noopener">PTA-1013</a></p><pre class=" language-lang-c++"><code class="language-lang-c++">for (int i = 0; i < k; ++i) {        int cur, cnt=0;        cin >> cur;        fill(visited, visited+n, false);        visited[cur] = true;  //相当于把本点去除        for (int j = 1; j <= n; ++j) {  //依次计算所有连通分量，而非从本点出发            if(!visited[j]){                dfs(j);                cnt++;            }        }        cout << cnt-1 << endl;    }</code></pre><pre class=" language-lang-c++"><code class="language-lang-c++">void dfs(int st){   //只用来 visit，不负责计数    visited[st] = true;    for (int i = 0; i <= n; ++i)        if(!visited[i] && e[st][i]){   //未访问且有路            dfs(i);        }}</code></pre><hr><h2 id="1014-Reversible-Primes（队列应用-排队问题）"><a href="#1014-Reversible-Primes（队列应用-排队问题）" class="headerlink" title="1014 Reversible Primes（队列应用 排队问题）"></a>1014 Reversible Primes（队列应用 排队问题）<span id="1014"></span></h2><h3 id="1-题目大意-6"><a href="#1-题目大意-6" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>n个窗口，每个窗口可以排队m人。有k位用户需要服务，给出了每位用户需要的minute数，所有客户在8点开始服务，如果有窗口还没排满就入队，否则就在黄线外等候。如果有某一列有一个用户走了服务完毕了，黄线外的人就进来一个。如果同时就选窗口数小的。求q个人的服务结束时间。<br>如果一个客户在17:00以及以后还没有开始服务（此处不是结束服务是开始17:00）就不再服务输出sorry；如果这个服务已经开始了，无论时间多长都要等他服务完毕。</p><h3 id="2-学习要点"><a href="#2-学习要点" class="headerlink" title="2. 学习要点"></a>2. 学习要点</h3><p>面对此类题目，总想按照 timeline 每分每秒去决策，其实队列的问题用队列解决。</p><p>以每个窗口为单位设计结构体，<code>poptime</code> 用于刚入黄线的人决策，<code>endtime</code> 用于已经在队内的人是否会 sorry 决策，<code>q</code> 表示本队列内排队情况。</p><p>给定客户序列，则以每一个客户为循环单位<code>(index++)</code>，队列有一个进就会有一个出，每次变动一个单位，更新各个队列的<code>poptime endtime q</code>等变量。</p><pre class=" language-lang-c++"><code class="language-lang-c++">struct node {    int poptime, endtime;  //队首的人出队（结束）的时间, 队尾的人结束的时间    queue<int> q;};int index = 1;    //第 1 步：n*m 个客户先抢占黄线内, 直接瞬间涌入，不存在选择    for (int i = 1; i <= cap; ++i) {    //看做行和列，先抢第一行        for (int j = 1; j <= n; ++j) {            if(index <= k){                win[j].q.push(time[index]);                if(win[j].endtime >= 540)                    sorry[index] = true;                win[j].endtime += time[index];                if(i == 1)  //对于第一行，出队时间即本队结束时间，做一个初始化                    win[j].poptime = win[j].endtime;                res[index] = win[j].endtime;  //刚进来的，自己是队尾，队伍结束就是自己结束                index++;            }        }    }    //第 2 步：后续的客户    while (index <= k){   //while一次循环表示一个客户的选择        int temp_min = win[1].poptime, temp_win = 1;  //初始化        for (int i = 2; i <= n; ++i) {   //找最早出队的            if(win[i].poptime < temp_min){                temp_win = i;                temp_min = win[i].poptime;            }        }        win[temp_win].q.pop();        win[temp_win].q.push(time[index]);        win[temp_win].poptime += win[temp_win].q.front(); //更新：最前面那个人完成的时间就是出队时间        if(win[temp_win].endtime >= 540)  //endtime 还未更新，是看的前一个人的完成时间，是否超时            sorry[index] = true;        win[temp_win].endtime += time[index];  //更新：刚进来的这个人的完成时间就是本队的完成时间        res[index] = win[temp_win].endtime;        index++;    }</code></pre><hr><h2 id="1016-Phone-Bills（日期类计算）"><a href="#1016-Phone-Bills（日期类计算）" class="headerlink" title="1016 Phone Bills（日期类计算）"></a>1016 Phone Bills（日期类计算）<span id="1016"></span></h2><h3 id="1-题目大意-7"><a href="#1-题目大意-7" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>给定固定格式的日期<code>mm:dd:hh:mm</code>计算各个时间的差值，以及每个时间段有不同的费率，计算总的账单费用。</p><h3 id="2-学习要点-1"><a href="#2-学习要点-1" class="headerlink" title="2. 学习要点"></a>2. 学习要点</h3><p>1.数据结构设计<br>因为输入是给定一个个 call 序列，所以便于存储，也以每个 call 为单位，统一放在一个数组里，按照名字、时间排序，计算的时候，只要考虑 i 和 i-1 前后两个元素是否有同一个 name 以及 status 一个为 0 一个为 1 即可。</p><pre class=" language-lang-c++"><code class="language-lang-c++">struct node {    string name;    int status, month, time, day, hour, minute;};bool cmp(node a, node b) {    return a.name != b.name ? a.name < b.name : a.time < b.time;}</code></pre><p>2.日期差值计算<br>直接”01:05:02:24“-”01:04:23:59“ 难以计算，每个小时还有不同的费率，所以 <strong>统一从本月 0 天 0 点开始计算</strong>，以<code>dd:hh:mm</code>格式再相减即可，隔天的情况则加上这一整天</p><pre class=" language-lang-c++"><code class="language-lang-c++">double billFromZero(node call, int *rate) {    double total;    total = rate[call.hour];  //本小时的费用    total += call.minute + rate[24] * 60 * call.day;  //从本月第 0 天到今天    for (int i = 0; i < call.hour; i++)        total += rate[i] * 60;  //本天内 0 点到现在累加    return total / 100.0;}</code></pre><hr><h2 id="1018-Public-Bike-Management（图最短路径-Dijkstra-DFS）"><a href="#1018-Public-Bike-Management（图最短路径-Dijkstra-DFS）" class="headerlink" title="1018 Public Bike Management（图最短路径 Dijkstra + DFS）"></a>1018 Public Bike Management（图最短路径 Dijkstra + DFS）<span id="1018"></span></h2><p>图例经过 Dijkstra 之后的结果</p><pre class=" language-lang-c++"><code class="language-lang-c++">dis[1] = 1      pre[1] = {0}dis[2] = 1      pre[2] = {0}dis[3] = 2      pre[3] = {1, 2}</code></pre><hr><h2 id="1020-Tree-Traversals-已知后序和中序，转前序-层序"><a href="#1020-Tree-Traversals-已知后序和中序，转前序-层序" class="headerlink" title="1020 Tree Traversals (已知后序和中序，转前序/层序)"></a>1020 Tree Traversals (已知后序和中序，转前序/层序)<span id="1020"></span></h2><h3 id="1-输入格式"><a href="#1-输入格式" class="headerlink" title="1. 输入格式"></a>1. 输入格式</h3><pre><code>72 3 1 5 7 6 41 2 3 4 5 6 7</code></pre><h3 id="2-分析-4"><a href="#2-分析-4" class="headerlink" title="2. 分析"></a>2. 分析</h3><p>和手动模拟差不多，找到根节点 (pre 的第一个，post 的最后一个) -&gt; 找到左右两段的端点，分别作为左右子树 -&gt; 递归重复。关键在于递归的写法，端点的寻找。</p><h3 id="3-个人代码-5"><a href="#3-个人代码-5" class="headerlink" title="3. 个人代码"></a>3. 个人代码</h3><p><code>level</code> 的作用即按照 <code>index</code> 存储，利用 2n+1 2n+2 的公式可以定位左右孩子，而索引的顺序恰好是层序遍历的顺序（具体第几层不知道）<br>转为前序遍历，只需在 pre 递归前打印 root 即可</p><pre class=" language-lang-c++"><code class="language-lang-c++">vector<int> in, post, level(100000, -1);void pre(int root, int start, int end, int index){  //start end 都是相对于中序序列    if(start > end) return;    level[index] = post[root];    int i = start;    while (i < end && in[i] != post[root]) i++;  //找到中序 in 序列中的根节点    int left_size = i-start, right_size = end-i;    pre(root-right_size-1, start, i-1, index*2+1);  //此 root 即为左孩子节点，可以确定为 index*2+1    pre(root-1, i+1, end, index*2+2);    //找到根节点 i，左子树 start ~ i-1, 右子树 i+1 ~ end}pre(n-1, 0, n-1, 0);</code></pre><hr><h2 id="1021-Deepest-Root-求树中最长的路径，DFS，连通分量"><a href="#1021-Deepest-Root-求树中最长的路径，DFS，连通分量" class="headerlink" title="1021 Deepest Root (求树中最长的路径，DFS，连通分量)"></a>1021 Deepest Root (求树中最长的路径，DFS，连通分量)<span id="1021"></span></h2><h3 id="1-题目大意-8"><a href="#1-题目大意-8" class="headerlink" title="1. 题目大意"></a>1. 题目大意</h3><p>给出n个结点（1~n）之间的n条边，问是否能构成一棵树，如果不能构成则输出它有的连通分量个数；如果能构成一棵树，输出能构成最深的树的高度时，树的根结点。如果有多个，按照从小到大输出。</p><h3 id="2-分析-5"><a href="#2-分析-5" class="headerlink" title="2. 分析"></a>2. 分析</h3><p>连通分量的个数：基本套路，一次 DFS 走完一个连通分量，统计循环中 DFS 开启的次数即可<br>本题关键在于，两次 DFS 即可求出所有最远端点（即所有最长路径中的所有端点）如果是一棵树的话，线段无非是两个端点，第一次 DFS 能找到一端，在这些最远端点中（可能有多个同样最远的，可能只有一个最远的）随便选一个作为起点，再来一次 DFS，即可找到另一端的最远端点。</p><h3 id="3-个人代码-6"><a href="#3-个人代码-6" class="headerlink" title="3. 个人代码"></a>3. 个人代码</h3><p><a href="https://pintia.cn/problem-sets/994805342720868352/problems/994805482919673856" target="_blank" rel="noopener">PTA-1003</a></p><pre class=" language-lang-c++"><code class="language-lang-c++">void dfs(int node, int height) {    if(height > maxheight) {        temp.clear();        temp.push_back(node);        maxheight = height;    } else if(height == maxheight){        temp.push_back(node);    }    visit[node] = true;    for(int i = 0; i < v[node].size(); i++) {        if(visit[v[node][i]] == false)            dfs(v[node][i], height + 1);    }</code></pre><h3 id="4-做题-Tips"><a href="#4-做题-Tips" class="headerlink" title="4. 做题 Tips"></a>4. 做题 Tips</h3><ol><li>本题不用考虑权重，所以无需开一个二维数组用邻接矩阵存储<br><code>e[10010][10010]; e[a]=e[b]=weight;</code><br>只需使用邻接表，把和自己直接相连的点存起来即可<br><code>v[node][i]; resize(n+1);</code><br><br></li><li>DFS 基本套路<pre class=" language-lang-c++"><code class="language-lang-c++"> dfs(int node, ...){   //遍历过程中顺带做其它操作     visit[node] = true;     for 与本点直接相连的 i:         if(!visit[i])             dfs(i, ...) }</code></pre></li></ol>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PAT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>欢迎来到 Hexo</title>
      <link href="/2020/02/06/hello-world/"/>
      <url>/2020/02/06/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class=" language-lang-bash"><code class="language-lang-bash">$ hexo new "My New Post"</code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class=" language-lang-bash"><code class="language-lang-bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class=" language-lang-bash"><code class="language-lang-bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class=" language-lang-bash"><code class="language-lang-bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
