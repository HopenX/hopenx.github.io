<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="机器学习基础(一) 概述, HopenX">
    <meta name="description" content="机器学习概述，常见的机器学习算法概览，包括监督学习和非监督学习，损失函数，优化方法，评价指标，参数调优的基本概念">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <title>机器学习基础(一) 概述 | HopenX</title>
    <link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io@master/favicon.png">

    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io@master/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io@master/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io@master/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io@master/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io@master/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io@master/css/highlight/styles/github.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io@master/css/matery.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io@master/css/my.css">
    
    <script src="https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io@master/libs/jquery/jquery.min.js"></script>
    
<meta name="generator" content="Hexo 4.2.0"></head>



<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    <div>
                        
                        <img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io@master/medias/logo.png" class="logo-img" alt="LOGO">
                        
                        <span class="logo-span">HopenX</span>
                    </div>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>

<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io@master/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">HopenX</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/HopenX/hopenx.github.io" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>

        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #267871;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/HopenX/hopenx.github.io" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>



    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io@master/medias/featureimages/11.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">机器学习基础(一) 概述</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io@master/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        height: calc(100vh - 250px);
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #267871;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;
    }

    #toc-content .is-active-link {
        color: #267871;
    }

    #toc-content .is-active-link::before {
        background-color: #267871;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        
<style type="text/css">

    #articleContent a {
        color: #267871; !important;
    }

    #artDetail .post-cate a {
        color: #267871; !important;
    }

    blockquote {
        border-left: 5px solid #267871; !important;
    }

    #artDetail .reprint a {
        color: #267871; !important;
    }

    pre {
        background: #f8f8f8;
    }

    .code-area::after {
        content: " ";
        position: absolute;
        border-radius: 50%;
        /*background: #ff5f56;*/
        width: 12px;
        height: 12px;
        top: 0;
        left: 12px;
        margin-top: 12px;
        /*-webkit-box-shadow: 20px 0 #ffbd2e, 40px 0 #27c93f;*/
        /*box-shadow: 20px 0 #ffbd2e, 40px 0 #27c93f;*/
    }

    @font-face {
        font-family: 'Menlo';
        font-style: normal;
        font-weight: 400;
        src: url('https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io@master/css/Menlo-Regular.ttf') format('ttf');
    }

    code{
        font-family: 'Menlo';
    }


</style>


<script src="https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io@master/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">
                                <span class="chip bg-color">机器学习</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="post-category">
                                机器学习
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2020-02-06
                </div>
                

                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.2k
                </div>
                

                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>

        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="机器学习概述"><a href="#机器学习概述" class="headerlink" title="机器学习概述"></a>机器学习概述</h1><h2 id="机器学习分类"><a href="#机器学习分类" class="headerlink" title="机器学习分类"></a>机器学习分类</h2><ol>
<li>监督学习：已经有数据，和数据对应的标签。</li>
<li>非监督学习：给定的样本无需输出/标签，让机器自己学习样本中隐含的内部结构。</li>
<li>半监督学习：二者结合。</li>
<li>强化学习：通过打分/评价的形式，类似于监督学习中的标签。</li>
</ol>
<h2 id="机器学习模型"><a href="#机器学习模型" class="headerlink" title="机器学习模型"></a>机器学习模型</h2><p>机器学习 = 数据 data + 模型 model + 优化方法 optimal strategy</p>
<h2 id="偏差-方差权衡"><a href="#偏差-方差权衡" class="headerlink" title="偏差/方差权衡"></a>偏差/方差权衡</h2><p>variance 和 bias，分别对应过拟合和欠拟合</p>
<p>来自 Wikipedia：</p>
<blockquote>
<p>在监督学习中，如果能将模型的方差与误差权衡好，那么可以认为该模型的泛化性能（对于新数据）将会表现出好的结果。</p>
<p>偏差刻画的是算法本身的性能。高偏差将会造成欠拟合(Underfitting) [miss the relevant relations between features and target outputs]。换句话说，模型越复杂偏差就越小；而模型越简单，偏差就越大。</p>
<p>方差用来衡量因训练集数据波动(fluctuations)而造成的误差影响。高方差将会造成过拟合(Overfitting)。</p>
</blockquote>
<p>在周志华老师&lt;机器学习&gt;书中是这样阐述的：</p>
<blockquote>
<p><em>偏差</em> 度量了学习算法的期望预测与真实结果的偏离程度，即刻画了算法本身的拟合能力；</p>
<p><em>方差</em> 度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响(在不同数据集上表现很不稳定);</p>
<p><em>噪声</em> 则表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题的本身难度</p>
<p>偏差-方差分解说明，泛化性能是由学习算法的能力、数据的充分性以及学习任务本身的难度所共同决定的。给定的学习任务，为了取得好的泛化性能，则需使偏差较小，即能够充分拟合数据，并且使方差较小，即使数据扰动产生的影响小。一般来说方差与偏差是有冲突的，这称为方差-偏差窘境。</p>
</blockquote>
<h2 id="常见机器学习算法概览"><a href="#常见机器学习算法概览" class="headerlink" title="常见机器学习算法概览"></a>常见机器学习算法概览</h2><h3 id="1-Linear-Algorithm-线性算法"><a href="#1-Linear-Algorithm-线性算法" class="headerlink" title="1. Linear Algorithm 线性算法"></a>1. Linear Algorithm 线性算法</h3><ol>
<li><p><strong>Linear Regression 线性回归</strong>：使用最小二乘法 Least Squares 拟合一条直线 → 计算 R<sup>2</sup> → 计算 R<sup>2</sup> 的 p 值。R<sup>2</sup> 表示 x 能多大程度反映 y 的变化，p 值表示可靠程度。拟合直线的过程使用「随机梯度下降」（SGD）</p>
</li>
<li><p><strong>Lasso 回归 和 Ridge 回归</strong>：都可以减少共线性带来的影响，即 X 自变量之间有相互关联。区别可以归结为L2和L1正则化的性质差异。</p>
</li>
<li><p><strong>Polynomial Regression 多项式回归</strong>：能够模拟非线性可分的数据（曲线），线性回归不能做到这一点。但容易过拟合。</p>
</li>
<li><p><strong>Logistic Regression 逻辑回归</strong>：判断 True or False，Y 值为 0-1 表示概率，用于分类。线性回归使用「Residual 偏差」，而逻辑回归使用「maximum likelihood 最大似然」</p>
</li>
</ol>
<h4 id="逻辑回归回顾"><a href="#逻辑回归回顾" class="headerlink" title="逻辑回归回顾"></a>逻辑回归回顾</h4><ul>
<li>逻辑回归是一种二分类模型, 给出的是划分为 (0, 1) 的概率. 通过$\theta^Tx$通过一个 sigmoid 函数映射到 0-1 范围内. sigmoid 将一个得分转化为概率输出. 设置一个阈值$h$, 大于$h$则判断为 1.</li>
</ul>
<script type="math/tex; mode=display">
\begin{array}{l}
p_{\theta}(y=1 | x)=\sigma\left(\theta^{T} x\right)=\frac{1}{1+e^{-\theta^{T} x}} \\
p_{\theta}(y=0 | x)=\frac{e^{-\theta^{T} x}}{1+e^{-\theta^{T} x}}
\end{array}</script><ul>
<li>均方误差 MSE<script type="math/tex; mode=display">
M S E=\frac{1}{n} \sum_{i}^{n}\left(\hat{y}_{i}-y_{i}\right)^{2}</script>标准差 RMSE <script type="math/tex">RMSE=\sqrt{MSE}</script></li>
</ul>
<blockquote>
<p>使用MSE的一个缺点就是其偏导值在输出概率值接近0或者接近1的时候非常小，这可能会造成模型刚开始训练时，偏导值几乎消失. 这导致模型在一开始学习的时候速率非常慢，而使用交叉熵作为损失函数则不会导致这样的情况发生.</p>
</blockquote>
<ul>
<li>交叉熵损失函数<br>该函数是<code>凸函数</code>，求导时能够得到全局最优值. (记得前面有一个负号)<br>多分类情况下:<script type="math/tex; mode=display">
L=\frac{1}{N} \sum_{i} L_{i}=\frac{1}{N} \sum_{i}-\sum_{c=1}^{M} y_{i c} \log \left(p_{i c}\right)</script>在二分的情况下，模型最后需要预测的结果只有两种情况，对于每个类别我们的预测得到的概率为 p 和 1-p. 此时表达式为：<script type="math/tex; mode=display">
L=\frac{1}{N} \sum_{i} L_{i}=\frac{1}{N} \sum_{i}-\left[y_{i} \cdot \log \left(p_{i}\right)+\left(1-y_{i}\right) \cdot \log \left(1-p_{i}\right)\right]</script>其中：<br>$y_i$ —— 表示样本i的label，正类为1，负类为0<br>$p_i$ —— 表示样本i预测为正的概率<br><code>pi</code> 就是 sigmoid 函数输出的结果, 也可以写作为:<script type="math/tex; mode=display">
\mathcal{L}\left(y, x, p_{\theta}\right)=-y \log \sigma\left(\theta^{T} x\right)-(1-y) \log \left(1-\sigma\left(\theta^{T} x\right)\right)</script>对应的梯度函数(导数):<script type="math/tex; mode=display">
\frac{\partial \mathcal{L}\left(y, x, p_{\theta}\right)}{\partial \theta}=(\sigma(\theta^{T} x)-y) x</script>给定一个学习率 $\eta$, 步长 = 梯度 x 学习率, 那么 $\theta$ 学习/更新的过程就是:<script type="math/tex; mode=display">
\theta \leftarrow \theta+\eta\left(y-\sigma\left(\theta^{T} x\right)\right) x</script></li>
<li>逻辑回归打分函数:<script type="math/tex; mode=display">s(x)=\theta_0+\theta_1x_1+\theta_2x_2</script>也就是$\theta^{T} x$ ,也就是 $b+w^Tx$ . 在直线的上方, 函数值就会大于 0, sigmoid 的函数值就会越接近 1, 分类归为 1. 打分越高, 越远离决策边界, 具有更高的分类<code>置信度</code>.</li>
</ul>
<h3 id="2-Decision-Tree-决策树"><a href="#2-Decision-Tree-决策树" class="headerlink" title="2. Decision Tree 决策树"></a>2. Decision Tree 决策树</h3><ol>
<li><p><strong>ID3</strong>: 计算「信息熵」 $Entropy(D)$，值越小，说明样本集合D的纯度就越高，进而选择用样本的某一个属性a来划分样本集合D时，就可以得出用属性a对样本D进行划分所带来的「信息增益」 $Gain(D, a)$，值越大，说明如果用属性a来划分样本集合D，那么纯度会提升。 <script type="math/tex">Entropy(t)=-\sum_{k} p\left(c_{k} | t\right) \log p\left(c_{k} | t\right)</script>  <script type="math/tex">Classificationerror (t)=1-\max _{k}\left[p\left(c_{k} | t\right)\right]</script></p>
</li>
<li><p><strong>C4.5</strong>: 提出Gainratio 「增益率」，解决ID3决策树的一个缺点，当一个属性的可取值数目较多时，那么可能在这个属性对应的可取值下的样本只有一个或者是很少个，那么这个时候它的信息增益是非常高的，这个时候纯度很高，ID3决策树会认为这个属性很适合划分，但是较多取值的属性来进行划分带来的问题是它的泛化能力比较弱。用 $I(·)$ 表示不纯度——可以是熵可以是基尼，信息增益：<script type="math/tex">\Delta=I(\text { parent })-\sum_{i=1}^{n} \frac{N\left(a_{i}\right)}{N} I\left(a_{i}\right)</script>信息增益率：<script type="math/tex">Gainratio =\frac{\Delta}{Entropy({parent})}</script></p>
</li>
<li><p><strong>CART(Classification and Regression Tree)</strong>: 通过计算 Gini 基尼系数（尽可能小），判断 impurity 不纯洁度。离散数据用「是否」划分子树，连续数据可以用「两两之间平均值」划分子树。<script type="math/tex">{Gini}(t)=1-\sum_{k}\left[p\left(c_{k} | t\right)\right]^{2}</script>D 分裂为 DL 和 DR，分裂后的信息增益<script type="math/tex">Gain(D, A)=\frac{\left|D_{L}\right|}{|D|} \operatorname{Gini}\left(D_{L}\right)+\frac{\left|D_{R}\right|}{|D|} \operatorname{Gini}\left(D_{R}\right)</script></p>
</li>
</ol>
<h3 id="3-SVM-支持向量机"><a href="#3-SVM-支持向量机" class="headerlink" title="3. SVM 支持向量机"></a>3. SVM 支持向量机</h3><h4 id="找到最小间隔"><a href="#找到最小间隔" class="headerlink" title="找到最小间隔"></a>找到最小间隔</h4><ul>
<li>$y_i$ 取 -1 或 1</li>
<li>思想: 找到离 decision boundry 最近的点, 找到 w 和 b, 让最小的 margin 获得比较大的值. <code>最大化最小的几何间隔</code></li>
<li>点到直线的距离:<script type="math/tex; mode=display">
d=\frac{\left|A x_{0}+B y_{0}+C\right|}{\sqrt{A^{2}+B^{2}}}</script>即<script type="math/tex; mode=display">
d=\frac{\left|(A, B)\left(\begin{array}{c}
x 0 \\
y 0
\end{array}\right)+C\right|}{\|(A, B)\|}</script>如果数据是 n 维时，此时直线就变成了平面，用 $w$ 表示平面的法向量得：<script type="math/tex; mode=display">
d=\frac{|w \cdot x+b|}{\|w\|}</script></li>
<li><strong>函数间隔</strong>:<br>由于分母${|w|}$都是相同的, 我们只需比较分子, 于是有了函数间隔<script type="math/tex; mode=display">
\hat{\gamma}^{(i)}=y^{(i)}\left(w^{T} x^{(i)}+b\right).</script>乘以 $y^{(i)}$ 是为了<code>取绝对值</code>, 非 -1 即 1</li>
<li><p><strong>几何间隔</strong>:<br>w 和 b 的赋值可以同时改变, 比如同时$*2$, $\hat{y}^{(i)}$ 值会变大, 但是分割超平面不会变化 (因为=0的解是一样的)<br>所以需要对法向量加某些约束, 我们可以除以一个||w||, 作为 normalizaiton<code>归一化</code>, 使得<code>函数间隔 -&gt; 几何间隔</code>. 即对 $w$ 使用<code>L2范数</code>进行规范化，使得间隔是确定的.</p>
<script type="math/tex; mode=display">
\hat{\gamma}^{(i)}=y^{(i)}\left[\left(\frac{w}{\|w\|}\right)^{T} x^{(i)}+\frac{b}{\|w\|}\right]</script><p>给定一个训练集, 就可以依次计算 $\hat{\gamma}^{(i)}$, 选出最小的几何间隔 ${\gamma}$, 作为整个数据集到超平面的几何间隔. 满足这个几何间隔的点, 就是<code>支撑向量</code>.</p>
</li>
<li><p><strong>优化目标</strong>:<br>对于几何间隔</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\max _{\gamma, \omega, b} \gamma\\
&\text { s.t. } \quad y^{(i)}\left(w^{T} x^{(i)}+b\right) \geq \gamma, \quad i=1, \dots, m\\
\end{aligned}</script><p>限定的条件是 $||w||=1$, 也就是 w 在一个环上, 不是一个凸集, 所以是一个<code>非凸约束</code>.<br>等同于优化一个<code>非凸的目标函数</code>, 这种情况下,  $||w||$ 作为分母, 仍然是一个非凸函数.(如果是分子, 就是凸函数)</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\max _{\gamma, w, b} {\frac{\hat{\gamma}}{\|w\|}}\\
&\text { s.t. } \quad y^{(i)}\left(w^{T} x^{(i)}+b\right) \geq \hat{\gamma}, \quad i=1, \ldots, m
\end{aligned}</script></li>
<li><p>调整后的目标函数:<br>由于分类间隔的变化, 即 w, b 的放缩不会影响决策边界, 将函数间隔<code>固定为 1</code>, 即$\hat{\gamma}=1$, 目标函数重写为 (二次规划)</p>
<script type="math/tex; mode=display">
\max _{w, b} \frac{1}{\|w\|}</script><p>等同于</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\min _{w, b} \frac{1}{2}{\|w\|}^2\\
&\text { s.t. } \quad y^{(i)}\left(w^{T} x^{(i)}+b\right) \geq \hat{\gamma}, \quad i=1, \ldots, m
\end{aligned}</script><blockquote>
<p>用直线画出来的区域/函数一定是一个凸集</p>
</blockquote>
</li>
</ul>
<h4 id="拉格朗日乘子法"><a href="#拉格朗日乘子法" class="headerlink" title="拉格朗日乘子法"></a>拉格朗日乘子法</h4><p>对于凸优化问题</p>
<script type="math/tex; mode=display">
\begin{array}{ll}
\min _{w} & f(w) \\
\text { s.t. } & h_{i}(w)=0, \quad i=1, \dots, l
\end{array}</script><p>拉格朗日函数定义为</p>
<script type="math/tex; mode=display">
\mathcal{L}(w, \beta)=f(w)+\sum_{i=1}^{l} \beta_{i} h_{i}(w)</script><blockquote>
<p>其中 $\beta_{i}$ 称之为拉格朗日乘子</p>
</blockquote>
<p>问题的解, 对 $w$ 和 $\beta$ 分别求导 =0 即可</p>
<script type="math/tex; mode=display">
\frac{\partial \mathcal{L}(w, \beta)}{\partial w}=0 \quad \frac{\partial \mathcal{L}(w, \beta)}{\partial \beta}=0</script><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/27/WFAhMKzVsPD7xZJ.png" alt="拉格朗日法-原理解析"><br>解求得的点, 就是 $h(w)=0$ 与等高线的一个切点, 在其他任何地方, $f(w)$ 都会更大. 在这里, $f(w)$ 即为 $ \frac{1}{2}{|w|}^2$</p>
<h4 id="通用凸优化问题"><a href="#通用凸优化问题" class="headerlink" title="通用凸优化问题"></a>通用凸优化问题</h4><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/28/97GTvousRYj8S1y.png" alt="凸优化问题"><br>满足约束条件的情况下, 后面两个式子一定 $=0$, 当约束条件不满足条件下, 一定能取 $\alpha_{i}$ 和 $\beta_{i}$, 最终拉格朗日函数 $=+\infty$<br>最后原始问题即为:</p>
<script type="math/tex; mode=display">
\theta_{\mathcal{P}}(w)=\left\{\begin{array}{ll}
f(w) & \text w 满足原始问题约束 \\
+\infty & 其他
\end{array}\right.</script><p>函数最小化问题</p>
<script type="math/tex; mode=display">
\min _{w} {\theta}_{\mathcal{P}}(w)=\min _{w} \max _{\alpha, \beta: \alpha_{i} \geq 0} \mathcal{L}(w, \alpha, \beta)</script><p>等同于原来的优化任务</p>
<script type="math/tex; mode=display">
\begin{array}{ll}
\min _{w} & f(w) \\
\text { s.t. } & g_{i}(w) \leq 0, \quad i=1, \ldots, k \\
& h_{i}(w)=0, \quad i=1, \ldots, l
\end{array}</script><p>原始问题的解即为</p>
<script type="math/tex; mode=display">
p^\star=\min _{w} \theta_{\mathcal{P}}(w)</script><p>求解 $f(w)$ 之所以添加一个 $\theta_{\mathcal{P}}(w)$, 就是为了省略掉 $g_{i}(w)$ 和 $h_{i}(w)=0$ 这些限制条件, 最后简洁地表示成一个函数.</p>
<h4 id="对偶问题-amp-KKT"><a href="#对偶问题-amp-KKT" class="headerlink" title="对偶问题 &amp; KKT"></a>对偶问题 &amp; KKT</h4><p>将研究的参数转化为 $\alpha$ 和 $\beta$</p>
<script type="math/tex; mode=display">
\max _{\alpha, \beta: \alpha_{i} \geq 0} \theta_{\mathcal{D}}(\alpha, \beta)=\max _{\alpha, \beta: \alpha_{i} \geq 0} \min _{w} \mathcal{L}(w, \alpha, \beta)</script><p>对偶问题的解为</p>
<script type="math/tex; mode=display">
d^\star=\max _{\alpha, \beta: \alpha_{i} \geq 0} \min _{w} \mathcal{L}(w, \alpha, \beta)</script><p>两者之间的关系 (恒成立)</p>
<script type="math/tex; mode=display">
d^\star=\max _{\alpha, \beta: \alpha_{i} \geq 0} \min _{w} \mathcal{L}(w, \alpha, \beta) \leq \min _{w} \max _{\alpha, \beta: \alpha_{i} \geq 0} \mathcal{L}(w, \alpha, \beta)=p^\star</script><p>满足一定条件, 可以使 $d^ \star =p^ \star $, 即 <code>KKT 条件</code><br>前提:</p>
<ul>
<li>假设 $f$ 以及 $g_i$ 是凸函数,并且 $h_i$ 为仿射函数,且 $g_i$ 严格满足可行域</li>
<li>必然存在 $(w^\star ,\alpha^\star ,\beta^\star )$, 满足: $w_\star $ 是原始问题的解, $\alpha^\star $ , $\beta^\star $ 是对偶问题的解, 且两个问题的解数值相等$d^ \star =p^ \star $</li>
<li>同时, $(w^\star ,\alpha^\star ,\beta^\star )$ 满足KKT条件<script type="math/tex; mode=display">
\begin{aligned}
&\frac{\partial}{\partial w_{i}} \mathcal{L}\left(w^{\star}, \alpha^{\star}, \beta^{\star}\right)=0, i=1, \dots, n\\
&\frac{\partial}{\partial \beta_{i}} \mathcal{L}\left(w^{\star}, \alpha^{\star}, \beta^{\star}\right)=0, i=1, \ldots, l\\
&\alpha_{i}^{\star} g_{i}\left(w^{\star}\right)=0, i=1, \dots, k\\
&g_{i}\left(w^{\star}\right) \leq 0, i=1, \dots, k\\
&{\alpha}^{\star} \geq 0, i=1, \ldots, k
\end{aligned}</script></li>
</ul>
<p>对 $w$ 和 对 $\beta_i$ 导数均为 0, 才能让他俩的梯度共线, 从而找到切点.<br>其中 $\alpha_{i}^ \star  g_{i}\left(w^ \star \right)=0, i=1, \dots, k$ 称为<code>KKT对偶互补条件</code><br>因为原始的约束中, $g_{i}\left(w^ \star \right)\leq0$, 是有可能 $&lt;0$ 的, 这个时候 $\alpha_i$ 一定要 $=0$; 当 $\alpha_i\ne0$,  这个时候 $g_{i}\left(w^ \star \right)$ 一定要 $=0$</p>
<h4 id="SVM-求解推导"><a href="#SVM-求解推导" class="headerlink" title="SVM 求解推导"></a>SVM 求解推导</h4><ul>
<li><strong>第一步: 利用拉格朗日对偶, 确定目标函数</strong></li>
<li>目标函数: <code>寻找最优间隔分类器</code><br>由于已经固定 $\hat{\gamma}=1$, 也就意味着到超平面的距离为1的点都是支持向量<br><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/28/kOjBcHx6t8zZFNa.png" alt="支持向量"><br>代入 $\hat{\gamma}=1$, 问题变成<script type="math/tex; mode=display">
\begin{aligned}
&\min _{w, b} \frac{1}{2}{\|w\|}^2\\
&\text { s.t. } \quad y^{(i)}\left(w^{T} x^{(i)}+b\right) \geq 1, \quad i=1, \ldots, m
\end{aligned}</script>构造拉格朗日函数<script type="math/tex; mode=display">
L(w, b, \alpha)=\frac{1}{2}\|w\|^2-\sum_{i=1}^{n} \alpha_{i}\left(y_{i}\left(w \cdot x_{i}+b\right)-1\right), \alpha_{i} \geq 0</script></li>
<li>$\alpha&gt;0$ 成立的条件, 仅当 $x_i$ 卡在支持向量上, 即 $g_{i}\left(w^\star\right)=0$ 的时候, 也即函数间隔 $\hat{\gamma}=1$ 的时候. 其他点都没有影响, 对应的 $\alpha_i=0$. 这也说明, 分割平面 $w$ 的确定, 只取决于全局函数间隔最小的这几个点的位置.</li>
<li>只要满足 KKT 条件, 求解对偶问题, 相当于间接地求解了原问题<br>现在需要最大化 $ L(w,b,\alpha )$，所以问题变成 <script type="math/tex; mode=display">
\min _{w, b} \max _{\alpha} L(w, b, \alpha)</script>根据拉格朗日对偶性, 问题变成<script type="math/tex; mode=display">
\max _{\alpha} \min _{w, b} L(w, b, \alpha)</script></li>
<li><p><strong>第二步: 求  $ \mathop {\min }\limits_{w,b} L(w,b,\alpha )$</strong><br>让  $L(w,b,\alpha)$  分别对 $w,b$ 求偏导，使导数为 0：<br>$ \frac{ {\partial L(w,b,\alpha )} }{ {\partial w} } = w - \sum\limits_{i = 1}^n { {\alpha _i}{y_i}{x_i} } = 0$  ————&gt;  $ w = \sum\limits_{i = 1}^n { {\alpha _i}{y_i}{x_i} }$<br>$ \frac{ {\partial L(w,b,\alpha )} }{ {\partial b} } = \sum\limits_{i = 1}^n { {\alpha _i}{y_i} } = 0$ ————————-&gt; $\sum\limits_{i = 1}^n { {\alpha _i} {y_i} } = 0 $<br>代入原式$L(w, b, \alpha)$可得</p>
<script type="math/tex; mode=display">
\min _{w, b} L(w, b, \alpha)=-\frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} x_{j}\right)+\sum_{i=1}^{n} \alpha_{i}</script></li>
<li><p><strong>第三步:  求 $ \mathop {\min }\limits_{w,b} L(w,b,\alpha )$ 对 $ \alpha $ 的极大值 - SMO</strong><br>问题变为： </p>
<script type="math/tex; mode=display">\mathop {\max }\limits_\alpha \ \ - \frac{1}{2}\sum\limits_{i = 1}^n {\sum\limits_{j = 1}^n { {\alpha _i}{\alpha _j}{y_i}{y_j}({x_i}{x_j})} } + \sum\limits_{i = 1}^n { {\alpha _i}}</script><p>约束条件为： <script type="math/tex">\sum\limits_{i = 1}^n { {\alpha _i}{y_i}} = 0，  {\alpha _i} \ge 0,i = 1,2...n</script><br>所以只要知道 $ {\alpha }$， 也就知道了$w$. 我们使用 SMO 算法, <code>假设已经求出</code>最优解为 $ {\alpha^ \star  } = {(\alpha _1^ \star  ,\alpha _2^ \star  ….\alpha _n^ \star  )^T}$</p>
</li>
<li><p><strong>第四步:  求 ${w^ \star }$ 和 $b^ \star $ 得到超平面</strong><br>已知最优的 ${\alpha ^ \star }$, 由第一步 $w=\sum\limits_{i = 1}^n {\alpha _i}{y_i}{x_i}$ 可求出最优的 ${w^ \star  } $ </p>
<script type="math/tex; mode=display">{w^\star } = \sum\limits_{i = 1}^n {\alpha _i^ \star  {y_i}{x_i}}</script><p>由 KKT 条件中的<code>对偶互补条件</code>, 至少存在一个 $ \alpha _i^ \star &gt;0 $ 使得 $\alpha_{i}^ \star  g_{i}\left(w^ \star \right)=0$, 即</p>
<script type="math/tex; mode=display">
\alpha_{i}^ \star \left(y_{i}\left(w^ \star  \cdot x_{i}+b^ \star \right)-1\right)=0, i=1,2 \ldots n</script><p>可以证明至少存在一个 $\alpha _j^  \star &gt;0$ 使等式成立，所以可得：</p>
<script type="math/tex; mode=display">{y_j}({w^ \star }\cdot {x_j} + {b^ \star }) - 1 = 0</script><p>因为 $ {y_i} = \{ + 1, - 1\}$ ，所以 $\frac{1}{ { {y_i}}} = {y_i}$ 进而得到</p>
<script type="math/tex; mode=display">
b^ \star =y_{j}-\sum_{i=1}^{n} \alpha_{i}^ \star  y_{i}\left(x_{i} \cdot x_{j}\right)</script><p>至此 $ w^ \star  和  b^ \star  $ 都已经得到，所以可以得到<code>超平面</code>：$ {w^ \star }\cdot x + {b^ \star } = 0$</p>
</li>
</ul>
<h4 id="SMO-序列最小优化"><a href="#SMO-序列最小优化" class="headerlink" title="SMO 序列最小优化"></a>SMO 序列最小优化</h4><ul>
<li>上面我们只是假设已经求得了 $ {\alpha ^ \star } = {(\alpha _1^ \star ,\alpha _2^ \star ….\alpha _n^ \star )^T}$，用到的方法是<code>SMO（Sequential Minimal Optimization）即序列最小优化</code>，感觉SMO其实不属于支持向量机算法的一部分.</li>
<li>SMO的思想<br>拉格朗日乘子 $\alpha$ 有许多个，如果采用单个 $\alpha$ 坐标上升法, 无法满足 $\sum_{i=1}^{m} \alpha_{i} y^{(i)}=0$ 的条件.<br>于是, 每次只选择其中<code>两个</code>变量作为未知量，其他乘子作为已知量，将N个问题转变成两个变量的求解问题. 当 $W(\alpha)$ 的变化小于一个阈值, 比如 0.01, 可以判定为收敛. 比如对于线性可分支持向量机的目标函数：<script type="math/tex; mode=display">\mathop {\max }\limits_\alpha \ \ - \frac{1}{2}\sum\limits_{i = 1}^n {\sum\limits_{j = 1}^n { {\alpha _i}{\alpha _j}{y_i}{y_j}({x_i}{x_j})} } + \sum\limits_{i = 1}^n { {\alpha _i}}</script>约束条件为： <script type="math/tex">\sum\limits_{i = 1}^n { {\alpha _i}{y_i}} = 0，  {\alpha _i} \ge 0,i = 1,2...n</script><br>一开始随机选取两个变量 $ {\alpha _1},{\alpha _2} $作为未知量，其他因子 ${\alpha _i}(i = 3,4…n)$ 当作已知来求目标函数的最小值，求出 ${\alpha _1},{\alpha _2}$ 再带入进去，再选取 $ {\alpha _3},{\alpha _4}$ 求目标函数最小值，求出后继续带入，直到目标函数收敛.<br>由于每一对 $(\alpha_i, \alpha_j)$ 都互相可以表示出来, 最终转化为以单个变量 $\alpha_j$ 的二次函数优化问题, 用二次函数即可求解.<script type="math/tex; mode=display">
\begin{array}{c}
\max _{\alpha_{2}} W\left(\alpha_{2}\right)=a \alpha_{2}^{2}+b \alpha_{2}+c \\
\text { s.t. } 0 \leq \alpha_{2} \leq C
\end{array}</script></li>
</ul>
<h4 id="核函数方法-Kernel-Function"><a href="#核函数方法-Kernel-Function" class="headerlink" title="核函数方法 Kernel Function"></a>核函数方法 Kernel Function</h4><ul>
<li>处理线性不可分的情况, 增加一个<code>松弛变量</code> $\xi$, 允许函数间隔小于 1, 甚至小于 0. <script type="math/tex; mode=display">
\begin{array}{l}
\min _{w, b, \xi} \frac{1}{2}\|w\|^{2}+c \sum_{i=1}^{m} \xi_{i} \\
\\
\text { s.t. } y^{(i)}\left(w^{T} x^{(i)}+b\right) \geq 1-\xi_{i}, \quad i=1, \ldots, m \\
\quad \xi_{i} \geq 0, \quad i=1, \ldots, m
\end{array}</script></li>
<li>拉格朗日函数<script type="math/tex; mode=display">
\mathcal{L}(w, b, \xi, \alpha, r)=\frac{1}{2} w^{T} w+C \sum_{i=1}^{m} \xi_{i}-\sum_{i=1}^{m} \alpha_{i}\left[y^{(i)}\left(x^{T} w+b\right)-1+\xi_{i}\right]-\sum_{i=1}^{m} r_{i} \xi_{i}</script></li>
<li>对偶问题: 此时 $\alpha$ 不再是 &gt;= 0, 而是 $0 \leq \alpha \leq C$<script type="math/tex; mode=display">
\begin{aligned}
&\max _{\alpha} W(\alpha)=\sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i, j=1}^{m} y^{(i)} y^{(j)} \alpha_{i} \alpha_{j} x^{(i)^{T}} x^{(j)}\\
&\text { s.t. } 0 \leq \alpha_{i} \leq C, \quad i=1, \ldots, m\\
&\sum_{i=1}^{m} \alpha_{i} y^{(i)}=0
\end{aligned}</script></li>
<li>基本思想: 将原来的 x 空间中的特征, 映射到 $\phi(x)$ 高维空间中. 原本只着眼于内积运算 $x^{(i)^{T}} x^{(j)}$, 给定 $\phi(x)$, 变成<code>核函数</code>运算 <script type="math/tex; mode=display">K\left(x^{(i)}, x^{(j)}\right)=\phi\left(x^{(i)}\right)^{T} \phi\left(x^{(j)}\right)</script>目标函数变成<script type="math/tex; mode=display">
W(\alpha)=\sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i, j=1}^{m} y^{(i)} y^{(j)} \alpha_{i} \alpha_{j} K\left(x^{(i)}, x^{(j)}\right)</script></li>
<li><code>核技巧</code> Kernel Trick<br>多数情况下, 可以直接定义  $K\left(x^{(i)}, x^{(j)}\right)$ 而不需要 $\phi(x)$, 比如直接假定 $K\left(x^{(i)}, x^{(j)}\right)=\left(x^{(i)^{T}} x^{(j)}\right)^{2}$<br>时间复杂度可以明显降低, 比如从 $O(n^2)$ 降低到  $O(n)$</li>
<li>给定核函数, 再通过 SMO 即可求得 $\alpha$, 再通过 $\alpha$ 求得 $w$ 和 $b$, $\phi(x)$ 的计算被约去, 得到函数间隔/目标函数为<script type="math/tex; mode=display">
\begin{aligned}
w^{ \star  T} x+b^ \star  &=\left(\sum_{i=1}^{m} \alpha_{i} y^{(i)} \phi\left(x^{(i)}\right)\right)^{T} \phi(x)+b^ \star  \\
&=\sum_{i=1}^{m} \alpha_{i} y^{(i)} K\left(x^{(i)}, x\right)+b^ \star 
\end{aligned}</script></li>
<li>高斯核函数 - 非常常用<br>对于 x 和 z 两个样例, 我们需要刻画其<code>相似性度量</code><script type="math/tex; mode=display">
K(x, z)=\exp \left(-\frac{\|{x}-z \|^{2}}{2 \sigma^{2}}\right)</script>也称为<code>径向基函数</code>(RBF)核. 将其中一个 x 设为高斯分布的中心点, 计算 z 的概率密度</li>
<li>核矩阵K<script type="math/tex; mode=display">
\left\{K_{i, j}\right\}_{i, j=1, \ldots, m}</script></li>
<li>有效核: 一定能写成两个 $\phi(x)$ 函数相乘, 一定是对称矩阵, 半正定矩阵 —&gt; 对于任何向量 z, $z^TKz\geq0$. 常见的有 RBF 核, 多项式核$K(x, z)=\left(x^{T} z\right)^{d}$, 余弦相似度核$K(x, z)=\frac{x^{T} z}{|x| \cdot|z|}$, sigmoid 核等. 使用 Sigmoid核的 SVM, 相似于于一个二层的感知机.</li>
<li>核函数, 同样可以映射到<code>广义的线性模型</code>, 只要涉及到映射 $\phi(x)$, 就可以通过 Kernel Trick 免去计算 ${\Phi}^{T} {\Phi}$, 而直接计算核矩阵<br><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/29/gyzILRlJv1SjseU.png" alt="广义线性模型的核技巧"></li>
</ul>
<h4 id="SVM-总结"><a href="#SVM-总结" class="headerlink" title="SVM 总结"></a>SVM 总结</h4><ul>
<li><p>SVM 独到的特点<br>不需要去具体计算 $w$ 和 $b$ 的值是多少, 新进来需要预测的点, 只需要计算<code>和已知的支持向量点的内积</code>即可, 即只需要计算两个点之间的相似度, 而不需要去具体计算支持向量是什么.</p>
</li>
<li><p>SVM 的损失函数<br>使用铰链损失 <code>Hinge Loss</code>, 只关注函数间隔 &lt;=1 的情况, &gt;1 的情况的 loss 忽略为 0. 而逻辑回归即使计算所得的 p 再高, 也依然要计算 loss.<br><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/28/OL46fhGWiAIQPgH.png" alt="损失函数对比"></p>
</li>
<li><p>SVM 分类</p>
</li>
</ul>
<ol>
<li>硬间隔支持向量机（线性可分支持向量机）：当训练数据线性可分时，可通过硬间隔最大化学得一个线性可分支持向量机</li>
<li>软间隔支持向量机：当训练数据近似线性可分时，可通过软间隔最大化得到一个线性支持向量机</li>
<li>非线性支持向量机：当训练数据线性不可分时，可通过核方法以及软间隔最大化得一个非线性支持向量机</li>
</ol>
<ul>
<li><p>优点：<br>SVM在中小量样本规模的时候容易得到数据和特征之间的非线性关系，可以避免使用神经网络结构选择和局部极小值问题，可解释性强，可以解决高维问题</p>
</li>
<li><p>缺点：<br>SVM对缺失数据敏感，对非线性问题没有通用的解决方案，核函数的正确选择不容易，计算复杂度高，主流的算法可以达到O(n2)O(n2)的复杂度，这对大规模的数据是吃不消的</p>
</li>
<li><p><a href="https://www.kaggle.com/zhpan92/boyu-svm" target="_blank" rel="noopener">代码实践👆(SMO 等具体细节)</a></p>
</li>
</ul>
<p>调用 scikit-learn 为例, 使用 rbf 核函数</p>
<pre><code class="lang-py">from sklearn import svm
# x, y = simple_synthetic_data(50, 5, 5)
x, y = spiral_data()

model = svm.SVC(kernel=&#39;rbf&#39;, gamma=50, tol=1e-6)
model.fit(x, y)

fig = plt.figure(figsize=(6,6))
ax = fig.add_subplot(111)
plot(ax, model.predict, x, &#39;SVM + RBF&#39;)
plt.show()
</code></pre>
<p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/29/KcCRFjglUkh3Gos.png" alt=""></p>
<h3 id="4-Naive-Bayes-Algorithms-朴素贝叶斯"><a href="#4-Naive-Bayes-Algorithms-朴素贝叶斯" class="headerlink" title="4. Naive Bayes Algorithms 朴素贝叶斯"></a>4. Naive Bayes Algorithms 朴素贝叶斯</h3><ol>
<li>Naive Bayes</li>
<li>Gaussian Naive Bayes</li>
<li>Multinomial Naive Bayes</li>
<li>Bayesian Belief Network (BBN)</li>
<li>Bayesian Network (BN)</li>
</ol>
<p>朴素贝叶斯基本公式：$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$<br><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/09/J46IjSoqFuD1vtW.png" alt="J46IjSoqFuD1vtW"></p>
<h3 id="5-KNN-k-NearestNeighbor-K-最邻近算法"><a href="#5-KNN-k-NearestNeighbor-K-最邻近算法" class="headerlink" title="5. KNN (k-NearestNeighbor) K 最邻近算法"></a>5. KNN (k-NearestNeighbor) K 最邻近算法</h3><p>用于分类</p>
<ol>
<li>计算测试数据与各个训练数据之间的距离；</li>
<li>按照距离的递增关系进行排序；</li>
<li>选取距离最小的K个点；</li>
<li>确定前K个点所在类别的出现频率；</li>
<li>返回前K个点中出现频率最高的类别作为测试数据的预测分类</li>
</ol>
<p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/09/ScAOKyPqMGWdrLH.png" alt="KNN 算法"></p>
<h3 id="6-Clustering-Algorithm-聚类算法"><a href="#6-Clustering-Algorithm-聚类算法" class="headerlink" title="6. Clustering Algorithm 聚类算法"></a>6. Clustering Algorithm 聚类算法</h3><ol>
<li>k-Means：选取平均值</li>
<li>k-Medians：由选取平均值改为选取中位数</li>
<li>Expectation Maximisation (EM)：有隐含随机变量的概率模型的参数的估计方法，它是一种无监督的算法</li>
<li><p>Hierarchical Clustering 层次聚类：<br>算法如下：</p>
<p>(1) 将每个对象看作一类，计算两两之间的最小距离；</p>
<p>(2) 将距离最小的两个类合并成一个新类；</p>
<p>(3) 重新计算新类与所有类之间的距离；</p>
<p>(4) 重复(2)、(3)，直到所有类最后合并成一类。</p>
</li>
</ol>
<h3 id="7-K-means-算法"><a href="#7-K-means-算法" class="headerlink" title="7. K-means 算法"></a>7. K-means 算法</h3><p>算法如下：</p>
<pre><code>选取k个初始质心(作为初始cluster);
repeat:
    对每个样本点，计算得到距其最近的质心，将其类别标为该质心所对应的cluster;
    重新计算k个cluser对应的质心;
until 质心不再发生变化
</code></pre><p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/09/VLCReWo92QXSNP4.jpg" alt=" K-means"></p>
<p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/05/11/xwHcWKXLUofrZdM.png" alt="KNN 与 K-means 的区别"></p>
<h3 id="8-Random-Forest-随机森林"><a href="#8-Random-Forest-随机森林" class="headerlink" title="8. Random Forest 随机森林"></a>8. Random Forest 随机森林</h3><h3 id="9-Dimensionality-Reduction-Algorithms-降维算法-PCA"><a href="#9-Dimensionality-Reduction-Algorithms-降维算法-PCA" class="headerlink" title="9. Dimensionality Reduction Algorithms 降维算法 PCA"></a>9. Dimensionality Reduction Algorithms 降维算法 PCA</h3><p>目的: 将数据的某一维的特征差异进行放大, 其他维度进行压缩甚至忽略. 也可以选择降到 k 维.</p>
<h4 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h4><ol>
<li><p>将数据集的中心移到 0</p>
<script type="math/tex; mode=display">
\mu=\frac{1}{m} \sum_{i=1}^{m} x^{(i)}</script><script type="math/tex; mode=display">
x^{(i)} \leftarrow x^{(i)}-\mu</script></li>
<li><p>统一每个变量的方差</p>
<script type="math/tex; mode=display">
\sigma_{j}^{2}=\frac{1}{m} \sum_{i=1}^{m}\left(x_{j}^{(i)}\right)^{2}</script><script type="math/tex; mode=display">
x_{j}^{(i)} \leftarrow x_{j}^{(i)} / \sigma_{j}</script></li>
</ol>
<h4 id="推导过程"><a href="#推导过程" class="headerlink" title="推导过程"></a>推导过程</h4><ul>
<li>利用特征值与特征向量, 找到一组标准正交基 —-&gt; 从而可以将任何向量用标准正交基表示 —-&gt; 从而可以将任何协方差矩阵用 $UWU^T$ 形式表示, U 为正交基矩阵, W 就是本矩阵的特征值.</li>
<li>PCA 的目的是找到一个方向 $v$, 使得在这个方向上数据 X 的投影的<code>方差最大</code>, 方差越大越能保留尽可能多的有效信息. 由于均值已经为 0, 数据 $X$ 在方向 $v$ 的方差就是 $||Xv||^2$. 最后推算的结论就是, 需要<code>最大特征值</code>对应的<code>特征向量</code>.</li>
<li>最终得到 X 的主成分矩阵为 $T=WX$, 其中 $W$ 为$ X^TX$ 的特征值矩阵<br><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/03/01/wXQY5E4Vpfb9SWN.png" alt=""><br><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/03/01/pR8nGAxBsk7oNrv.png" alt=""></li>
</ul>
<h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><p>主要步骤, 给定数据 X, 进行均值归 0, 方差统一处理后, 得到 Z. 然后根据协方差矩阵 $Z^TZ$ 的特征值从大到小进行排序, 得到特征向量的矩阵 $V^ \star $. 计算 $Z^ \star  = ZV^ \star<br>$</p>
<p><a href="https://www.kaggle.com/coverdark/pca-principal-component-analysis/" target="_blank" rel="noopener">👆PCA 实现</a></p>
<pre><code class="lang-py">def pca_custom(X, k):
    n, p = np.shape(X)
    # 设置均值为 0
    Z = X - np.mean(X, axis=0)
    covZ = np.cov(Z.T) # 协方差矩阵按列计算, 需要转置一下
    eigValue, eigVector = np.linalg.eig(covZ)  # 计算特征值, 特征向量
    index = np.argsort(-eigValue)

    if k &gt; p:
        print (&quot;k must lower than input data dimension！&quot;)
        return 1
    else:
        # 要降到 k 维, 选择前 k 个最大的
        selectVector = eigVector[:, index[:k]]
        T = np.matmul(Z, selectVector)
    return T, selectVector
</code></pre>
<h3 id="10-Gradient-Boosting-algorithms-梯度提升算法"><a href="#10-Gradient-Boosting-algorithms-梯度提升算法" class="headerlink" title="10. Gradient Boosting algorithms 梯度提升算法"></a>10. Gradient Boosting algorithms 梯度提升算法</h3><ol>
<li>GBM</li>
<li>XGBoost</li>
<li>LightGBM</li>
<li>CatBoost</li>
</ol>
<h3 id="11-Deep-Learning-Algorithms-深度学习"><a href="#11-Deep-Learning-Algorithms-深度学习" class="headerlink" title="11. Deep Learning Algorithms 深度学习"></a>11. Deep Learning Algorithms 深度学习</h3><ol>
<li>Convolutional Neural Network (CNN)</li>
<li>Recurrent Neural Networks (RNNs)</li>
<li>Long Short-Term Memory Networks (LSTMs)</li>
<li>Stacked Auto-Encoders</li>
<li>Deep Boltzmann Machine (DBM)</li>
<li>Deep Belief Networks (DBN)</li>
</ol>
<hr>
<h2 id="机器学习损失函数"><a href="#机器学习损失函数" class="headerlink" title="机器学习损失函数"></a>机器学习损失函数</h2><ol>
<li>0-1损失函数<script type="math/tex; mode=display">
L(y,f(x)) =
\begin{cases}
0, & \text{y = f(x)}  \\
1, & \text{y $\neq$ f(x)}
\end{cases}</script></li>
<li>绝对值损失函数<script type="math/tex; mode=display">
L(y,f(x))=|y-f(x)|</script></li>
<li>平方损失函数<script type="math/tex; mode=display">
L(y,f(x))=(y-f(x))^2</script></li>
<li>log对数损失函数<script type="math/tex; mode=display">
L(y,f(x))=log(1+e^{-yf(x)})</script></li>
<li>指数损失函数<script type="math/tex; mode=display">
L(y,f(x))=exp(-yf(x))</script></li>
<li>Hinge损失函数<script type="math/tex; mode=display">
L(w,b)=max\{0,1-yf(x)\}</script></li>
</ol>
<hr>
<h2 id="机器学习优化方法"><a href="#机器学习优化方法" class="headerlink" title="机器学习优化方法"></a>机器学习优化方法</h2><p>梯度下降是最常用的优化方法之一，它使用梯度的反方向 $ \nabla_\theta J(\theta) $ 更新参数 $ \theta $，使得目标函数$J(\theta)$达到最小化的一种优化方法，这种方法我们叫做梯度更新. </p>
<ol>
<li>(全量)梯度下降<script type="math/tex; mode=display">
\theta=\theta-\eta\nabla_\theta J(\theta)</script></li>
<li>随机梯度下降<script type="math/tex; mode=display">
\theta=\theta-\eta\nabla_\theta J(\theta;x^{(i)},y^{(i)})</script></li>
<li>小批量梯度下降<script type="math/tex; mode=display">
\theta=\theta-\eta\nabla_\theta J(\theta;x^{(i:i+n)},y^{(i:i+n)})</script></li>
<li>引入动量的梯度下降<script type="math/tex; mode=display">
\begin{cases}
v_t=\gamma v_{t-1}+\eta \nabla_\theta J(\theta)  \\
\theta=\theta-v_t
\end{cases}</script></li>
<li>自适应学习率的Adagrad算法<script type="math/tex; mode=display">
\begin{cases}
g_t= \nabla_\theta J(\theta)  \\
\theta_{t+1}=\theta_{t,i}-\frac{\eta}{\sqrt{G_t+\varepsilon}} \cdot g_t
\end{cases}</script></li>
<li><p>牛顿法</p>
<script type="math/tex; mode=display">
\theta_{t+1}=\theta_t-H^{-1}\nabla_\theta J(\theta_t)</script><p> 其中:<br> $t$: 迭代的轮数</p>
<p> $\eta$: 学习率</p>
<p> $G_t$: 前t次迭代的梯度和</p>
<p> $\varepsilon:$很小的数,防止除0错误</p>
<p> $H$: 损失函数相当于$\theta$的Hession矩阵在$\theta_t$处的估计</p>
</li>
</ol>
<hr>
<h2 id="机器学习的评价指标"><a href="#机器学习的评价指标" class="headerlink" title="机器学习的评价指标"></a>机器学习的评价指标</h2><ol>
<li>MSE(Mean Squared Error)<script type="math/tex; mode=display">
MSE(y,f(x))=\frac{1}{N}\sum_{i=1}^{N}(y-f(x))^2</script></li>
<li>MAE(Mean Absolute Error)<script type="math/tex; mode=display">
MSE(y,f(x))=\frac{1}{N}\sum_{i=1}^{N}|y-f(x)|</script></li>
<li>RMSE(Root Mean Squard Error)<script type="math/tex; mode=display">
RMSE(y,f(x))=\frac{1}{1+MSE(y,f(x))}</script></li>
<li>Top-k准确率<script type="math/tex; mode=display">
Top_k(y,pre_y)=\begin{cases}
1, {y \in pre_y}  \\
0, {y \notin pre_y}
\end{cases}</script></li>
<li>混淆矩阵</li>
</ol>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">混淆矩阵</th>
<th style="text-align:center">Predicted as Positive</th>
<th style="text-align:center">Predicted as Negative</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Labeled as Positive</td>
<td style="text-align:center">True Positive(TP)</td>
<td style="text-align:center">False Negative(FN)</td>
</tr>
<tr>
<td style="text-align:center">Labeled as Negative</td>
<td style="text-align:center">False Positive(FP)</td>
<td style="text-align:center">True Negative(TN)</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>真正例(True Positive, TP):真实类别为正例, 预测类别为正例</li>
<li>假负例(False Negative, FN): 真实类别为正例, 预测类别为负例</li>
<li>假正例(False Positive, FP): 真实类别为负例, 预测类别为正例 </li>
<li><p>真负例(True Negative, TN): 真实类别为负例, 预测类别为负例</p>
</li>
<li><p>真正率(True Positive Rate, TPR): 被预测为正的正样本数 / 正样本实际数</p>
<script type="math/tex; mode=display">
TPR=\frac{TP}{TP+FN}</script></li>
<li><p>假负率(False Negative Rate, FNR): 被预测为负的正样本数/正样本实际数</p>
<script type="math/tex; mode=display">
FNR=\frac{FN}{TP+FN}</script></li>
<li><p>假正率(False Positive Rate, FPR): 被预测为正的负样本数/负样本实际数，</p>
<script type="math/tex; mode=display">
FPR=\frac{FP}{FP+TN}</script></li>
<li>真负率(True Negative Rate, TNR): 被预测为负的负样本数/负样本实际数，<script type="math/tex; mode=display">
TNR=\frac{TN}{FP+TN}</script></li>
<li>准确率(Accuracy)<script type="math/tex; mode=display">
ACC=\frac{TP+TN}{TP+FN+FP+TN}</script></li>
<li>精准率<script type="math/tex; mode=display">
P=\frac{TP}{TP+FP}</script></li>
<li>召回率<script type="math/tex; mode=display">
R=\frac{TP}{TP+FN}</script></li>
<li>F1-Score<script type="math/tex; mode=display">
\frac{2}{F_1}=\frac{1}{P}+\frac{1}{R}</script></li>
<li><strong>ROC</strong></li>
</ul>
<p>ROC曲线的横轴为“假正例率”，纵轴为“真正例率”. 以FPR为横坐标，TPR为纵坐标，那么ROC曲线就是改变各种阈值后得到的所有坐标点 (FPR,TPR) 的连线，画出来如下。红线是随机乱猜情况下的ROC，曲线越<code>靠左上角</code>，分类器越佳. </p>
<ul>
<li><strong>AUC(Area Under Curve)</strong></li>
</ul>
<p>AUC就是ROC曲线下的面积. 真实情况下，由于数据是一个一个的，阈值被离散化，呈现的曲线便是锯齿状的，当然数据越多，阈值分的越细，”曲线”越光滑. </p>
<p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/09/Ky4FT1MVe3PUYai.jpg" alt="Ky4FT1MVe3PUYai"></p>
<p>用AUC<code>判断分类器（预测模型）优劣</code>的标准:</p>
<ul>
<li>AUC = 1 是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数预测的场合，不存在完美分类器.</li>
<li>0.5 &lt; AUC &lt; 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值.</li>
<li>AUC &lt; 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测.</li>
</ul>
<h2 id="机器学习模型选择"><a href="#机器学习模型选择" class="headerlink" title="机器学习模型选择"></a>机器学习模型选择</h2><ol>
<li>交叉验证</li>
</ol>
<p>所有数据分为三部分：训练集、交叉验证集和测试集。交叉验证集不仅在选择模型时有用，在超参数选择、正则项参数 [公式] 和评价模型中也很有用。</p>
<ol>
<li>k-折叠交叉验证</li>
</ol>
<ul>
<li>假设训练集为S ，将训练集等分为k份:$\{S_1, S_2, …, S_k\}$. </li>
<li>然后每次从集合中拿出k-1份进行训练</li>
<li>利用集合中剩下的那一份来进行测试并计算损失值</li>
<li>最后得到k次测试得到的损失值，并选择平均损失值最小的模型</li>
</ul>
<ol>
<li>Bias与Variance，欠拟合与过拟合</li>
</ol>
<p><strong>欠拟合</strong>一般表示模型对数据的表现能力不足，通常是模型的复杂度不够，并且Bias高，训练集的损失值高，测试集的损失值也高.</p>
<p><strong>过拟合</strong>一般表示模型对数据的表现能力过好，通常是模型的复杂度过高，并且Variance高，训练集的损失值低，测试集的损失值高.</p>
<p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/09/dIiSlJVkjFL6EpB.jpg" alt="dIiSlJVkjFL6EpB"></p>
<p><img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://i.loli.net/2020/02/09/Dn5bB8wUVgmsIzC.jpg" alt="Dn5bB8wUVgmsIzC"></p>
<ol>
<li>解决方法</li>
</ol>
<ul>
<li>增加训练样本: 解决高Variance情况</li>
<li>减少特征维数: 解决高Variance情况</li>
<li>增加特征维数: 解决高Bias情况</li>
<li>增加模型复杂度: 解决高Bias情况</li>
<li>减小模型复杂度: 解决高Variance情况</li>
</ul>
<h2 id="机器学习参数调优"><a href="#机器学习参数调优" class="headerlink" title="机器学习参数调优"></a>机器学习参数调优</h2><ol>
<li>网格搜索</li>
</ol>
<p>一种调参手段；穷举搜索：在所有候选的参数选择中，通过循环遍历，尝试每一种可能性，表现最好的参数就是最终的结果</p>
<ol>
<li>随机搜索</li>
</ol>
<p>与网格搜索相比，随机搜索并未尝试所有参数值，而是从指定的分布中采样固定数量的参数设置。它的理论依据是，如果随即样本点集足够大，那么也可以找到全局的最大或最小值，或它们的近似值。通过对搜索范围的随机取样，随机搜索一般会比网格搜索要快一些。</p>
<ol>
<li>贝叶斯优化算法</li>
</ol>
<p>贝叶斯优化用于机器学习调参由J. Snoek(2012)提出，主要思想是，给定优化的目标函数(广义的函数，只需指定输入和输出即可，无需知道内部结构以及数学性质)，通过不断地添加样本点来更新目标函数的后验分布(高斯过程,直到后验分布基本贴合于真实分布。简单的说，就是考虑了上一次参数的信息，从而更好的调整当前的参数。</p>

            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://hopenx.github.io" rel="external nofollow noreferrer">HopenX</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://hopenx.github.io/2020/02/06/ml-study-notes/">https://hopenx.github.io/2020/02/06/ml-study-notes/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="https://hopenx.github.io" target="_blank">HopenX</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">
                                    <span class="chip bg-color">机器学习</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io@master/libs/share/css/share.min.css">

<div id="article-share">
    
    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io@master/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io@master/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io@master/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>
            
        </div>
    </div>

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2020/02/07/math-notes/">
                    <div class="card-image">
                        
                        
                        <img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io@master/medias/featureimages/16.jpg" class="responsive-img" alt="数学基础备忘录">
                        
                        <span class="card-title">数学基础备忘录</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            一些基础的公式，方便自己查用
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2020-02-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E6%95%B0%E5%AD%A6/" class="post-category">
                                    数学
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/math/">
                        <span class="chip bg-color">math</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2020/02/06/pat-advance/">
                    <div class="card-image">
                        
                        
                        <img src="https://i.loli.net/2020/02/09/2OERK3tFJve4YDH.gif" data-original="https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io@master/medias/featureimages/1.jpg" class="responsive-img" alt="OJ 做题笔记">
                        
                        <span class="card-title">OJ 做题笔记</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            PAT-Advanced Level 重点题目分类记录，对于夯实数据结构和算法基础有很重要的帮助. 后期加入了 TK 题库, 牛客网等等.
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2020-02-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E7%AE%97%E6%B3%95/" class="post-category">
                                    算法
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/PAT/">
                        <span class="chip bg-color">PAT</span>
                    </a>
                    
                    <a href="/tags/OJ/">
                        <span class="chip bg-color">OJ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io@master/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io@master/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io@master/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io@master/libs/codeBlock/codeShrink.js"></script>


<!-- 代码块折行 -->

<style type="text/css">
code[class*="language-"], pre[class*="language-"] { white-space: pre !important; }
</style>

    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io@master/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>


<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\(', '\)']]}
    });
</script>


    <footer class="page-footer bg-color">
    <div class="container row center-align" style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            <span id="year">2020</span>
            <a href="https://hopenx.github.io" target="_blank">HopenX</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                class="white-color">82.4k</span>&nbsp;字
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;次
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                |&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv"
                    class="white-color"></span>&nbsp;人
            </span>
            
            <br>
            
            <span id="sitetime">载入运行时间...</span>
            <script>
                function siteTime() {
                    var seconds = 1000;
                    var minutes = seconds * 60;
                    var hours = minutes * 60;
                    var days = hours * 24;
                    var years = days * 365;
                    var today = new Date();
                    var startYear = "2020";
                    var startMonth = "1";
                    var startDate = "2";
                    var startHour = "0";
                    var startMinute = "";
                    var startSecond = "";
                    var todayYear = today.getFullYear();
                    var todayMonth = today.getMonth() + 1;
                    var todayDate = today.getDate();
                    var todayHour = today.getHours();
                    var todayMinute = today.getMinutes();
                    var todaySecond = today.getSeconds();
                    var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                    var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                    var diff = t2 - t1;
                    var diffYears = Math.floor(diff / years);
                    var diffDays = Math.floor((diff / days) - diffYears * 365);
                    var diffHours = Math.floor((diff - (diffYears * 365 + diffDays) * days) / hours);
                    var diffMinutes = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours) /
                        minutes);
                    var diffSeconds = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours -
                        diffMinutes * minutes) / seconds);
                    if (startYear == todayYear) {
                        document.getElementById("year").innerHTML = todayYear;
                        document.getElementById("sitetime").innerHTML = "本站已安全运行 " + diffDays + " 天 " + diffHours +
                            " 小时 ";
                    } else {
                        document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                        document.getElementById("sitetime").innerHTML = "本站已安全运行 " + diffYears + " 年 " + diffDays +
                            " 天 " + diffHours + " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
                    }
                }
                setInterval(siteTime, 1000);
            </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/hopenx" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>









    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=1152214965" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 1152214965" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>





    <a href="https://www.zhihu.com/" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>

<script src="https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io@master/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io@master/js/search.js"></script>
<script type="text/javascript">
$(function () {
    searchFunc("https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io@master/" + "search.xml", 'searchInput', 'searchResult');
});
</script>
    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io@master/libs/materialize/materialize.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io@master/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io@master/libs/aos/aos.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io@master/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io@master/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io@master/js/matery.js"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->


    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    
    <script async src="https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io@master/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    

    

    
    
    
    <script src="https://cdn.jsdelivr.net/gh/hopenx/hopenx.github.io@master/libs/instantpage/instantpage.js" type="module"></script>
    

<script>!function(e){var c=Array.prototype.slice.call(document.querySelectorAll("img[data-original]"));function i(){for(var r=0;r<c.length;r++)t=c[r],0<=(n=t.getBoundingClientRect()).bottom&&0<=n.left&&n.top<=(e.innerHeight||document.documentElement.clientHeight)&&function(){var t,n,e,i,o=c[r];t=o,n=function(){c=c.filter(function(t){return o!==t})},e=new Image,i=t.getAttribute("data-original"),e.onload=function(){t.src=i,n&&n()},e.src=i}();var t,n}i(),e.addEventListener("scroll",function(){var t,n;t=i,n=e,clearTimeout(t.tId),t.tId=setTimeout(function(){t.call(n)},500)})}(this);</script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script></body>

</html>
